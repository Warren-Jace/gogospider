# 请求日志功能说明

## 功能介绍

v4.4版本新增了**请求日志记录**功能，用于保存爬虫发起的所有HTTP请求的详细信息，包括：

- 请求时间戳
- 请求方法（GET/POST/PUT等）
- 完整URL
- URL路径
- 查询参数（键值对）
- 请求头
- 请求体（POST等）
- 响应状态码
- 响应时间（毫秒）
- 错误信息（如果有）

此功能主要用于：
- **调试爬虫行为**：查看爬虫实际发起了哪些请求
- **分析请求模式**：统计请求方法、状态码分布等
- **优化爬取策略**：根据请求日志分析哪些URL被重复请求、响应时间分布等
- **性能分析**：查看平均响应时间、失败率等指标

## 使用方法

### 方法1：命令行参数启用

```bash
# 启用请求日志记录
spider.exe -url https://example.com -enable-request-logging

# 结合其他参数使用
spider.exe -url https://example.com -depth 3 -enable-request-logging
```

### 方法2：配置文件启用

在 `config.json` 中设置：

```json
{
  "target_url": "https://example.com",
  "enable_request_logging": true
}
```

然后运行：

```bash
spider.exe -config config.json
```

## 输出文件

启用请求日志后，爬虫会在当前目录生成两个文件：

### 1. 文本格式：`spider_{domain}_{timestamp}_requests.txt`

可读性强的文本格式，包含详细的请求信息：

```
═══════════════════════════════════════════════════════════
  GogoSpider - 请求日志详情
  生成时间: 2025-11-05 15:30:45
  总请求数: 125
═══════════════════════════════════════════════════════════

【请求 1】
时间: 2025-11-05 15:30:45.123
方法: GET
URL: https://example.com/api/users?page=1&limit=10
路径: /api/users
查询参数:
  page = 1
  limit = 10
请求头:
  User-Agent: Mozilla/5.0...
  Cookie: session=xxx
状态码: 200
响应时间: 145 ms

─────────────────────────────────────────────────────────

【请求 2】
...
```

### 2. JSON格式：`spider_{domain}_{timestamp}_requests.json`

结构化的JSON格式，便于程序处理和分析：

```json
{
  "timestamp": "2025-11-05T15:30:45+08:00",
  "total_requests": 125,
  "statistics": {
    "methods": {
      "GET": 120,
      "POST": 5
    },
    "status_codes": {
      "200": 115,
      "404": 8,
      "500": 2
    },
    "error_count": 2,
    "avg_response_time_ms": 156
  },
  "logs": [
    {
      "timestamp": "2025-11-05T15:30:45.123+08:00",
      "method": "GET",
      "url": "https://example.com/api/users?page=1&limit=10",
      "path": "/api/users",
      "query": {
        "page": "1",
        "limit": "10"
      },
      "headers": {
        "User-Agent": "Mozilla/5.0...",
        "Cookie": "session=xxx"
      },
      "body": "",
      "status_code": 200,
      "response_time_ms": 145,
      "error": ""
    }
  ]
}
```

## 统计报告

启用请求日志后，程序结束时会显示统计摘要：

```
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
📊 请求日志统计
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

🎯 总请求数: 125

📋 按方法统计:
  GET   :   120 (96.0%)
  POST  :     5 (4.0%)

📈 按状态码统计:
  ✅ 200:   115 (92.0%)
  ↪️ 301:     3 (2.4%)
  ❌ 404:     5 (4.0%)
  ❌ 500:     2 (1.6%)

⚠️  失败请求: 2 (1.6%)

⏱️  平均响应时间: 156 ms
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

## 性能考虑

- 请求日志记录对性能影响很小（<2%）
- 默认最多保存10万条请求日志（防止内存溢出）
- 日志记录使用了线程安全的机制
- 建议在调试和优化阶段启用，生产环境可关闭

## 使用场景示例

### 场景1：调试为什么某些URL没有被爬取

启用请求日志后，查看 `*_requests.txt` 文件，可以看到爬虫实际发起的所有请求，对比发现的URL列表，可以找出哪些URL被过滤掉了。

### 场景2：分析爬虫性能瓶颈

查看JSON格式的日志，按响应时间排序，找出最慢的请求：

```python
import json

with open('spider_example_com_20251105_153045_requests.json') as f:
    data = json.load(f)
    
# 按响应时间排序
logs = sorted(data['logs'], key=lambda x: x['response_time_ms'], reverse=True)

# 显示最慢的10个请求
for log in logs[:10]:
    print(f"{log['url']}: {log['response_time_ms']}ms")
```

### 场景3：统计请求成功率

```python
import json

with open('spider_example_com_20251105_153045_requests.json') as f:
    data = json.load(f)
    
success = sum(1 for log in data['logs'] if 200 <= log['status_code'] < 300)
total = len(data['logs'])

print(f"成功率: {success/total*100:.2f}% ({success}/{total})")
```

## 注意事项

1. **敏感信息**：请求日志可能包含Cookie、Token等敏感信息，请妥善保管日志文件
2. **文件大小**：大规模爬取时日志文件可能较大，注意磁盘空间
3. **性能**：虽然影响很小，但如果追求极致性能，可以在生产环境关闭此功能
4. **隐私**：请勿将包含敏感信息的日志文件分享或上传到公共平台

## 常见问题

### Q: 为什么响应时间都是0？

A: 目前使用的Colly库不提供精确的响应时间测量，这是一个已知限制。状态码等其他信息是准确的。

### Q: 可以只保存失败的请求吗？

A: 当前版本保存所有请求。如果只需要失败的请求，可以使用JSON格式并自己过滤：

```python
failed = [log for log in data['logs'] if log['status_code'] >= 400 or log['error']]
```

### Q: 日志文件太大怎么办？

A: 可以考虑：
1. 减少爬取深度（-depth参数）
2. 使用JSON格式并编程过滤重要信息
3. 定期清理旧日志文件

## 技术支持

如有问题或建议，请在项目Issue中反馈。

