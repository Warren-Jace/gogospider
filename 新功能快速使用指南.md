# Spider Pro 新功能快速使用指南

## 🚀 快速开始

### 功能1：批量URL扫描

```bash
# 1. 创建URL列表文件
cat > urls.txt << EOF
http://example1.com
http://example2.com
http://example3.com
EOF

# 2. 批量爬取
./spider_pro -urls urls.txt -depth 2
```

**输出示例：**
```
从文件 urls.txt 加载了 3 个URL

╔═══════════════════════════════════════════════╗
║  处理目标 [1/3]: http://example1.com
╚═══════════════════════════════════════════════╝

开始爬取: http://example1.com
...
报告已生成: spider_example1.com_20251021_150405.txt

╔═══════════════════════════════════════════════╗
║  处理目标 [2/3]: http://example2.com
╚═══════════════════════════════════════════════╝
...
```

---

### 功能2：表单模糊匹配（自动启用）

**无需额外配置**，爬虫会自动使用模糊匹配填充表单。

**测试效果：**
```html
<!-- 原始表单 -->
<form>
  <input name="userEmail">        → 自动填充：test@example.com ✓
  <input name="user_phone">       → 自动填充：13800138000 ✓
  <input name="contactName">      → 自动填充：张三 ✓
  <input name="emial_addr">       → 模糊匹配email，填充：test@example.com ✓
</form>
```

**查看匹配效果：**
```bash
./spider_pro -url http://example.com -depth 2

# 输出会显示智能表单填充统计
[智能表单填充]
  支持的字段类型: 21种
  Fuzz载荷类型: 4种
```

---

### 功能3：扩展技术栈检测（自动启用）

**无需额外配置**，爬虫会自动检测50+种技术栈。

**运行爬虫：**
```bash
./spider_pro -url http://example.com -depth 2
```

**查看检测结果：**
```
═══════════════════════════════════════════════
【技术栈识别】
═══════════════════════════════════════════════

[前端框架]
  ✓ React 18.2.0 (置信度:90%)
     证据: HTML特征匹配
  ✓ Bootstrap 5.1.3 (置信度:85%)
     证据: HTML特征匹配

[Web服务器]
  ✓ Nginx 1.18.0 (置信度:100%)
     证据: Header: Server=nginx/1.18.0

[CDN]
  ✓ Cloudflare (置信度:95%)
     证据: Header: CF-RAY=xxx
```

**新增检测技术（部分）：**
- ✅ Bootstrap, Tailwind CSS
- ✅ Next.js, Nuxt.js, Svelte
- ✅ Express, Flask, Ruby on Rails
- ✅ Joomla, Drupal, Magento
- ✅ Axios, Lodash, Moment.js
- ✅ Vercel, Netlify

---

### 功能4：子域名提取（自动启用）

**无需额外配置**，爬虫会自动提取子域名。

**运行爬虫：**
```bash
./spider_pro -url http://example.com -depth 2
```

**实时输出：**
```
  [子域名] 发现 3 个新子域名
  [子域名] 发现 2 个新子域名
```

**报告中的子域名部分：**
```
【扫描统计】
  • 发现的子域名: 12

[子域名发现]
  发现子域名总数: 12
  子域名列表:
    1. admin.example.com
    2. api.example.com
    3. cdn.example.com
    4. dev.example.com
    5. images.example.com
    ...
```

**子域名来源：**
- HTML中的链接和资源引用
- JavaScript代码中的域名配置
- CSS文件中的url()引用
- 注释中的域名信息

---

### 功能5：代理服务器模式

#### 步骤1：启动代理服务器

```bash
# 基础启动（监听 127.0.0.1:8080）
./spider_pro -proxy

# 指定监听地址
./spider_pro -proxy -proxy-addr 127.0.0.1:9090

# 只记录特定域名
./spider_pro -proxy -url http://example.com
```

**输出：**
```
╔═══════════════════════════════════════════════╗
║         Spider Pro - 代理服务器模式            ║
╚═══════════════════════════════════════════════╝

【代理服务器已启动】
  监听地址: 127.0.0.1:8080
  目标域名: example.com (仅记录此域名的请求)

【配置说明】
  1. 在浏览器或工具中配置HTTP代理为: 127.0.0.1:8080
  2. 访问目标网站，代理将自动拦截和分析请求
  3. 按 Ctrl+C 停止服务器并查看报告

正在监听请求...
```

#### 步骤2：配置浏览器代理

**Windows (Chrome/Edge):**
1. 按 `Win + I` 打开设置
2. 网络和Internet → 代理
3. 手动设置代理：
   - HTTP代理：127.0.0.1
   - 端口：8080
4. 保存

**macOS (Chrome/Edge):**
1. 系统偏好设置 → 网络
2. 高级 → 代理
3. 勾选"网页代理(HTTP)"
   - 服务器：127.0.0.1
   - 端口：8080
4. 好

**Firefox（所有平台）:**
1. 设置 → 常规 → 网络设置
2. 手动代理配置
3. HTTP代理：127.0.0.1 端口：8080
4. 勾选"也将此代理用于HTTPS"
5. 确定

#### 步骤3：使用浏览器访问目标网站

```
正在监听请求...

[拦截] GET http://example.com/ - 200 (12345 bytes)
[拦截] GET http://example.com/api/users - 200 (2345 bytes)
[拦截] POST http://example.com/login - 302 (89 bytes)
[HTTPS隧道] api.example.com:443
[统计] 请求: 15, 响应: 15, 流量: 0.34 MB
[拦截] GET http://example.com/profile - 200 (5678 bytes)
```

#### 步骤4：停止并查看报告

按 `Ctrl+C` 停止：

```
收到停止信号，正在关闭代理服务器...

╔═══════════════════════════════════════════════╗
║              代理拦截报告                      ║
╚═══════════════════════════════════════════════╝

=== 代理服务器统计 ===
总请求数: 156
总响应数: 156
总流量: 12.45 MB
运行时间: 305.23 秒

按主机统计:
  example.com: 89 请求
  api.example.com: 45 请求
  cdn.example.com: 22 请求

按方法统计:
  GET: 134 请求
  POST: 18 请求
  PUT: 3 请求
  DELETE: 1 请求

详细报告已保存到: proxy_report_20251021_150405.txt
```

---

## 💡 实用技巧

### 技巧1：批量扫描 + 深度爬取
```bash
# 创建高价值目标列表
cat > targets.txt << EOF
http://admin.example.com
http://api.example.com
http://portal.example.com
EOF

# 深度扫描
./spider_pro -urls targets.txt -depth 3 -deep
```

### 技巧2：代理 + 手动测试
```bash
# 启动代理
./spider_pro -proxy -url http://example.com

# 使用Burp Suite或其他工具，通过代理访问
# 代理会记录所有请求，方便后续分析
```

### 技巧3：查看子域名收集效果
```bash
# 1. 运行爬虫
./spider_pro -url http://example.com -depth 3

# 2. 在报告中搜索子域名部分
grep -A 20 "子域名发现" spider_*.txt
```

### 技巧4：技术栈快速识别
```bash
# 1. 爬取目标
./spider_pro -url http://example.com -depth 1

# 2. 查看技术栈
grep -A 30 "技术栈识别" spider_*.txt
```

---

## 🎯 常见场景示例

### 场景1：资产管理
```bash
# 1. 收集所有子系统URL
cat > company_assets.txt << EOF
http://www.company.com
http://admin.company.com
http://api.company.com
http://portal.company.com
http://mobile.company.com
EOF

# 2. 批量爬取并收集子域名
./spider_pro -urls company_assets.txt -depth 2

# 3. 合并所有报告中的子域名
grep "子域名列表" spider_*.txt | sort -u
```

### 场景2：API接口发现
```bash
# 1. 启动代理
./spider_pro -proxy -url http://api.example.com

# 2. 使用移动APP或Web应用
# 所有API请求都会被拦截记录

# 3. Ctrl+C 停止，查看报告
# 报告中包含所有API端点和参数
```

### 场景3：技术栈调研
```bash
# 1. 准备竞品URL列表
cat > competitors.txt << EOF
http://competitor1.com
http://competitor2.com
http://competitor3.com
EOF

# 2. 爬取
./spider_pro -urls competitors.txt -depth 1

# 3. 提取技术栈信息
grep -A 20 "技术栈识别" spider_*.txt > tech_stack_report.txt
```

### 场景4：安全测试
```bash
# 1. 启动代理模式
./spider_pro -proxy -url http://target.com

# 2. 手动测试各种功能
# - 登录/注册
# - 文件上传
# - 数据修改
# - 删除操作

# 3. 查看拦截的所有请求
# 报告中包含完整的请求体和响应
```

---

## 📝 命令速查表

```bash
# 单URL爬取
./spider_pro -url http://example.com -depth 2

# 批量URL爬取
./spider_pro -urls urls.txt -depth 2

# 启动代理（默认端口）
./spider_pro -proxy

# 启动代理（指定端口）
./spider_pro -proxy -proxy-addr 127.0.0.1:9090

# 代理+域名过滤
./spider_pro -proxy -url http://example.com

# 深度爬取
./spider_pro -url http://example.com -depth 3 -deep

# 使用配置文件
./spider_pro -config config.json

# 从Burp导入
./spider_pro -burp requests.xml

# 从HAR导入
./spider_pro -har archive.har
```

---

## ⚠️ 注意事项

### 批量URL模式
- ✅ URL文件支持注释（# 开头）
- ✅ 自动跳过无效URL
- ✅ 失败不影响后续处理
- ⚠️ 大量URL建议分批处理

### 代理模式
- ✅ 支持HTTP和HTTPS（隧道模式）
- ✅ 自动过滤静态资源
- ⚠️ HTTPS内容无法解密查看
- ⚠️ 使用完记得关闭浏览器代理

### 子域名提取
- ✅ 自动从多种来源提取
- ✅ 智能去重
- ⚠️ 只提取目标主域名的子域名
- ⚠️ 深度越大，发现越多

### 技术栈检测
- ✅ 50+种技术自动识别
- ✅ 支持版本检测
- ⚠️ 某些技术需要深度爬取才能发现
- ⚠️ 置信度仅供参考

---

## 🆘 问题排查

### Q1: 批量爬取时某些URL失败
**A:** 检查URL格式，必须包含协议（http://或https://）

### Q2: 代理模式无法拦截HTTPS
**A:** HTTPS使用隧道模式，无法查看内容，这是正常的

### Q3: 子域名提取数量少
**A:** 增加爬取深度，或使用代理模式手动访问更多页面

### Q4: 技术栈检测不准确
**A:** 
- 增加爬取深度
- 某些技术可能故意隐藏特征
- 查看报告中的"证据"字段

### Q5: 代理模式端口已被占用
**A:** 使用 `-proxy-addr` 指定其他端口

---

## 📚 相关文档

- [新增功能说明.md](新增功能说明.md) - 详细功能说明
- [README.md](README.md) - 项目完整文档
- [高级功能使用指南.md](高级功能使用指南.md) - 更多高级用法

---

**Spider Pro v2.2 - 让Web爬取更简单！** 🎉

