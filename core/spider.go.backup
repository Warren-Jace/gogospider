package core

import (
	"encoding/json"
	"fmt"
	"io"
	"log"
	"log/slog"
	"net/http"
	"net/url"
	"os"
	"sort"
	"strings"
	"sync"
	"time"

	"spider-golang/config"
)

// Spider 主爬虫协调器
type Spider struct {
	config              *config.Config
	staticCrawler       StaticCrawler
	dynamicCrawler      DynamicCrawler
	jsAnalyzer          *JSAnalyzer
	paramHandler        *ParamHandler
	duplicateHandler    *DuplicateHandler
	smartDeduplication  *SmartDeduplication
	smartParamDedup     *SmartParamDeduplicator // 智能参数值去重器（v2.6.1）
	businessFilter      *BusinessAwareURLFilter  // 业务感知过滤器（v2.7）
	urlPatternDedup     *URLPatternDeduplicator  // URL模式去重器（v2.9）
	hiddenPathDiscovery *HiddenPathDiscovery
	cdnDetector         *CDNDetector // CDN检测器
	workerPool          *WorkerPool  // 并发工作池

	// 新增优化组件
	formFiller       *SmartFormFiller      // 智能表单填充器
	advancedScope    *AdvancedScope        // 高级作用域控制
	scopeController  *ScopeController      // 作用域控制器（v3.1）
	perfOptimizer    *PerformanceOptimizer // 性能优化器

	// 高级功能组件
	techDetector       *TechStackDetector     // 技术栈检测器
	sensitiveDetector  *SensitiveInfoDetector // 敏感信息检测器
	passiveCrawler     *PassiveCrawler        // 被动爬取器
	subdomainExtractor *SubdomainExtractor    // 子域名提取器
	domSimilarity      *DOMSimilarityDetector // DOM相似度检测器
	sitemapCrawler     *SitemapCrawler        // Sitemap爬取器
	assetClassifier    *AssetClassifier       // 静态资源分类器
	ipDetector         *IPDetector            // IP地址检测器
	
	// 🆕 v2.7+ 新增组件
	cssAnalyzer         *CSSAnalyzer            // CSS分析器
	resourceClassifier  *ResourceClassifier     // 资源分类器
	urlDeduplicator     *URLDeduplicator        // URL去重器（忽略参数值）
	urlStructureDedup   *URLStructureDeduplicator // URL结构化去重器（路径变量+参数）
	priorityScheduler   *URLPriorityScheduler   // 优先级调度器（可选）
	
	// 🆕 v3.2 新增组件
	cookieManager      *CookieManager      // Cookie管理器
	loginWallDetector  *LoginWallDetector  // 登录墙检测器
	redirectManager    *RedirectManager    // 重定向管理器
	
	// 🆕 v3.4 新增组件
	adaptiveLearner    *AdaptivePriorityLearner // 自适应优先级学习器
	
	// 🆕 v3.5 新增组件 - URL质量控制
	urlValidator       *URLValidator            // URL验证器（过滤无效URL）
	postDetector       *POSTRequestDetector     // POST请求检测器（增强POST检测）

	results           []*Result
	sitemapURLs       []string         // 从sitemap发现的URL
	robotsURLs        []string         // 从robots.txt发现的URL
	externalLinks     []string         // 记录外部链接
	hiddenPaths       []string         // 记录隐藏路径
	securityFindings  []string         // 记录安全发现
	crossDomainJS     []string         // 记录跨域JS发现的URL
	detectedTechs     []*TechInfo      // 检测到的技术栈
	sensitiveFindings []*SensitiveInfo // 敏感信息发现
	mutex             sync.Mutex
	targetDomain      string          // 目标域名
	visitedURLs       map[string]bool // 已访问URL

	// 资源管理（优化：防止泄漏）
	done     chan struct{}  // 完成信号
	wg       sync.WaitGroup // 等待所有goroutine完成
	closed   bool           // 是否已关闭
	closeMux sync.Mutex     // 关闭锁

	// v2.6: 日志和监控
	logger Logger // 结构化日志记录器
}

// NewSpider 创建爬虫实例
func NewSpider(cfg *config.Config) *Spider {
	// v2.6: 创建日志记录器
	var logOutput io.Writer = os.Stdout
	if cfg.LogSettings.OutputFile != "" {
		file, err := os.OpenFile(cfg.LogSettings.OutputFile,
			os.O_CREATE|os.O_WRONLY|os.O_APPEND, 0644)
		if err != nil {
			log.Printf("无法打开日志文件 %s: %v，使用标准输出", cfg.LogSettings.OutputFile, err)
		} else {
			logOutput = file
		}
	}

	logLevel := parseLogLevel(cfg.LogSettings.Level)
	logger := NewLogger(logLevel, logOutput)

	// 创建结果通道和停止通道
	resultChan := make(chan Result, 100)
	stopChan := make(chan struct{})

	// 计算并发worker数量（默认20个，可配置）
	workerCount := 20
	if cfg.DepthSettings.MaxDepth > 2 {
		workerCount = 30 // 深度爬取时增加worker数
	}

	// 速率限制（每秒最多20个请求，避免过载）
	maxQPS := 20

	spider := &Spider{
		config:             cfg,
		staticCrawler:      NewStaticCrawler(cfg, resultChan, stopChan),
		dynamicCrawler:     NewDynamicCrawler(),
		jsAnalyzer:         NewJSAnalyzer(),
		paramHandler:       NewParamHandler(),
		duplicateHandler:   NewDuplicateHandler(cfg.DeduplicationSettings.SimilarityThreshold),
		smartDeduplication: NewSmartDeduplication(),                                                                                                             // 初始化智能去重
		smartParamDedup:    NewSmartParamDeduplicator(cfg.DeduplicationSettings.MaxParamValueVariantsPerGroup, cfg.DeduplicationSettings.EnableSmartParamDedup), // v2.6.1: 智能参数值去重
		businessFilter:     NewBusinessAwareURLFilter(BusinessFilterConfig{                                                                                       // v2.7: 业务感知过滤器
			MinBusinessScore:        cfg.DeduplicationSettings.BusinessFilterMinScore,
			HighValueThreshold:      cfg.DeduplicationSettings.BusinessFilterHighValueThreshold,
			MaxSamePatternLowValue:  cfg.DeduplicationSettings.BusinessFilterMaxLowValue,
			MaxSamePatternMidValue:  cfg.DeduplicationSettings.BusinessFilterMaxMidValue,
			MaxSamePatternHighValue: cfg.DeduplicationSettings.BusinessFilterMaxHighValue,
			EnableAdaptiveLearning:  cfg.DeduplicationSettings.BusinessFilterAdaptiveLearning,
			Enabled:                 cfg.DeduplicationSettings.EnableBusinessAwareFilter,
		}),
		urlPatternDedup: NewURLPatternDeduplicator(), // v2.9: URL模式去重器
		cdnDetector:     NewCDNDetector(),            // 初始化CDN检测器
		workerPool:      NewWorkerPool(workerCount, maxQPS), // 初始化工作池

		// 初始化新增组件
		formFiller:      NewSmartFormFiller(),         // 智能表单填充器
		advancedScope:   nil,                          // 将在Start中初始化
		scopeController: nil,                          // 将在Start中初始化（v3.1）
		perfOptimizer:   NewPerformanceOptimizer(500), // 性能优化器（限制500MB）

		// 初始化高级功能组件
		techDetector:      NewTechStackDetector(),         // 技术栈检测器
		sensitiveDetector: NewSensitiveInfoDetector(),     // 敏感信息检测器
		
		// 🆕 v3.5: 初始化URL质量控制组件
		urlValidator:      NewURLValidator(),              // URL验证器
		postDetector:      NewPOSTRequestDetector(),       // POST请求检测器
		passiveCrawler:    nil,                            // 按需创建
		domSimilarity:     NewDOMSimilarityDetector(0.85), // DOM相似度检测器（阈值85%）
		sitemapCrawler:    NewSitemapCrawler(),            // Sitemap爬取器
		assetClassifier:   NewAssetClassifier(),           // 静态资源分类器
		ipDetector:        NewIPDetector(),                // IP地址检测器
		
		// 🆕 v2.7+ 新增组件
		cssAnalyzer:        NewCSSAnalyzer(),              // CSS分析器
		resourceClassifier: nil,                           // 将在Start中初始化（需要目标域名）
		urlDeduplicator:    NewURLDeduplicator(),         // URL去重器
		urlStructureDedup:  NewURLStructureDeduplicator(), // URL结构化去重器
		priorityScheduler:  nil,                           // 将在Start中初始化（可选，需要配置）
		
		// 🆕 v3.2 新增组件
		cookieManager:     NewCookieManager(),      // Cookie管理器
		loginWallDetector: NewLoginWallDetector(),  // 登录墙检测器
		redirectManager:   NewRedirectManager(),    // 重定向管理器
		
		// 🆕 v3.4 新增组件
		adaptiveLearner:   nil,                     // 将在Start中初始化（如果启用）

		hiddenPathDiscovery: nil, // 将在Start方法中初始化，需要用户代理
		results:             make([]*Result, 0),
		externalLinks:       make([]string, 0),
		hiddenPaths:         make([]string, 0),
		securityFindings:    make([]string, 0),
		crossDomainJS:       make([]string, 0),
		detectedTechs:       make([]*TechInfo, 0),
		sensitiveFindings:   make([]*SensitiveInfo, 0),
		sitemapURLs:         make([]string, 0),
		robotsURLs:          make([]string, 0),
		visitedURLs:         make(map[string]bool),

		// 初始化资源管理
		done:   make(chan struct{}),
		closed: false,

		// v2.6: 初始化日志
		logger: logger,
	}

	// 配置各个组件
	spider.staticCrawler.Configure(cfg)
	spider.dynamicCrawler.Configure(cfg)

	// 设置JS分析器的目标域名
	spider.jsAnalyzer.SetTargetDomain(cfg.TargetURL)
	
	// 🆕 v3.2: 将Cookie管理器和重定向管理器传递给静态爬虫
	if staticCrawlerImpl, ok := spider.staticCrawler.(*StaticCrawlerImpl); ok {
		staticCrawlerImpl.SetCookieManager(spider.cookieManager)
		staticCrawlerImpl.SetRedirectManager(spider.redirectManager)
	}

	return spider
}

// parseLogLevel 解析日志级别字符串为 slog.Level
func parseLogLevel(level string) slog.Level {
	switch strings.ToUpper(level) {
	case "DEBUG":
		return slog.LevelDebug
	case "INFO":
		return slog.LevelInfo
	case "WARN", "WARNING":
		return slog.LevelWarn
	case "ERROR":
		return slog.LevelError
	default:
		return slog.LevelInfo
	}
}

// LoadSensitiveRules 加载外部敏感信息规则文件
func (s *Spider) LoadSensitiveRules(filename string) error {
	if s.sensitiveDetector == nil {
		return fmt.Errorf("敏感信息检测器未初始化")
	}
	return s.sensitiveDetector.LoadRulesFromFile(filename)
}

// MergeSensitiveRules 合并外部敏感信息规则文件（不清空现有规则）
func (s *Spider) MergeSensitiveRules(filename string) error {
	if s.sensitiveDetector == nil {
		return fmt.Errorf("敏感信息检测器未初始化")
	}
	return s.sensitiveDetector.MergeRulesFromFile(filename)
}

// LoadCookieFromFile 从文件加载Cookie
func (s *Spider) LoadCookieFromFile(filename string) error {
	if s.cookieManager == nil {
		return fmt.Errorf("Cookie管理器未初始化")
	}
	return s.cookieManager.LoadFromFile(filename)
}

// LoadCookieFromString 从字符串加载Cookie
func (s *Spider) LoadCookieFromString(cookieString string) error {
	if s.cookieManager == nil {
		return fmt.Errorf("Cookie管理器未初始化")
	}
	return s.cookieManager.LoadFromString(cookieString)
}

// GetCookieManager 获取Cookie管理器
func (s *Spider) GetCookieManager() *CookieManager {
	return s.cookieManager
}

// Start 开始爬取
func (s *Spider) Start(targetURL string) error {
	// 确保资源清理（优化：防止泄漏）
	defer s.cleanup()

	// 解析目标URL并提取域名
	parsedURL, err := url.Parse(targetURL)
	if err != nil {
		return fmt.Errorf("无效的URL: %v", err)
	}
	s.targetDomain = parsedURL.Host

	// 设置JS分析器的目标域名
	s.jsAnalyzer.SetTargetDomain(s.targetDomain)
	
	// 设置CSS分析器的目标域名
	s.cssAnalyzer.SetTargetDomain(s.targetDomain)
	
	// 初始化资源分类器
	s.resourceClassifier = NewResourceClassifier(s.targetDomain)
	
	// 🆕 初始化优先级调度器（如果配置启用）
	// 可以通过配置文件控制是否使用优先级队列模式
	s.priorityScheduler = NewURLPriorityScheduler(s.targetDomain)

	// 初始化高级作用域控制
	s.advancedScope = NewAdvancedScope(s.targetDomain)
	s.advancedScope.SetMode(ScopeRDN)         // 根域名模式
	s.advancedScope.PresetStaticFilterScope() // 过滤静态资源
	
	// 🆕 v3.1: 初始化作用域控制器
	scopeConfig := ScopeConfig{
		IncludeDomains:    s.config.ScopeSettings.IncludeDomains,
		ExcludeDomains:    s.config.ScopeSettings.ExcludeDomains,
		IncludePaths:      s.config.ScopeSettings.IncludePaths,
		ExcludePaths:      s.config.ScopeSettings.ExcludePaths,
		IncludeRegex:      s.config.ScopeSettings.IncludeRegex,
		ExcludeRegex:      s.config.ScopeSettings.ExcludeRegex,
		IncludeExtensions: s.config.ScopeSettings.IncludeExtensions,
		ExcludeExtensions: s.config.ScopeSettings.ExcludeExtensions,
		IncludeParams:     []string{}, // 暂不支持参数过滤
		ExcludeParams:     []string{}, // 暂不支持参数过滤
		MaxDepth:          s.config.DepthSettings.MaxDepth,
		AllowSubdomains:   s.config.ScopeSettings.AllowSubdomains,
		StayInDomain:      s.config.ScopeSettings.StayInDomain,
		AllowHTTP:         s.config.ScopeSettings.AllowHTTP,
		AllowHTTPS:        s.config.ScopeSettings.AllowHTTPS,
	}
	s.scopeController, err = NewScopeController(scopeConfig)
	if err != nil {
		return fmt.Errorf("初始化作用域控制器失败: %v", err)
	}

	// 初始化子域名提取器
	s.subdomainExtractor = NewSubdomainExtractor(targetURL)

	// 检查是否重复
	if s.duplicateHandler.IsDuplicateURL(targetURL) {
		return fmt.Errorf("URL已处理过: %s", targetURL)
	}

	// v2.6: 使用结构化日志
	s.logger.Info("开始爬取",
		"url", targetURL,
		"target_domain", s.targetDomain,
		"max_depth", s.config.DepthSettings.MaxDepth,
		"version", "v2.6")

	// 显示功能清单（保留用户友好的格式）
	fmt.Printf("\n【已启用功能】Spider Ultimate v2.6\n")
	fmt.Printf("  ✓ 跨域JS分析（支持60+个CDN）\n")
	fmt.Printf("  ✓ 智能表单填充（支持20+种字段类型）\n")
	fmt.Printf("  ✓ 作用域精确控制（10个过滤维度）\n")
	fmt.Printf("  ✓ 性能优化（对象池+连接池）\n")
	fmt.Printf("  ✓ 技术栈识别（15+种框架）\n")
	fmt.Printf("  ✓ 敏感信息检测（30+种模式）\n")
	fmt.Printf("  ✓ JavaScript事件触发（点击、悬停、输入、滚动）\n")
	fmt.Printf("  ✓ AJAX请求拦截（动态URL捕获）\n")
	fmt.Printf("  ✓ 增强JS分析（对象、路由、配置）\n")
	fmt.Printf("  ✓ 静态资源分类（7种类型）\n")
	fmt.Printf("  ✓ IP地址检测（内网泄露识别）\n")
	fmt.Printf("  ✓ URL优先级排序（智能爬取策略）\n")
	fmt.Printf("  ✓ 结构化日志系统（分级、文件、JSON）🆕\n")
	fmt.Printf("\n爬取配置:\n")
	fmt.Printf("  深度: %d 层 | 并发: 20-30 | 日志: %s\n",
		s.config.DepthSettings.MaxDepth, s.config.LogSettings.Level)
	fmt.Printf("\n")

	// 初始化隐藏路径发现器
	userAgent := ""
	if len(s.config.AntiDetectionSettings.UserAgents) > 0 {
		userAgent = s.config.AntiDetectionSettings.UserAgents[0]
	}
	s.hiddenPathDiscovery = NewHiddenPathDiscovery(targetURL, userAgent)

	// === 优化：先爬取sitemap.xml和robots.txt ===
	s.logger.Info("开始爬取sitemap和robots.txt", "target", targetURL)
	sitemapURLs, robotsInfo := s.sitemapCrawler.GetAllURLs(targetURL)
	s.mutex.Lock()
	s.sitemapURLs = sitemapURLs
	s.robotsURLs = append(robotsInfo.DisallowPaths, robotsInfo.AllowPaths...)
	s.mutex.Unlock()

	s.logger.Info("sitemap和robots.txt爬取完成",
		"sitemap_urls", len(sitemapURLs),
		"disallow_paths", len(robotsInfo.DisallowPaths),
		"allow_paths", len(robotsInfo.AllowPaths),
		"extra_sitemaps", len(robotsInfo.SitemapURLs))

	// 将sitemap和robots中的URL添加到待爬取列表
	for _, u := range sitemapURLs {
		s.visitedURLs[u] = false // 标记为待爬取
	}
	for _, u := range robotsInfo.DisallowPaths {
		s.visitedURLs[u] = false // Disallow路径也要爬取
	}

	// 开始隐藏路径发现（可选）
	if s.config.StrategySettings.EnableCommonPathScan {
		s.logger.Info("开始扫描隐藏路径")
		hiddenPaths := s.hiddenPathDiscovery.DiscoverAllHiddenPaths()
		s.mutex.Lock()
		s.hiddenPaths = append(s.hiddenPaths, hiddenPaths...)
		s.mutex.Unlock()
		s.logger.Info("隐藏路径扫描完成", "count", len(hiddenPaths))
	} else {
		s.logger.Info("跳过隐藏路径扫描（EnableCommonPathScan=false）")
	}

	// 根据配置决定使用哪种爬虫策略
	if s.config.StrategySettings.EnableStaticCrawler {
		s.logger.Info("使用静态爬虫", "url", targetURL)
		result, err := s.staticCrawler.Crawl(parsedURL)
		if err != nil {
			s.logger.Error("静态爬虫失败", "url", targetURL, "error", err)
		} else {
			s.addResult(result)
			s.logger.Info("静态爬虫完成",
				"url", targetURL,
				"links", len(result.Links),
				"assets", len(result.Assets),
				"forms", len(result.Forms),
				"apis", len(result.APIs))
		}
	}

	// 如果启用了动态爬虫，总是使用（Phase 2/3优化：捕获AJAX和JS动态内容）
	if s.config.StrategySettings.EnableDynamicCrawler {
		s.logger.Info("使用动态爬虫", "url", targetURL, "mode", "ajax_intercept")
		result, err := s.dynamicCrawler.Crawl(parsedURL)
		if err != nil {
			s.logger.Error("动态爬虫失败", "url", targetURL, "error", err)
		} else {
			s.addResult(result)
			s.logger.Info("动态爬虫完成",
				"url", targetURL,
				"links", len(result.Links),
				"assets", len(result.Assets),
				"forms", len(result.Forms),
				"apis", len(result.APIs))
		}
	}

	// 参数爆破功能已移除，专注于纯爬虫
	// 不再生成参数爆破URL，只爬取真实发现的链接

	// 分析跨域JS文件（在递归爬取之前）
	s.processCrossDomainJS()

	// 如果启用了递归爬取，继续爬取发现的链接
	if s.config.DepthSettings.MaxDepth > 1 {
		// 🆕 v3.4: 支持四种调度算法
		// 1. BFS: 广度优先（默认，全面覆盖）
		// 2. DFS: 深度优先（快速深入）
		// 3. PRIORITY_QUEUE: 纯优先级队列（智能但可能遗漏）
		// 4. HYBRID: 混合策略（BFS框架+优先级排序，推荐）✨
		
		algorithm := strings.ToUpper(s.config.SchedulingSettings.Algorithm)
		
		// 如果未配置，向下兼容旧配置
		if algorithm == "" {
			if s.config.StrategySettings.UsePriorityQueue {
				algorithm = "PRIORITY_QUEUE"
			} else {
				algorithm = s.config.DepthSettings.SchedulingAlgorithm
			}
		}
		
		// 初始化自适应学习器（如果启用混合策略）
		if algorithm == "HYBRID" && s.config.SchedulingSettings.HybridConfig.EnableAdaptiveLearning {
			learningRate := s.config.SchedulingSettings.HybridConfig.LearningRate
			s.adaptiveLearner = NewAdaptivePriorityLearner(learningRate)
			s.logger.Info("自适应优先级学习器已启用", 
				"learning_rate", learningRate)
		}
		
		// 根据算法选择爬取策略
		switch algorithm {
		case "HYBRID":
			// 混合策略：BFS框架 + 智能优先级排序（推荐）
			s.logger.Info("使用混合调度策略", "algorithm", "HYBRID")
			s.crawlWithHybridStrategy()
			
		case "PRIORITY_QUEUE":
			// 纯优先级队列模式
			s.logger.Info("使用优先级队列模式", "algorithm", "PRIORITY_QUEUE")
			s.crawlWithPriorityQueue()
			
		case "DFS":
			// 深度优先（暂未实现，回退到BFS）
			s.logger.Warn("DFS模式暂未实现，回退到BFS", "algorithm", "DFS")
			s.crawlRecursivelyMultiLayer()
			
		default:
			// BFS模式（默认）
			s.logger.Info("使用BFS模式", "algorithm", "BFS")
			s.crawlRecursivelyMultiLayer()
		}
		
		// 打印自适应学习报告（如果启用）
		if s.adaptiveLearner != nil {
			s.adaptiveLearner.PrintReport()
		}
	}

	return nil
}

// shouldUseDynamicCrawler 判断是否需要使用动态爬虫
func (s *Spider) shouldUseDynamicCrawler() bool {
	// 如果没有发现足够的链接或API，可能需要动态爬虫
	if len(s.results) == 0 {
		return true
	}

	// 检查最近的结果
	lastResult := s.results[len(s.results)-1]
	// 降低触发动态爬虫的阈值（更容易触发）
	if len(lastResult.Links) < 20 && len(lastResult.APIs) < 10 {
		return true
	}

	return false
}

// isInTargetDomain 检查URL是否属于目标域名
func (s *Spider) isInTargetDomain(urlStr string) bool {
	// 忽略特殊协议
	if strings.HasPrefix(urlStr, "mailto:") || 
	   strings.HasPrefix(urlStr, "tel:") ||
	   strings.HasPrefix(urlStr, "javascript:") ||
	   strings.HasPrefix(urlStr, "data:") {
		return false
	}
	
	// 解析URL提取域名
	parsedURL, err := url.Parse(urlStr)
	if err != nil {
		return false
	}
	
	// 获取URL的域名（不含端口）
	urlHost := parsedURL.Hostname()
	if urlHost == "" {
		return false
	}
	
	// 目标域名（不含端口）
	targetHost := s.targetDomain
	
	// 完全匹配
	if urlHost == targetHost {
		return true
	}
	
	// 子域名匹配（例如：api.example.com 匹配 example.com）
	if strings.HasSuffix(urlHost, "."+targetHost) {
		return true
	}
	
	return false
}

// addResult 添加爬取结果（增强版：包含DOM相似度检测、技术栈检测、敏感信息检测和登录墙检测）
func (s *Spider) addResult(result *Result) {
	s.mutex.Lock()
	defer s.mutex.Unlock()
	
	// 🆕 v3.2 登录墙检测
	if s.loginWallDetector != nil && result != nil {
		s.loginWallDetector.RecordPage(result.URL, result.HTMLContent)
		
		// 如果登录墙占比过高，发出警告
		if s.loginWallDetector.ShouldWarn() && len(s.results) % 100 == 0 {
			// 每100个页面检查一次
			s.loginWallDetector.PrintWarning()
		}
	}
	
	// 🆕 v3.5 POST请求检测（增强版）
	if s.postDetector != nil && result != nil && result.HTMLContent != "" {
		detectedPOST := s.postDetector.DetectFromHTML(result.HTMLContent, result.URL)
		if len(detectedPOST) > 0 {
			// 转换为Result中的POSTRequest格式
			for _, dp := range detectedPOST {
				postReq := POSTRequest{
					URL:         dp.URL,
					Method:      dp.Method,
					Parameters:  dp.Parameters,
					ContentType: dp.ContentType,
					Body:        "",
					FromForm:    dp.Source == "html-form",
					FormAction:  dp.URL,
				}
				result.POSTRequests = append(result.POSTRequests, postReq)
			}
			
			s.logger.Info("检测到POST请求",
				"page_url", result.URL,
				"post_count", len(detectedPOST))
		}
	}
	
	// 🆕 将域内URL添加到去重器（只添加目标域名的URL）
	if s.urlDeduplicator != nil && result != nil {
		// 添加当前页面URL
		if result.URL != "" && s.isInTargetDomain(result.URL) {
			s.urlDeduplicator.AddURL(result.URL)
		}
		
		// 添加发现的所有链接（只添加域内的）
		if len(result.Links) > 0 {
			for _, link := range result.Links {
				if s.isInTargetDomain(link) {
					s.urlDeduplicator.AddURL(link)
				}
			}
		}
		
		// 添加API端点（只添加域内的）
		if len(result.APIs) > 0 {
			for _, api := range result.APIs {
				if s.isInTargetDomain(api) {
					s.urlDeduplicator.AddURL(api)
				}
			}
		}
		
		// 添加表单action（只添加域内的）
		for _, form := range result.Forms {
			if form.Action != "" && s.isInTargetDomain(form.Action) {
				s.urlDeduplicator.AddURL(form.Action)
			}
		}
		
		// 添加POST请求URL（只添加域内的）
		for _, postReq := range result.POSTRequests {
			if postReq.URL != "" && s.isInTargetDomain(postReq.URL) {
				s.urlDeduplicator.AddURL(postReq.URL)
			}
		}
	}

	// 如果有HTML内容，先进行DOM相似度检测
	if result.HTMLContent != "" && s.domSimilarity != nil {
		isSimilar, record := s.domSimilarity.CheckSimilarity(result.URL, result.HTMLContent)
		if isSimilar && record != nil {
			fmt.Printf("  [DOM相似度] 发现相似页面！相似度: %.1f%%, 相似于: %s\n",
				record.Similarity*100, record.SimilarToURL)
			fmt.Printf("  [DOM相似度] 原因: %s\n", record.Reason)
			fmt.Printf("  [DOM相似度] ✓ 跳过重复爬取，节省资源\n")
			// 相似页面仍然记录，但标记为已跳过
			result.IsSimilar = true
			result.SimilarToURL = record.SimilarToURL
		}
	}

	s.results = append(s.results, result)

	// 如果有HTML内容，进行高级检测
	if result.HTMLContent != "" {
		// 技术栈检测
		if s.techDetector != nil {
			techs := s.techDetector.DetectFromContent(result.HTMLContent, result.Headers)
			s.detectedTechs = append(s.detectedTechs, techs...)

			if len(techs) > 0 {
				techNames := make([]string, 0)
				for _, tech := range techs {
					if tech.Version != "" {
						techNames = append(techNames, tech.Name+" "+tech.Version)
					} else {
						techNames = append(techNames, tech.Name)
					}
				}
				fmt.Printf("  [技术栈] 检测到: %s\n", strings.Join(techNames, ", "))
			}
		}

		// 敏感信息检测（根据配置决定是否启用）
		if s.config.SensitiveDetectionSettings.Enabled && s.sensitiveDetector != nil {
			findings := make([]*SensitiveInfo, 0)
			
			// 扫描HTML内容（根据配置）
			if s.config.SensitiveDetectionSettings.ScanResponseBody {
				bodyFindings := s.sensitiveDetector.Scan(result.HTMLContent, result.URL)
				findings = append(findings, bodyFindings...)
				s.sensitiveFindings = append(s.sensitiveFindings, bodyFindings...)
			}

			// 扫描HTTP头（根据配置）
			if s.config.SensitiveDetectionSettings.ScanResponseHeaders && len(result.Headers) > 0 {
				headerContent := ""
				for key, value := range result.Headers {
					headerContent += key + ": " + value + "\n"
				}
				headerFindings := s.sensitiveDetector.Scan(headerContent, result.URL+" (Headers)")
				s.sensitiveFindings = append(s.sensitiveFindings, headerFindings...)
				findings = append(findings, headerFindings...)
			}

			// 实时输出（根据配置）
			if s.config.SensitiveDetectionSettings.RealTimeOutput && len(findings) > 0 {
				// 按严重级别过滤
				filteredFindings := s.filterBySeverity(findings)
				
				if len(filteredFindings) > 0 {
					highCount := 0
					for _, finding := range filteredFindings {
						if finding.Severity == "HIGH" {
							highCount++
						}
					}

					if highCount > 0 {
						fmt.Printf("  [敏感信息] ⚠️  发现 %d 处高危敏感信息！\n", highCount)
					} else {
						fmt.Printf("  [敏感信息] 发现 %d 处敏感信息\n", len(filteredFindings))
					}
				}
			}
		}
	}
	
	// v2.7: 业务感知过滤器 - 自适应学习
	if s.config.DeduplicationSettings.EnableBusinessAwareFilter && 
	   s.config.DeduplicationSettings.BusinessFilterAdaptiveLearning {
		// 计算响应时间（如果可用）
		responseTime := 0.0
		
		// 检查是否发现新内容
		hasNewLinks := len(result.Links) > 0
		hasNewForms := len(result.Forms) > 0
		hasNewAPIs := len(result.APIs) > 0
		
		// 更新爬取结果，用于自适应学习
		s.businessFilter.UpdateCrawlResult(
			result.URL,
			result.StatusCode,
			responseTime,
			hasNewLinks,
			hasNewForms,
			hasNewAPIs,
		)
	}
}

// addResultWithDetection 添加结果并进行检测
func (s *Spider) addResultWithDetection(result *Result, response *http.Response, htmlContent string) {
	s.mutex.Lock()
	defer s.mutex.Unlock()

	s.results = append(s.results, result)

	// 技术栈检测
	if response != nil && s.techDetector != nil {
		techs := s.techDetector.Detect(response, htmlContent)
		s.detectedTechs = append(s.detectedTechs, techs...)

		if len(techs) > 0 {
			fmt.Printf("  [技术栈] 检测到: ")
			techNames := make([]string, 0)
			for _, tech := range techs {
				if tech.Version != "" {
					techNames = append(techNames, tech.Name+" "+tech.Version)
				} else {
					techNames = append(techNames, tech.Name)
				}
			}
			fmt.Printf("%s\n", strings.Join(techNames, ", "))
		}
	}

	// 敏感信息检测
	if s.sensitiveDetector != nil {
		findings := s.sensitiveDetector.Scan(htmlContent, result.URL)
		s.sensitiveFindings = append(s.sensitiveFindings, findings...)

		if len(findings) > 0 {
			highCount := 0
			for _, finding := range findings {
				if finding.Severity == "HIGH" {
					highCount++
				}
			}

			if highCount > 0 {
				fmt.Printf("  [敏感信息] ⚠️  发现 %d 处高危敏感信息！\n", highCount)
			} else {
				fmt.Printf("  [敏感信息] 发现 %d 处敏感信息\n", len(findings))
			}
		}
	}

	// 子域名提取
	if s.subdomainExtractor != nil && htmlContent != "" {
		// 从HTML内容提取子域名
		subdomains := s.subdomainExtractor.ExtractFromHTML(htmlContent)
		if len(subdomains) > 0 {
			fmt.Printf("  [子域名] 发现 %d 个新子域名\n", len(subdomains))
		}

		// 从URL本身提取
		s.subdomainExtractor.ExtractFromURL(result.URL)
	}
}

// GetResults 获取所有爬取结果
func (s *Spider) GetResults() []*Result {
	s.mutex.Lock()
	defer s.mutex.Unlock()

	// v2.6.1: 打印智能参数值去重统计
	if s.config.DeduplicationSettings.EnableSmartParamDedup && s.smartParamDedup != nil {
		s.smartParamDedup.PrintStatistics()
	}

	// 返回结果副本
	results := make([]*Result, len(s.results))
	copy(results, s.results)
	return results
}

// processParams 已废弃（参数爆破功能已移除）
// 保留定义以避免编译错误，但不再使用
func (s *Spider) processParams(rawURL string) []string {
	// 参数爆破功能已移除，直接返回原始URL
	return []string{rawURL}
}

/*
// 以下是原 processParams 的代码（已废弃）
func (s *Spider) processParamsOLD(rawURL string) []string {
	// 提取参数
	params, err := s.paramHandler.ExtractParams(rawURL)
	if err != nil {
		// 如果提取参数失败，至少返回原始URL
		return []string{rawURL}
	}

	// ===  修复：防止参数爆破无限递归 ===
	// 检测URL是否可能是参数爆破生成的（避免对爆破结果再次爆破）
	if len(params) > 0 && s.config.StrategySettings.EnableParamFuzzing {
		// 完整的参数爆破参数列表（来自GenerateParameterFuzzList）
		fuzzedParamNames := []string{
			// 通用参数
			"id", "page", "limit", "offset", "sort", "order", "search", "q", "query",
			"filter", "category", "type", "status", "action", "method", "format",
			// 用户相关
			"user", "username", "userid", "uid", "email", "password", "pass", "pwd",
			"token", "auth", "session", "key", "api_key", "access_token",
			// 文件相关
			"file", "filename", "path", "dir", "folder", "upload", "download",
			"image", "img", "pic", "photo", "document", "doc", "pdf",
			// 其他常见参数
			"debug", "test", "dev", "admin", "config",
		}

		// 测试值列表（用于检测参数值是否是测试值）
		testValues := []string{"1", "test", "admin", "null", "../", "", "false", "true"}

		fuzzedParamCount := 0
		originalParamCount := 0
		testValueCount := 0

		parsedURL, _ := url.Parse(rawURL)
		if parsedURL != nil {
			queryParams := parsedURL.Query()
			for paramName, values := range queryParams {
				// 检查参数名是否是爆破参数
				isFuzzedParam := false
				for _, fuzzName := range fuzzedParamNames {
					if paramName == fuzzName {
						isFuzzedParam = true
						fuzzedParamCount++
						break
					}
				}
				if !isFuzzedParam {
					originalParamCount++
				}

				// 检查参数值是否是测试值
				if len(values) > 0 {
					paramValue := values[0]
					for _, testVal := range testValues {
						if paramValue == testVal || strings.Contains(paramValue, testVal) {
							testValueCount++
							break
						}
					}
				}
			}

			// 判断是否是爆破生成的URL（优化后的检测规则）：
			// 核心原则：只跳过"纯粹由爆破生成"的URL，不误杀真实URL
			//
			// 规则1：包含2个以上爆破参数（如 ?id=1&page=1）
			// 规则2：只有爆破参数，没有原始参数（如 ?search=1, ?limit=）
			//
			// 不跳过的情况：
			// - 真实参数+爆破参数的组合（如 ?article_id=123&id=1）
			// - 真实参数碰巧值是测试值（如 ?article_id=1）
			shouldSkip := false
			skipReason := ""

			if fuzzedParamCount >= 2 {
				// 规则1：多个爆破参数，明显是爆破生成的
				shouldSkip = true
				skipReason = fmt.Sprintf("包含%d个爆破参数", fuzzedParamCount)
			} else if fuzzedParamCount >= 1 && originalParamCount == 0 {
				// 规则2：只有爆破参数，没有原始参数
				// 例如：?search=1, ?limit=, ?id=1
				shouldSkip = true
				skipReason = fmt.Sprintf("只包含爆破参数（%d个），无原始参数", fuzzedParamCount)
			}

			if shouldSkip {
				fmt.Printf("为URL %s 生成 %d 个参数变体\n", rawURL, 1)
				fmt.Printf("  变体: %s\n", rawURL)
				fmt.Printf("  [参数爆破] 检测到该URL可能是爆破生成的（%s），跳过再次爆破\n", skipReason)
				return []string{rawURL}
			}
		}
	}

	// === 新增：对无参数URL进行参数爆破 ===
	if len(params) == 0 && s.config.StrategySettings.EnableParamFuzzing {
		fmt.Printf("  [参数爆破] 检测到无参数URL，开始参数枚举...\n")

		// 生成参数爆破列表
		fuzzList := s.paramHandler.GenerateParameterFuzzList(rawURL)

		// 应用限制（避免生成过多URL）
		if s.config.StrategySettings.ParamFuzzLimit > 0 && len(fuzzList) > s.config.StrategySettings.ParamFuzzLimit {
			fuzzList = fuzzList[:s.config.StrategySettings.ParamFuzzLimit]
			fmt.Printf("  [参数爆破] 限制爆破数量为 %d 个（可配置）\n", s.config.StrategySettings.ParamFuzzLimit)
		}

		if len(fuzzList) > 0 {
			fmt.Printf("  [参数爆破] 为无参数URL生成 %d 个参数爆破变体\n", len(fuzzList))
			fmt.Printf("  [参数爆破] 示例: %s\n", fuzzList[0])
			if len(fuzzList) > 1 {
				fmt.Printf("  [参数爆破] 示例: %s\n", fuzzList[1])
			}
			if len(fuzzList) > 2 {
				fmt.Printf("  [参数爆破] 示例: %s\n", fuzzList[2])
			}
			fmt.Printf("  [参数爆破] ... 还有 %d 个爆破URL\n", len(fuzzList)-3)

			return fuzzList
		}

		// 如果爆破失败，返回原始URL
		return []string{rawURL}
	}

	// === 原有逻辑：对有参数URL进行变体生成（纯爬虫模式，不含攻击payload） ===
	// 对每个参数进行信息分析（仅记录，不用于攻击）
	for paramName := range params {
		risk, level := s.paramHandler.AnalyzeParameterSecurity(paramName)
		if level >= 2 { // 中等风险以上
			finding := fmt.Sprintf("PARAM_INFO: %s - %s (Risk Level: %d)", paramName, risk, level)
			s.mutex.Lock()
			s.securityFindings = append(s.securityFindings, finding)
			s.mutex.Unlock()
			fmt.Printf("  [参数分析] %s\n", finding)
		}
	}

	// 生成参数变体（只使用正常值，不含攻击payload）
	variations := s.paramHandler.GenerateParamVariations(rawURL)

	// === 移除：安全测试变体（攻击性payload） ===
	// 作为纯爬虫工具，不应发送SQL注入、XSS等攻击性payload
	// securityVariations := s.paramHandler.GenerateSecurityTestVariations(rawURL)
	// variations = append(variations, securityVariations...)

	// 如果没有生成变体，返回原始URL
	if len(variations) == 0 {
		return []string{rawURL}
	}

	// 打印生成的变体
	fmt.Printf("  [参数变体] 为URL生成 %d 个参数变体（正常测试值）\n", len(variations))
	for i, variation := range variations {
		if i < 5 { // 只显示前5个，避免输出过多
			fmt.Printf("    变体: %s\n", variation)
		}
	}
	if len(variations) > 5 {
		fmt.Printf("    ... 还有 %d 个变体\n", len(variations)-5)
	}

	return variations
}
*/

// processForms 已废弃（POST参数爆破功能已移除）
// 保留定义以避免编译错误，但不再使用
func (s *Spider) processForms(targetURL string) {
	// POST参数爆破功能已移除
	return
}

/*
// 以下是原 processForms 的代码（已废弃）
func (s *Spider) processFormsOLD(targetURL string) {
	// 检查是否启用POST参数爆破
	if !s.config.StrategySettings.EnablePOSTParamFuzzing {
		return
	}

	s.mutex.Lock()
	defer s.mutex.Unlock()

	// 收集所有空表单或无效表单
	emptyForms := make([]string, 0)
	totalForms := 0

	for _, result := range s.results {
		totalForms += len(result.Forms)

		for _, form := range result.Forms {
			// 检查表单是否有有效字段
			hasValidFields := false
			for _, field := range form.Fields {
				// 跳过提交按钮和普通按钮
				fieldTypeLower := strings.ToLower(field.Type)
				if fieldTypeLower != "submit" && fieldTypeLower != "button" && field.Name != "" {
					hasValidFields = true
					break
				}
			}

			// 如果表单没有有效字段，添加到爆破列表
			if !hasValidFields {
				emptyForms = append(emptyForms, form.Action)
			}
		}

		// 同样检查POST请求
		for _, postReq := range result.POSTRequests {
			// 如果POST请求没有参数或参数为空
			if len(postReq.Parameters) == 0 {
				emptyForms = append(emptyForms, postReq.URL)
			}
		}
	}

	// 去重
	uniqueEmptyForms := make(map[string]bool)
	for _, formURL := range emptyForms {
		uniqueEmptyForms[formURL] = true
	}

	if len(uniqueEmptyForms) == 0 {
		if totalForms > 0 {
			fmt.Printf("  [POST爆破] 发现 %d 个表单，全部有字段，无需爆破\n", totalForms)
		}
		return
	}

	fmt.Printf("  [POST爆破] 检测到 %d 个空表单，开始POST参数爆破...\n", len(uniqueEmptyForms))

	// 对每个空表单生成POST爆破请求
	totalPOSTFuzzRequests := 0
	for formURL := range uniqueEmptyForms {
		// 生成POST参数爆破列表
		postFuzzList := s.paramHandler.GeneratePOSTParameterFuzzList(formURL)

		// 应用限制
		if s.config.StrategySettings.POSTParamFuzzLimit > 0 && len(postFuzzList) > s.config.StrategySettings.POSTParamFuzzLimit {
			postFuzzList = postFuzzList[:s.config.StrategySettings.POSTParamFuzzLimit]
		}

		if len(postFuzzList) > 0 {
			// 添加到第一个结果的POSTRequests中
			if len(s.results) > 0 {
				s.results[0].POSTRequests = append(s.results[0].POSTRequests, postFuzzList...)
			}
			totalPOSTFuzzRequests += len(postFuzzList)
		}
	}

	if totalPOSTFuzzRequests > 0 {
		fmt.Printf("  [POST爆破] 为 %d 个空表单生成 %d 个POST爆破请求\n", len(uniqueEmptyForms), totalPOSTFuzzRequests)
		fmt.Printf("  [POST爆破] 示例: POST %s {username=admin, password=admin123}\n", emptyForms[0])
		if len(uniqueEmptyForms) > 1 {
			fmt.Printf("  [POST爆破] 示例: POST %s {search=test, q=admin}\n", emptyForms[0])
		}
	}
}
*/

// processCrossDomainJS 处理跨域JS文件
func (s *Spider) processCrossDomainJS() {
	fmt.Println("\n开始分析跨域JS文件...")

	// 收集所有资源链接
	allAssets := make(map[string]bool)

	s.mutex.Lock()
	for _, result := range s.results {
		for _, asset := range result.Assets {
			allAssets[asset] = true
		}
		// 也检查Links中的JS文件
		for _, link := range result.Links {
			if strings.HasSuffix(strings.ToLower(link), ".js") {
				allAssets[link] = true
			}
		}
	}
	s.mutex.Unlock()

	// 过滤出需要分析的JS文件
	jsToAnalyze := make([]string, 0)
	for asset := range allAssets {
		// 检查是否为JS文件
		if !strings.HasSuffix(strings.ToLower(asset), ".js") {
			continue
		}

		// 解析URL
		parsedURL, err := url.Parse(asset)
		if err != nil {
			continue
		}

		domain := parsedURL.Host
		if domain == "" {
			continue
		}

		// 检查是否需要分析
		shouldAnalyze := false
		reason := ""

		// 1. 是目标域名 - 已经正常爬取了，不需要特殊处理
		if domain == s.targetDomain {
			continue
		}

		// 2. 是同源域名
		if s.cdnDetector.IsSameBaseDomain(domain, s.targetDomain) {
			shouldAnalyze = true
			reason = "同源域名"
		}

		// 3. 是已知CDN
		if s.cdnDetector.IsCDN(domain) {
			shouldAnalyze = true
			cdnInfo := s.cdnDetector.GetCDNInfo(domain)
			reason = cdnInfo
		}

		if shouldAnalyze {
			jsToAnalyze = append(jsToAnalyze, asset)
			fmt.Printf("  发现跨域JS: %s (%s)\n", asset, reason)
		}
	}

	if len(jsToAnalyze) == 0 {
		fmt.Println("未发现需要分析的跨域JS文件")
		return
	}

	fmt.Printf("准备分析 %d 个跨域JS文件...\n", len(jsToAnalyze))

	// 分析每个JS文件
	totalURLsFound := 0
	for _, jsURL := range jsToAnalyze {
		urls := s.analyzeExternalJS(jsURL)
		if len(urls) > 0 {
			fmt.Printf("  从 %s 提取了 %d 个URL\n", jsURL, len(urls))
			totalURLsFound += len(urls)

			// 添加到跨域JS发现列表
			s.mutex.Lock()
			s.crossDomainJS = append(s.crossDomainJS, urls...)
			s.mutex.Unlock()

			// 添加到爬取队列（如果启用递归爬取）
			if s.config.DepthSettings.MaxDepth > 1 {
				addedCount := 0
				filteredCount := 0
				for _, u := range urls {
					// 🆕 v3.5: 使用URL验证器过滤无效URL
					if s.urlValidator != nil && !s.urlValidator.IsValidBusinessURL(u) {
						filteredCount++
						continue
					}
					
					// 添加到结果中（作为发现的链接）
					if len(s.results) > 0 {
						s.results[0].Links = append(s.results[0].Links, u)
						addedCount++
					}
				}
				if filteredCount > 0 {
					fmt.Printf("    [跨域JS过滤] 过滤了 %d 个无效URL，保留 %d 个有效URL\n", filteredCount, addedCount)
				}
			}
		}
	}

	fmt.Printf("跨域JS分析完成！共从 %d 个JS文件中提取了 %d 个目标域名URL\n\n", len(jsToAnalyze), totalURLsFound)
}

// analyzeExternalJS 下载并分析外部JS文件（使用性能优化）
func (s *Spider) analyzeExternalJS(jsURL string) []string {
	// 使用性能优化的HTTP客户端
	req, err := http.NewRequest("GET", jsURL, nil)
	if err != nil {
		fmt.Printf("    创建请求失败: %v\n", err)
		return []string{}
	}

	// 设置User-Agent
	if len(s.config.AntiDetectionSettings.UserAgents) > 0 {
		req.Header.Set("User-Agent", s.config.AntiDetectionSettings.UserAgents[0])
	} else {
		req.Header.Set("User-Agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
	}

	// 使用优化的HTTP客户端（带连接池）
	resp, err := s.perfOptimizer.DoRequest(req)
	if err != nil {
		fmt.Printf("    下载失败: %v\n", err)
		return []string{}
	}
	defer resp.Body.Close()

	// 检查状态码
	if resp.StatusCode != 200 {
		fmt.Printf("    HTTP %d\n", resp.StatusCode)
		return []string{}
	}

	// 使用Buffer池读取内容
	buf := s.perfOptimizer.GetBuffer()
	defer s.perfOptimizer.PutBuffer(buf)

	// 限制文件大小（最大5MB）
	const maxSize = 5 * 1024 * 1024
	limitedReader := &io.LimitedReader{R: resp.Body, N: maxSize}

	_, err = buf.ReadFrom(limitedReader)
	if err != nil {
		fmt.Printf("    读取内容失败: %v\n", err)
		return []string{}
	}

	// 使用增强的JS分析器提取URL
	jsCode := buf.String()

	// 使用增强分析
	enhancedResult := s.jsAnalyzer.EnhancedAnalyze(jsCode)

	// 合并所有发现的URL
	urls := make([]string, 0)
	seen := make(map[string]bool)
	relativeURLs := make([]string, 0)  // ✅ 修复9: 收集相对URL

	for category, categoryURLs := range enhancedResult {
		for _, discoveredURL := range categoryURLs {
			if !seen[discoveredURL] {
				seen[discoveredURL] = true
				
				// ✅ 修复9: 检查是否为相对URL或路径
				if isRelativeURL(discoveredURL) {
					relativeURLs = append(relativeURLs, discoveredURL)
				} else {
					urls = append(urls, discoveredURL)
				}
			}
		}

		// 打印各类别的发现
		if len(categoryURLs) > 0 {
			fmt.Printf("    [%s] 发现 %d 个URL\n", category, len(categoryURLs))
		}
	}
	
	// ✅ 修复9: 将相对URL与目标域名拼接
	if len(relativeURLs) > 0 {
		fmt.Printf("    [CDN JS拼接] 发现 %d 个相对URL,与目标域名拼接...\n", len(relativeURLs))
		baseURL := fmt.Sprintf("https://%s", s.targetDomain)
		if parsedBase, err := url.Parse(baseURL); err == nil {
			for _, relURL := range relativeURLs {
				absoluteURL := parsedBase.ResolveReference(&url.URL{Path: relURL})
				fullURL := absoluteURL.String()
				if !seen[fullURL] {
					seen[fullURL] = true
					urls = append(urls, fullURL)
					fmt.Printf("      拼接: %s + %s = %s\n", baseURL, relURL, fullURL)
				}
			}
		}
	}

	return urls
}

// crawlRecursively 递归爬取发现的链接（单层爬取，已废弃）
// 请使用 crawlRecursivelyMultiLayer
func (s *Spider) crawlRecursively() {
	s.crawlRecursivelyMultiLayer()
}

// crawlRecursivelyMultiLayer 真正的多层递归爬取（修复深度问题）
func (s *Spider) crawlRecursivelyMultiLayer() {
	fmt.Println("开始多层递归爬取...")

	currentDepth := 1
	totalCrawled := 0

	// 循环爬取每一层，直到达到最大深度
	for currentDepth < s.config.DepthSettings.MaxDepth {
		currentDepth++

		fmt.Printf("\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n")
		fmt.Printf("【第 %d 层爬取】最大深度: %d\n", currentDepth, s.config.DepthSettings.MaxDepth)
		fmt.Printf("━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n")

		// 收集当前层需要爬取的链接
		layerLinks := s.collectLinksForLayer(currentDepth)

		if len(layerLinks) == 0 {
			fmt.Printf("第 %d 层没有新链接，递归结束\n", currentDepth)
			break
		}

		fmt.Printf("第 %d 层准备爬取 %d 个链接...\n", currentDepth, len(layerLinks))

		// 爬取当前层的所有链接
		newResults := s.crawlLayer(layerLinks, currentDepth)

		// 合并结果
		s.mutex.Lock()
		s.results = append(s.results, newResults...)
		s.mutex.Unlock()

		totalCrawled += len(layerLinks)
		fmt.Printf("第 %d 层爬取完成！本层爬取 %d 个URL，累计 %d 个\n",
			currentDepth, len(layerLinks), totalCrawled)

		// 检查是否达到URL限制
		if totalCrawled >= 500 {
			fmt.Printf("已达到URL限制(500)，递归结束\n")
			break
		}
	}

	fmt.Printf("\n多层递归爬取完成！总共爬取 %d 个URL，深度 %d 层\n", totalCrawled, currentDepth)
}

// collectLinksForLayer 收集指定层需要爬取的链接
func (s *Spider) collectLinksForLayer(targetDepth int) []string {
	allLinks := make(map[string]bool)
	externalLinks := make([]string, 0)

	s.mutex.Lock()
	// 从所有结果中收集链接
	for _, result := range s.results {
		for _, link := range result.Links {
			// 检查是否已访问
			if s.visitedURLs[link] {
				continue
			}

			// 解析链接
			parsedURL, err := url.Parse(link)
			if err != nil {
				continue
			}

			// 作用域检查
			inScope, _ := s.advancedScope.InScope(link)
			if inScope {
				// 规范化URL
				normalizedURL, err := s.paramHandler.NormalizeURL(link)
				if err == nil {
					allLinks[normalizedURL] = true
				} else {
					allLinks[link] = true
				}
			} else {
				if parsedURL.Host != s.targetDomain && parsedURL.Host != "" {
					externalLinks = append(externalLinks, link)
				}
			}
		}
	}
	s.mutex.Unlock()

	// 记录外部链接
	if len(externalLinks) > 0 {
		s.mutex.Lock()
		s.externalLinks = append(s.externalLinks, externalLinks...)
		s.mutex.Unlock()
		fmt.Printf("  发现 %d 个外部链接（已记录但不爬取）\n", len(externalLinks))
	}

	// 转换为列表并优先级排序
	tasksToSubmit := make([]string, 0)
	skippedBySmart := 0 // 统计智能去重跳过的数量
	skippedByBusiness := 0 // 统计业务感知过滤器跳过的数量
	skippedByPattern := 0 // 统计URL模式去重跳过的数量
	skippedByResourceType := 0 // 🆕 统计资源分类跳过的数量（静态资源/域外）
	skippedByLoginWall := 0 // 🆕 v3.2 统计登录墙跳过的数量
	
	for link := range allLinks {
		// 🆕 v3.2: 登录墙检测 - 跳过重复的登录页面变体
		if s.loginWallDetector != nil {
			shouldSkip, reason := s.loginWallDetector.ShouldSkipURL(link)
			if shouldSkip {
				skippedByLoginWall++
				if skippedByLoginWall <= 3 {
					s.logger.Debug("登录墙过滤",
						"url", link,
						"reason", reason)
				}
				continue
			}
		}
		
		// 🆕 v3.1: 使用exclude_extensions判断是否需要请求
		// JS/CSS文件始终请求，其他被排除的扩展名只记录不请求
		if s.scopeController != nil {
			shouldRequest, reason := s.scopeController.ShouldRequestURL(link)
			if !shouldRequest {
				skippedByResourceType++
				if skippedByResourceType <= 5 {
					s.logger.Debug("扩展名过滤：只记录不请求",
						"url", link,
						"reason", reason)
				}
				// URL已被记录（在addResult中），这里只是跳过HTTP请求
				continue
			}
		}
		
		// v2.9: URL模式去重检查
		shouldProcess, _, reason := s.urlPatternDedup.ShouldProcess(link, "GET")
		if !shouldProcess {
			skippedByPattern++
			if skippedByPattern <= 3 { // 只打印前3个，避免日志过多
				s.logger.Debug("URL模式去重跳过",
					"url", link,
					"reason", reason)
			}
			continue
		}
		
		// 去重检查
		if s.duplicateHandler.IsDuplicateURL(link) {
			continue
		}

		// v2.6.1: 智能参数值去重检查
		if s.config.DeduplicationSettings.EnableSmartParamDedup {
			shouldCrawl, reason := s.smartParamDedup.ShouldCrawl(link)
			if !shouldCrawl {
				skippedBySmart++
				if skippedBySmart <= 5 { // 只打印前5个，避免日志过多
					fmt.Printf("  [智能去重] 跳过: %s\n  原因: %s\n", link, reason)
				}
				continue
			}
		}

		// v2.7: 业务感知过滤检查
		if s.config.DeduplicationSettings.EnableBusinessAwareFilter {
			shouldCrawl, reason, score := s.businessFilter.ShouldCrawlURL(link)
			if !shouldCrawl {
				skippedByBusiness++
				if skippedByBusiness <= 5 { // 只打印前5个，避免日志过多
					s.logger.Debug("业务感知过滤跳过URL",
						"url", link,
						"reason", reason,
						"score", score)
				}
				continue
			}
			// 记录高价值URL
			if score >= s.config.DeduplicationSettings.BusinessFilterHighValueThreshold {
				s.logger.Info("发现高价值URL",
					"url", link,
					"score", score,
					"reason", reason)
			}
		}

		// 验证格式
		if !IsValidURL(link) {
			continue
		}

		tasksToSubmit = append(tasksToSubmit, link)

		// 每层限制100个URL
		if len(tasksToSubmit) >= 100 {
			break
		}
	}

	// v2.6.1: 打印智能去重统计
	if skippedBySmart > 0 {
		fmt.Printf("  [智能去重] 本层跳过 %d 个相似参数值URL\n", skippedBySmart)
	}
	
	// v2.7: 打印业务感知过滤统计
	if skippedByBusiness > 0 {
		fmt.Printf("  [业务感知] 本层过滤 %d 个低价值URL\n", skippedByBusiness)
	}
	
	// v2.9: 打印URL模式去重统计
	if skippedByPattern > 0 {
		fmt.Printf("  [URL模式去重] 本层跳过 %d 个重复模式URL\n", skippedByPattern)
	}
	
	// 🆕 v3.1: 打印扩展名过滤统计
	if skippedByResourceType > 0 {
		fmt.Printf("  [扩展名过滤] 本层跳过 %d 个静态资源URL（已记录不请求，JS/CSS除外）\n", skippedByResourceType)
	}
	
	// 🆕 v3.2: 打印登录墙过滤统计
	if skippedByLoginWall > 0 {
		fmt.Printf("  [登录墙过滤] 本层跳过 %d 个重复的登录页面变体\n", skippedByLoginWall)
	}

	// 优先级排序（🆕 传入实际深度，用于精确优先级计算）
	tasksToSubmit = s.prioritizeURLsWithDepth(tasksToSubmit, targetDepth)

	return tasksToSubmit
}

// prioritizeURLsWithDepth 带深度参数的优先级排序
func (s *Spider) prioritizeURLsWithDepth(urls []string, depth int) []string {
	// 如果有优先级调度器，使用精确计算（混合决策模式）
	if s.priorityScheduler != nil {
		return s.prioritizeURLsWithPreciseCalculation(urls, depth)
	}
	
	// 否则使用简单分类
	return s.prioritizeURLs(urls)
}

// crawlLayer 爬取一层的所有链接
func (s *Spider) crawlLayer(links []string, depth int) []*Result {
	results := make([]*Result, 0)

	// 标记为已访问
	s.mutex.Lock()
	for _, link := range links {
		s.visitedURLs[link] = true
	}
	s.mutex.Unlock()

	// 为每层创建新的工作池（修复：避免复用已关闭的工作池）
	layerWorkerPool := NewWorkerPool(30, 20)

	// 启动工作池
	layerWorkerPool.Start(func(task Task) (*Result, error) {
		return s.crawlURL(task.URL)
	})

	// 提交所有任务
	for _, link := range links {
		task := Task{
			URL:   link,
			Depth: depth,
		}
		if err := layerWorkerPool.Submit(task); err != nil {
			fmt.Printf("  提交任务失败 %s: %v\n", link, err)
		}
	}

	// 等待完成（不显示进度，避免干扰）
	layerWorkerPool.Wait()

	// 收集结果
	results = layerWorkerPool.GetResults()

	// 停止工作池
	layerWorkerPool.Stop()

	// 显示统计
	stats := layerWorkerPool.GetStats()
	fmt.Printf("  本层统计 - 总任务: %d, 成功: %d, 失败: %d\n",
		stats["total"], stats["completed"]-stats["failed"], stats["failed"])

	return results
}

// crawlURL 爬取单个URL（供工作池使用）
func (s *Spider) crawlURL(targetURL string) (*Result, error) {
	// 解析URL
	parsedURL, err := url.Parse(targetURL)
	if err != nil {
		return nil, fmt.Errorf("URL解析失败: %v", err)
	}

	// 使用静态爬虫
	result, err := s.staticCrawler.Crawl(parsedURL)
	if err != nil {
		// 如果静态爬虫失败，尝试动态爬虫
		if s.config.StrategySettings.EnableDynamicCrawler {
			result, err = s.dynamicCrawler.Crawl(parsedURL)
			if err != nil {
				return nil, fmt.Errorf("爬取失败: %v", err)
			}
		} else {
			return nil, fmt.Errorf("静态爬虫失败: %v", err)
		}
	}

	return result, nil
}

// showProgress 显示爬取进度
func (s *Spider) showProgress() {
	ticker := time.NewTicker(2 * time.Second)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			progress := s.workerPool.GetProgress()
			stats := s.workerPool.GetStats()

			// 计算进度条
			barWidth := 30
			filled := int(progress * float64(barWidth) / 100)
			bar := strings.Repeat("█", filled) + strings.Repeat("░", barWidth-filled)

			fmt.Printf("\r[进度] %s %.1f%% (%d/%d)", bar, progress, stats["completed"], stats["total"])

			if stats["completed"] >= stats["total"] {
				fmt.Println()
				return
			}
		}
	}
}

// ImportFromBurp 从Burp Suite文件导入
func (s *Spider) ImportFromBurp(filename string) error {
	fmt.Printf("从Burp Suite导入流量: %s\n", filename)

	// 创建被动爬取器
	s.passiveCrawler = NewPassiveCrawler("burp")

	// 加载Burp文件
	err := s.passiveCrawler.LoadFromBurp(filename)
	if err != nil {
		return err
	}

	// 过滤目标域名的URL
	targetURLs := s.passiveCrawler.FilterByDomain(s.targetDomain)
	fmt.Printf("过滤后得到目标域名URL: %d个\n", len(targetURLs))

	// 将导入的URL和表单加入结果
	passiveResult := s.passiveCrawler.ExportToResult(s.targetDomain)
	s.addResult(passiveResult)

	return nil
}

// ImportFromHAR 从HAR文件导入
func (s *Spider) ImportFromHAR(filename string) error {
	fmt.Printf("从HAR文件导入流量: %s\n", filename)

	// 创建被动爬取器
	s.passiveCrawler = NewPassiveCrawler("har")

	// 加载HAR文件
	err := s.passiveCrawler.LoadFromHAR(filename)
	if err != nil {
		return err
	}

	// 过滤目标域名的URL
	targetURLs := s.passiveCrawler.FilterByDomain(s.targetDomain)
	fmt.Printf("过滤后得到目标域名URL: %d个\n", len(targetURLs))

	// 将导入的URL和表单加入结果
	passiveResult := s.passiveCrawler.ExportToResult(s.targetDomain)
	s.addResult(passiveResult)

	return nil
}

// Stop 停止爬取
func (s *Spider) Stop() {
	fmt.Println("停止爬取...")
	s.staticCrawler.Stop()
	s.dynamicCrawler.Stop()

	// 关闭性能优化器
	if s.perfOptimizer != nil {
		s.perfOptimizer.Close()
	}
}

// Close 优雅关闭爬虫，释放所有资源（实现 io.Closer 接口）
func (s *Spider) Close() error {
	s.closeMux.Lock()
	defer s.closeMux.Unlock()

	// 防止重复关闭
	if s.closed {
		return nil
	}

	fmt.Println("\n正在关闭爬虫，清理资源...")

	// 停止爬取
	s.Stop()

	// 等待所有 goroutine 完成
	s.wg.Wait()

	// 关闭 done channel
	close(s.done)

	// 标记为已关闭
	s.closed = true

	fmt.Println("资源清理完成")
	return nil
}

// cleanup 内部清理方法（在 Start 中使用 defer 调用）
func (s *Spider) cleanup() {
	// Close 方法已经处理了所有清理工作
	s.Close()
}

// prioritizeURLs URL优先级排序（v2.8增强：BFS + 优先级混合决策）
func (s *Spider) prioritizeURLs(urls []string) []string {
	// 🆕 v2.8混合决策：如果有优先级调度器，使用精确优先级计算
	if s.priorityScheduler != nil {
		return s.prioritizeURLsWithPreciseCalculation(urls, 2) // depth默认2（会在调用处传入实际深度）
	}
	
	// 向下兼容：使用原有的简单三级分类
	highPriority := make([]string, 0)   // 高优先级
	mediumPriority := make([]string, 0) // 中优先级
	lowPriority := make([]string, 0)    // 低优先级

	for _, url := range urls {
		urlLower := strings.ToLower(url)

		// 高优先级：带多个参数的URL、admin/api/login等敏感路径
		if (strings.Count(url, "=") >= 2) ||
			strings.Contains(urlLower, "/admin") ||
			strings.Contains(urlLower, "/api/") ||
			strings.Contains(urlLower, "/login") ||
			strings.Contains(urlLower, "/user") ||
			strings.Contains(urlLower, "/account") {
			highPriority = append(highPriority, url)
		} else if strings.Contains(url, "?") {
			// 中优先级：带参数的URL
			mediumPriority = append(mediumPriority, url)
		} else {
			// 低优先级：普通URL
			lowPriority = append(lowPriority, url)
		}
	}

	// 合并：高 → 中 → 低
	result := make([]string, 0, len(urls))
	result = append(result, highPriority...)
	result = append(result, mediumPriority...)
	result = append(result, lowPriority...)

	return result
}

// prioritizeURLsWithPreciseCalculation 🆕 使用精确优先级计算排序（混合决策核心）
func (s *Spider) prioritizeURLsWithPreciseCalculation(urls []string, depth int) []string {
	type URLWithPriority struct {
		URL      string
		Priority float64
	}
	
	urlsWithPriority := make([]URLWithPriority, 0, len(urls))
	
	// 计算每个URL的精确优先级
	for _, url := range urls {
		priority := s.priorityScheduler.CalculatePriority(url, depth)
		urlsWithPriority = append(urlsWithPriority, URLWithPriority{
			URL:      url,
			Priority: priority,
		})
	}
	
	// 按优先级从高到低排序
	sort.Slice(urlsWithPriority, func(i, j int) bool {
		return urlsWithPriority[i].Priority > urlsWithPriority[j].Priority
	})
	
	// 打印本层优先级TOP3（让用户看到混合决策的效果）
	if len(urlsWithPriority) > 0 {
		fmt.Printf("\n  [混合决策] 本层优先级TOP3（BFS框架 + 智能排序）:\n")
		topCount := 3
		if len(urlsWithPriority) < 3 {
			topCount = len(urlsWithPriority)
		}
		for i := 0; i < topCount; i++ {
			fmt.Printf("    %d. [优先级:%.2f] %s\n", 
				i+1, urlsWithPriority[i].Priority, urlsWithPriority[i].URL)
		}
		
		if len(urlsWithPriority) > 3 {
			fmt.Printf("    ... 还有 %d 个URL按优先级排序\n", len(urlsWithPriority)-3)
		}
	}
	
	// 提取排序后的URL列表
	result := make([]string, 0, len(urls))
	for _, item := range urlsWithPriority {
		result = append(result, item.URL)
	}
	
	return result
}

// isRelativeURL 检查是否为相对URL（用于CDN JS中的URL拼接）
func isRelativeURL(urlStr string) bool {
	// 完整URL（http://或https://）不是相对URL
	if strings.HasPrefix(urlStr, "http://") || strings.HasPrefix(urlStr, "https://") {
		return false
	}
	
	// 协议相对URL（//example.com）不是相对URL
	if strings.HasPrefix(urlStr, "//") {
		return false
	}
	
	// 以下情况视为相对URL：
	// - 以/开头（绝对路径）: /api/user
	// - 以./开头（相对路径）: ./assets/app.js
	// - 以../开头（上级路径）: ../config.json
	// - 不含://但含路径分隔符: api/endpoint
	return strings.HasPrefix(urlStr, "/") || 
	       strings.HasPrefix(urlStr, "./") || 
	       strings.HasPrefix(urlStr, "../") ||
	       (strings.Contains(urlStr, "/") && !strings.Contains(urlStr, "://"))
}

// IsValidURL 检查URL是否为有效的HTTP/HTTPS链接
func IsValidURL(url string) bool {
	// 检查是否为空
	if url == "" {
		return false
	}
	
	// 过滤明显无效的字符串（长度检查）
	if len(url) < 2 || len(url) > 2048 {
		return false
	}

	// 检查是否为javascript:或mailto:等非HTTP链接
	if strings.HasPrefix(url, "javascript:") ||
		strings.HasPrefix(url, "mailto:") ||
		strings.HasPrefix(url, "tel:") ||
		strings.HasPrefix(url, "sms:") ||
		strings.HasPrefix(url, "ftp:") ||
		strings.HasPrefix(url, "file:") ||
		strings.HasPrefix(url, "data:") {
		return false
	}

	// 检查是否为HTTP/HTTPS协议（完整URL）
	if strings.HasPrefix(url, "http://") || strings.HasPrefix(url, "https://") {
		return true
	}
	
	// 检查是否为//开头的协议相对URL
	if strings.HasPrefix(url, "//") {
		return true
	}

	// 对于相对URL，需要更严格的验证
	if !strings.Contains(url, "://") {
		// 排除明显不是URL的字符串
		
		// 1. 不包含空格（除非是编码后的%20）
		if strings.Contains(url, " ") && !strings.Contains(url, "%20") {
			return false
		}
		
		// 2. 不能只是单个单词（没有路径分隔符）
		// 允许：/path, /api/endpoint, ../relative, ./file
		// 拒绝：word, emoji_name, keyword
		if !strings.Contains(url, "/") && !strings.Contains(url, "?") && !strings.Contains(url, "#") {
			// 单个单词，很可能是无效的
			// 但要允许 index.html 这样的文件名
			if !strings.Contains(url, ".") {
				return false
			}
		}
		
		// 3. 排除常见的非URL模式
		// - emoji名称（包含下划线但没有路径）
		// - HTML属性（aria-*, data-*）
		// - CSS类名
		if !strings.HasPrefix(url, "/") && !strings.HasPrefix(url, ".") && !strings.HasPrefix(url, "?") {
			// 检查是否看起来像emoji或属性名
			if strings.Contains(url, "_") && !strings.Contains(url, "/") && !strings.Contains(url, "=") {
				// 可能是 emoji_name 格式
				return false
			}
			if strings.HasPrefix(url, "aria-") || strings.HasPrefix(url, "data-") {
				return false
			}
		}
		
		// 4. 相对URL必须以 / . ? # 开头，或者看起来像文件路径
		firstChar := url[0]
		if firstChar != '/' && firstChar != '.' && firstChar != '?' && firstChar != '#' {
			// 不以这些字符开头的，检查是否包含路径分隔符
			if !strings.Contains(url, "/") {
				return false
			}
		}
		
		return true
	}

	// 其他情况视为无效
	return false
}

// ExportResults 导出结果
func (s *Spider) ExportResults() map[string]interface{} {
	s.mutex.Lock()
	defer s.mutex.Unlock()

	exportData := make(map[string]interface{})
	exportData["total_results"] = len(s.results)

	// 统计信息
	totalLinks := 0
	totalAssets := 0
	totalForms := 0
	totalAPIs := 0

	// 详细结果数据
	detailedResults := make([]map[string]interface{}, 0)
	allLinks := make([]string, 0)
	allAPIs := make([]string, 0)

	// 使用智能去重处理所有URL和表单
	for _, result := range s.results {
		totalLinks += len(result.Links)
		totalAssets += len(result.Assets)
		totalForms += len(result.Forms)
		totalAPIs += len(result.APIs)

		// 保存详细结果
		resultData := make(map[string]interface{})
		resultData["url"] = result.URL
		resultData["status_code"] = result.StatusCode
		resultData["content_type"] = result.ContentType
		resultData["links"] = result.Links
		resultData["assets"] = result.Assets
		resultData["forms"] = result.Forms
		resultData["apis"] = result.APIs
		resultData["post_requests"] = result.POSTRequests // 添加POST请求数据
		detailedResults = append(detailedResults, resultData)

		// 处理链接进行智能去重
		for _, link := range result.Links {
			s.smartDeduplication.ProcessURL(link)
		}

		// 处理表单进行智能去重
		for _, form := range result.Forms {
			s.smartDeduplication.ProcessForm(form)
		}

		// 收集所有链接
		allLinks = append(allLinks, result.Links...)
		allAPIs = append(allAPIs, result.APIs...)
	}

	exportData["total_links"] = totalLinks
	exportData["total_assets"] = totalAssets
	exportData["total_forms"] = totalForms
	exportData["total_apis"] = totalAPIs
	exportData["detailed_results"] = detailedResults
	exportData["links"] = allLinks
	exportData["apis"] = allAPIs
	exportData["external_links"] = s.externalLinks
	exportData["hidden_paths"] = s.hiddenPaths
	exportData["security_findings"] = s.securityFindings
	exportData["cross_domain_js_urls"] = s.crossDomainJS
	exportData["total_hidden_paths"] = len(s.hiddenPaths)
	exportData["total_security_findings"] = len(s.securityFindings)
	exportData["total_cross_domain_js_urls"] = len(s.crossDomainJS)

	// 添加智能去重统计
	exportData["deduplication_stats"] = s.smartDeduplication.GetDeduplicationStats()
	exportData["unique_url_patterns"] = s.smartDeduplication.GetUniqueURLs()
	exportData["unique_form_patterns"] = s.smartDeduplication.GetUniqueForms()

	// 添加新功能统计
	if s.advancedScope != nil {
		exportData["scope_stats"] = s.advancedScope.GetStatistics()
	}
	if s.perfOptimizer != nil {
		exportData["performance_stats"] = s.perfOptimizer.GetStatistics()
	}
	if s.formFiller != nil {
		exportData["form_filler_stats"] = s.formFiller.GetStatistics()
	}

	// 添加高级功能统计
	exportData["detected_technologies"] = s.detectedTechs
	exportData["tech_stack_summary"] = s.techDetector.GetTechStackSummary(s.detectedTechs)
	exportData["sensitive_findings"] = s.sensitiveFindings
	exportData["sensitive_stats"] = s.sensitiveDetector.GetStatistics()
	exportData["total_sensitive_findings"] = len(s.sensitiveFindings)

	// 被动爬取统计（如果使用）
	if s.passiveCrawler != nil {
		exportData["passive_stats"] = s.passiveCrawler.GetStatistics()
	}

	// 子域名提取统计
	if s.subdomainExtractor != nil {
		exportData["subdomains"] = s.subdomainExtractor.ExportSubdomains()
		exportData["subdomain_stats"] = s.subdomainExtractor.GetStatistics()
		exportData["total_subdomains"] = s.subdomainExtractor.GetSubdomainCount()
	}

	// DOM相似度检测统计
	if s.domSimilarity != nil {
		exportData["dom_similarity_stats"] = s.domSimilarity.GetStatistics()
		exportData["similar_pages"] = s.domSimilarity.GetSimilarPages()
		exportData["total_similar_pages"] = len(s.domSimilarity.GetSimilarPages())
	}

	// Sitemap和robots.txt统计
	exportData["sitemap_urls"] = s.sitemapURLs
	exportData["robots_urls"] = s.robotsURLs
	exportData["total_sitemap_urls"] = len(s.sitemapURLs)
	exportData["total_robots_urls"] = len(s.robotsURLs)

	// === 新增：静态资源分类 ===
	allAssets := make([]string, 0)
	for _, result := range s.results {
		allAssets = append(allAssets, result.Assets...)
	}

	if s.assetClassifier != nil {
		classifiedAssets := s.assetClassifier.ClassifyAssets(allAssets)
		exportData["classified_assets"] = classifiedAssets
		exportData["assets_stats"] = s.assetClassifier.GetAssetStats(classifiedAssets)
	}

	// === 新增：IP链接分类 ===
	// 重置allLinks用于IP检测（包含所有链接源）
	allLinks = make([]string, 0)
	for _, result := range s.results {
		allLinks = append(allLinks, result.Links...)
	}
	// 也检查外部链接和其他链接源
	allLinks = append(allLinks, s.externalLinks...)
	allLinks = append(allLinks, s.crossDomainJS...)

	if s.ipDetector != nil {
		classifiedIPs := s.ipDetector.ClassifyIPLinks(allLinks)
		exportData["ip_links"] = map[string]interface{}{
			"private_ips":   classifiedIPs["private_ip"],
			"public_ips":    classifiedIPs["public_ip"],
			"private_count": len(classifiedIPs["private_ip"]),
			"public_count":  len(classifiedIPs["public_ip"]),
			"total_count":   len(classifiedIPs["private_ip"]) + len(classifiedIPs["public_ip"]),
			"has_leak":      len(classifiedIPs["private_ip"]) > 0,
		}
	}

	// v2.7: 业务感知过滤器统计
	if s.businessFilter != nil && s.config.DeduplicationSettings.EnableBusinessAwareFilter {
		exportData["business_filter_stats"] = s.businessFilter.GetStatistics()
		exportData["business_top_patterns"] = s.businessFilter.GetTopPatterns(20)
	}
	
	// 🆕 v2.8: URL去重统计
	if s.urlDeduplicator != nil {
		exportData["url_deduplication"] = s.urlDeduplicator.GetStatistics()
		exportData["unique_url_patterns"] = s.urlDeduplicator.GetUniquePatterns()
		exportData["all_urls_with_variants"] = s.urlDeduplicator.GetAllURLs()
	}

	return exportData
}

// SaveUniqueURLsToFile 保存去重后的URL到文件（给其他工具使用）
func (s *Spider) SaveUniqueURLsToFile(filepath string) error {
	if s.urlDeduplicator == nil {
		return fmt.Errorf("URL去重器未初始化")
	}
	
	s.mutex.Lock()
	defer s.mutex.Unlock()
	
	// 获取唯一的URL模式
	uniquePatterns := s.urlDeduplicator.GetUniquePatterns()
	
	if len(uniquePatterns) == 0 {
		return fmt.Errorf("没有URL可保存")
	}
	
	// 创建文件
	file, err := os.Create(filepath)
	if err != nil {
		return fmt.Errorf("创建文件失败: %v", err)
	}
	defer file.Close()
	
	// 写入URL（每行一个）
	for _, pattern := range uniquePatterns {
		_, err := file.WriteString(pattern + "\n")
		if err != nil {
			return fmt.Errorf("写入文件失败: %v", err)
		}
	}
	
	// 打印统计
	stats := s.urlDeduplicator.GetStatistics()
	fmt.Printf("\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n")
	fmt.Printf("  ✅ URL去重文件已保存: %s\n", filepath)
	fmt.Printf("  唯一URL模式: %d 个\n", stats["unique_patterns"])
	fmt.Printf("  原始URL总数: %d 个\n", stats["total_urls"])
	if stats["total_urls"] > stats["unique_patterns"] {
		reduction := stats["total_urls"] - stats["unique_patterns"]
		reductionPercent := float64(reduction) / float64(stats["total_urls"]) * 100
		fmt.Printf("  去重效果: 减少 %d 个 (%.1f%%)\n", reduction, reductionPercent)
	}
	fmt.Printf("━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n")
	
	return nil
}

// PrintURLDeduplicationReport 打印URL去重详细报告
func (s *Spider) PrintURLDeduplicationReport() {
	if s.urlDeduplicator != nil {
		s.urlDeduplicator.PrintReport()
		// 显示前10个最多变体的URL模式
		s.urlDeduplicator.PrintDetailedReport(10)
	}
}

// PrintBusinessFilterReport 打印业务感知过滤器的详细报告
func (s *Spider) PrintBusinessFilterReport() {
	if s.businessFilter != nil && s.config.DeduplicationSettings.EnableBusinessAwareFilter {
		s.businessFilter.PrintReport()
	}
}

// PrintURLPatternDedupReport 打印URL模式去重报告
func (s *Spider) PrintURLPatternDedupReport() {
	if s.urlPatternDedup != nil {
		s.urlPatternDedup.PrintReport()
	}
}

// SaveSensitiveInfoToFile 保存敏感信息到文件（独立输出，包含来源URL）
func (s *Spider) SaveSensitiveInfoToFile(filepath string) error {
	s.mutex.Lock()
	defer s.mutex.Unlock()
	
	if s.sensitiveDetector == nil {
		return fmt.Errorf("敏感信息检测器未初始化")
	}
	
	findings := s.sensitiveDetector.GetFindings()
	if len(findings) == 0 {
		fmt.Println("[敏感信息] 未发现敏感信息，跳过文件保存")
		return nil
	}
	
	// 创建文件
	file, err := os.Create(filepath)
	if err != nil {
		return fmt.Errorf("创建敏感信息文件失败: %v", err)
	}
	defer file.Close()
	
	// 写入标题
	file.WriteString("==========================================\n")
	file.WriteString("   敏感信息泄露检测报告\n")
	file.WriteString("==========================================\n\n")
	
	// 统计信息
	stats := s.sensitiveDetector.GetStatistics()
	file.WriteString(fmt.Sprintf("扫描页面数: %d\n", stats["total_scanned"]))
	file.WriteString(fmt.Sprintf("发现总数: %d\n", stats["total_findings"]))
	file.WriteString(fmt.Sprintf("  - 高危: %d\n", stats["high_severity"]))
	file.WriteString(fmt.Sprintf("  - 中危: %d\n", stats["medium_severity"]))
	file.WriteString(fmt.Sprintf("  - 低危: %d\n", stats["low_severity"]))
	file.WriteString("\n==========================================\n\n")
	
	// 按严重程度分组
	highFindings := s.sensitiveDetector.GetFindingsBySeverity("HIGH")
	mediumFindings := s.sensitiveDetector.GetFindingsBySeverity("MEDIUM")
	lowFindings := s.sensitiveDetector.GetFindingsBySeverity("LOW")
	
	// 写入高危发现
	if len(highFindings) > 0 {
		file.WriteString("【高危发现】\n")
		file.WriteString(strings.Repeat("-", 60) + "\n\n")
		for i, finding := range highFindings {
			file.WriteString(fmt.Sprintf("[%d] %s\n", i+1, finding.Type))
			file.WriteString(fmt.Sprintf("    来源URL: %s\n", finding.SourceURL))
			file.WriteString(fmt.Sprintf("    位置: %s\n", finding.Location))
			file.WriteString(fmt.Sprintf("    值: %s\n", finding.Value))
			file.WriteString("\n")
		}
		file.WriteString("\n")
	}
	
	// 写入中危发现
	if len(mediumFindings) > 0 {
		file.WriteString("【中危发现】\n")
		file.WriteString(strings.Repeat("-", 60) + "\n\n")
		for i, finding := range mediumFindings {
			file.WriteString(fmt.Sprintf("[%d] %s\n", i+1, finding.Type))
			file.WriteString(fmt.Sprintf("    来源URL: %s\n", finding.SourceURL))
			file.WriteString(fmt.Sprintf("    位置: %s\n", finding.Location))
			file.WriteString(fmt.Sprintf("    值: %s\n", finding.Value))
			file.WriteString("\n")
		}
		file.WriteString("\n")
	}
	
	// 写入低危发现
	if len(lowFindings) > 0 {
		file.WriteString("【低危发现】\n")
		file.WriteString(strings.Repeat("-", 60) + "\n\n")
		
		// 按类型分组统计（低危数量可能很多，只显示统计）
		typeCount := make(map[string]int)
		typeExamples := make(map[string]*SensitiveInfo)
		for _, finding := range lowFindings {
			typeCount[finding.Type]++
			if typeExamples[finding.Type] == nil {
				typeExamples[finding.Type] = finding
			}
		}
		
		for findingType, count := range typeCount {
			example := typeExamples[findingType]
			file.WriteString(fmt.Sprintf("类型: %s (共 %d 个)\n", findingType, count))
			file.WriteString(fmt.Sprintf("  示例来源: %s\n", example.SourceURL))
			file.WriteString(fmt.Sprintf("  示例值: %s\n", example.Value))
			file.WriteString("\n")
		}
	}
	
	// 打印统计
	fmt.Printf("\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n")
	fmt.Printf("  ✅ 敏感信息报告已保存: %s\n", filepath)
	fmt.Printf("  总发现: %d 个 (高危:%d, 中危:%d, 低危:%d)\n", 
		stats["total_findings"], stats["high_severity"], stats["medium_severity"], stats["low_severity"])
	fmt.Printf("━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n")
	
	return nil
}

// SaveSensitiveInfoToJSON 保存敏感信息到JSON文件
func (s *Spider) SaveSensitiveInfoToJSON(filepath string) error {
	s.mutex.Lock()
	defer s.mutex.Unlock()
	
	if s.sensitiveDetector == nil {
		return fmt.Errorf("敏感信息检测器未初始化")
	}
	
	findings := s.sensitiveDetector.GetFindings()
	if len(findings) == 0 {
		fmt.Println("[敏感信息] 未发现敏感信息，跳过JSON保存")
		return nil
	}
	
	// 构建JSON数据
	report := map[string]interface{}{
		"scan_time": time.Now().Format("2006-01-02 15:04:05"),
		"target_domain": s.targetDomain,
		"statistics": s.sensitiveDetector.GetStatistics(),
		"findings": s.sensitiveDetector.ExportFindings(),
	}
	
	// 转换为JSON
	data, err := json.MarshalIndent(report, "", "  ")
	if err != nil {
		return fmt.Errorf("JSON编码失败: %v", err)
	}
	
	// 写入文件
	if err := os.WriteFile(filepath, data, 0644); err != nil {
		return fmt.Errorf("写入JSON文件失败: %v", err)
	}
	
	stats := s.sensitiveDetector.GetStatistics()
	fmt.Printf("\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n")
	fmt.Printf("  ✅ 敏感信息JSON报告已保存: %s\n", filepath)
	fmt.Printf("  总发现: %d 个 (高危:%d, 中危:%d, 低危:%d)\n", 
		stats["total_findings"], stats["high_severity"], stats["medium_severity"], stats["low_severity"])
	fmt.Printf("━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n")
	
	return nil
}

// SaveStructureUniqueURLsToFile 保存结构化去重后的URL到文件
// 该方法会识别路径变量（如 /product-123/ → /product-{num}/）和参数值变化
// 只保存每个结构的代表性URL，避免保存大量相似URL
func (s *Spider) SaveStructureUniqueURLsToFile(filepath string) error {
	if s.urlStructureDedup == nil {
		return fmt.Errorf("URL结构化去重器未初始化")
	}
	
	s.mutex.Lock()
	defer s.mutex.Unlock()
	
	// 获取唯一的结构化URL
	uniqueStructures := s.urlStructureDedup.GetUniqueStructures()
	
	if len(uniqueStructures) == 0 {
		return fmt.Errorf("没有URL可保存")
	}
	
	// 创建文件
	file, err := os.Create(filepath)
	if err != nil {
		return fmt.Errorf("创建文件失败: %v", err)
	}
	defer file.Close()
	
	// 写入URL（每行一个）
	for _, url := range uniqueStructures {
		_, err := file.WriteString(url + "\n")
		if err != nil {
			return fmt.Errorf("写入文件失败: %v", err)
		}
	}
	
	// 打印统计
	stats := s.urlStructureDedup.GetStatistics()
	fmt.Printf("\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n")
	fmt.Printf("  ✅ 结构化去重URL文件已保存: %s\n", filepath)
	fmt.Printf("  唯一URL结构: %d 个\n", stats["unique_structures"])
	fmt.Printf("  原始URL总数: %d 个\n", stats["total_urls"])
	if stats["total_urls"] > stats["unique_structures"] {
		reduction := stats["duplicate_urls"]
		reductionPercent := float64(reduction) / float64(stats["total_urls"]) * 100
		fmt.Printf("  去重效果: 减少 %d 个 (%.1f%%)\n", reduction, reductionPercent)
	}
	fmt.Printf("━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n")
	
	return nil
}

// PrintStructureDeduplicationReport 打印结构化去重详细报告
func (s *Spider) PrintStructureDeduplicationReport() {
	if s.urlStructureDedup != nil {
		s.urlStructureDedup.PrintReport()
	}
}

// PrintURLFilterReport 打印URL过滤统计报告（v3.5新增）
func (s *Spider) PrintURLFilterReport() {
	if s.urlValidator == nil {
		return
	}
	
	fmt.Println("\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")
	fmt.Println("📊 URL质量过滤报告 (v3.5)")
	fmt.Println("━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")
	
	// 统计所有收集的URL和过滤的URL
	totalCollected := 0
	totalFiltered := 0
	validURLs := 0
	
	// 遍历所有结果统计
	for _, result := range s.results {
		totalCollected += len(result.Links)
		for _, link := range result.Links {
			if s.urlValidator.IsValidBusinessURL(link) {
				validURLs++
			}
		}
	}
	
	totalFiltered = totalCollected - validURLs
	filterRate := 0.0
	if totalCollected > 0 {
		filterRate = float64(totalFiltered) / float64(totalCollected) * 100
	}
	
	fmt.Printf("\n🎯 过滤效果:\n")
	fmt.Printf("  原始收集URL: %d\n", totalCollected)
	fmt.Printf("  有效业务URL: %d\n", validURLs)
	fmt.Printf("  过滤垃圾URL: %d\n", totalFiltered)
	fmt.Printf("  过滤率: %.1f%%\n", filterRate)
	
	fmt.Println("\n✨ 过滤规则:")
	fmt.Println("  ✓ MIME类型字符串（application/vnd.*）")
	fmt.Println("  ✓ JavaScript关键字（Math, Object, Array等）")
	fmt.Println("  ✓ URL编码的代码片段")
	fmt.Println("  ✓ 无意义的单字符路径（/a, /b, /M等）")
	fmt.Println("  ✓ HTML标签片段")
	fmt.Println("  ✓ 只保留有业务价值的URL")
	
	fmt.Println("\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n")
}

// PrintPOSTDetectionReport 打印POST请求检测报告（v3.5新增）
func (s *Spider) PrintPOSTDetectionReport() {
	totalPOST := 0
	uniquePOSTURLs := make(map[string]bool)
	
	// 统计POST请求
	for _, result := range s.results {
		totalPOST += len(result.POSTRequests)
		for _, post := range result.POSTRequests {
			uniquePOSTURLs[post.URL] = true
		}
	}
	
	if totalPOST == 0 {
		fmt.Println("\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")
		fmt.Println("📮 POST请求检测报告 (v3.5)")
		fmt.Println("━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")
		fmt.Println("\n⚠️  未检测到POST请求")
		fmt.Println("\n💡 可能原因:")
		fmt.Println("  1. 网站使用纯AJAX提交（建议使用动态爬虫）")
		fmt.Println("  2. 表单是JavaScript动态生成的")
		fmt.Println("  3. 网站不包含表单功能")
		fmt.Println("\n💡 建议:")
		fmt.Println("  - 确保启用了动态爬虫（enable_dynamic_crawler: true）")
		fmt.Println("  - 检查浏览器开发者工具的Network面板")
		fmt.Println("  - 考虑使用 -depth 3 或更大值进行深度爬取")
		fmt.Println("━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n")
		return
	}
	
	fmt.Println("\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")
	fmt.Println("📮 POST请求检测报告 (v3.5)")
	fmt.Println("━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")
	
	fmt.Printf("\n🎯 检测结果:\n")
	fmt.Printf("  总POST请求数: %d\n", totalPOST)
	fmt.Printf("  唯一POST端点: %d\n", len(uniquePOSTURLs))
	
	// 统计来源类型
	sourceTypes := make(map[string]int)
	for _, result := range s.results {
		for _, post := range result.POSTRequests {
			if post.FromForm {
				sourceTypes["HTML表单"]++
			} else {
				sourceTypes["JavaScript/AJAX"]++
			}
		}
	}
	
	if len(sourceTypes) > 0 {
		fmt.Println("\n📊 来源分布:")
		for source, count := range sourceTypes {
			percent := float64(count) / float64(totalPOST) * 100
			fmt.Printf("  - %s: %d (%.1f%%)\n", source, count, percent)
		}
	}
	
	// 显示前10个POST端点示例
	if len(uniquePOSTURLs) > 0 {
		fmt.Println("\n📝 POST端点示例（前10个）:")
		count := 0
		for postURL := range uniquePOSTURLs {
			if count >= 10 {
				break
			}
			count++
			fmt.Printf("  %d. %s\n", count, postURL)
		}
		
		if len(uniquePOSTURLs) > 10 {
			fmt.Printf("  ... 还有 %d 个\n", len(uniquePOSTURLs)-10)
		}
	}
	
	fmt.Println("━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n")
}

// PrintLoginWallReport 打印登录墙检测报告
func (s *Spider) PrintLoginWallReport() {
	if s.loginWallDetector != nil {
		s.loginWallDetector.PrintSummary()
	}
}

// PrintRedirectReport 打印重定向检测报告
func (s *Spider) PrintRedirectReport() {
	if s.redirectManager != nil {
		s.redirectManager.PrintReport()
	}
}

// GetRedirectManager 获取重定向管理器
func (s *Spider) GetRedirectManager() *RedirectManager {
	return s.redirectManager
}

// CollectAllURLsForStructureDedup 收集所有URL并添加到结构化去重器
// 该方法应在爬取完成后调用，用于分析和去重所有发现的URL
func (s *Spider) CollectAllURLsForStructureDedup() {
	if s.urlStructureDedup == nil {
		return
	}
	
	s.mutex.Lock()
	defer s.mutex.Unlock()
	
	// 收集所有URL（只收集目标域名的URL）
	for _, result := range s.results {
		// 添加页面URL
		if s.isInTargetDomain(result.URL) {
			s.urlStructureDedup.AddURL(result.URL)
		}
		
		// 添加发现的链接（只添加域内的）
		for _, link := range result.Links {
			if s.isInTargetDomain(link) {
				s.urlStructureDedup.AddURL(link)
			}
		}
		
		// 添加API（只添加域内的）
		for _, api := range result.APIs {
			if s.isInTargetDomain(api) {
				s.urlStructureDedup.AddURL(api)
			}
		}
		
		// 添加表单URL（只添加域内的）
		for _, form := range result.Forms {
			if form.Action != "" && s.isInTargetDomain(form.Action) {
				s.urlStructureDedup.AddURL(form.Action)
			}
		}
	}
}

// filterBySeverity 按严重级别过滤敏感信息发现
func (s *Spider) filterBySeverity(findings []*SensitiveInfo) []*SensitiveInfo {
	minSeverity := s.config.SensitiveDetectionSettings.MinSeverity
	
	// 如果设置为LOW，返回所有发现
	if minSeverity == "LOW" || minSeverity == "" {
		return findings
	}
	
	filtered := make([]*SensitiveInfo, 0)
	for _, finding := range findings {
		if minSeverity == "MEDIUM" {
			// 只返回MEDIUM和HIGH
			if finding.Severity == "MEDIUM" || finding.Severity == "HIGH" {
				filtered = append(filtered, finding)
			}
		} else if minSeverity == "HIGH" {
			// 只返回HIGH
			if finding.Severity == "HIGH" {
				filtered = append(filtered, finding)
			}
		}
	}
	
	return filtered
}

// URLWithPriority URL优先级信息（用于混合策略）
type URLWithPriority struct {
	URL      string
	Priority float64
	Depth    int
}

// crawlWithHybridStrategy 🆕 v3.4 混合调度策略爬取（BFS框架+智能优先级排序）
func (s *Spider) crawlWithHybridStrategy() {
	fmt.Println("\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")
	fmt.Println("【混合调度策略】BFS框架 + 智能优先级排序")
	fmt.Println("━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")
	fmt.Println("✨ 结合BFS全面性和优先级智能性")
	fmt.Println("✨ 自适应学习，越爬越聪明")
	fmt.Println("")
	
	currentDepth := 1
	totalCrawled := 0
	
	// 循环爬取每一层
	for currentDepth < s.config.DepthSettings.MaxDepth {
		currentDepth++
		
		fmt.Printf("\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n")
		fmt.Printf("【第 %d 层爬取】混合策略模式 | 最大深度: %d\n", currentDepth, s.config.DepthSettings.MaxDepth)
		fmt.Printf("━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n")
		
		// 1. 收集当前层的所有URL（BFS框架）
		layerURLs := s.collectLinksForLayer(currentDepth)
		
		if len(layerURLs) == 0 {
			fmt.Printf("第 %d 层没有新链接，爬取结束\n", currentDepth)
			break
		}
		
		fmt.Printf("收集到 %d 个候选URL\n", len(layerURLs))
		
		// 2. 计算每个URL的精确优先级
		urlsWithPriority := s.calculateURLPriorities(layerURLs, currentDepth)
		
		// 3. 按优先级排序（高优先级在前）
		sort.Slice(urlsWithPriority, func(i, j int) bool {
			return urlsWithPriority[i].Priority > urlsWithPriority[j].Priority
		})
		
		// 4. 应用层级限制（如果配置了）
		maxURLs := s.config.SchedulingSettings.HybridConfig.MaxURLsPerLayer
		originalCount := len(urlsWithPriority)
		if maxURLs > 0 && len(urlsWithPriority) > maxURLs {
			// 保留高优先级的URL
			urlsWithPriority = urlsWithPriority[:maxURLs]
			fmt.Printf("  [智能限制] 本层限制爬取前 %d 个高优先级URL（原始: %d个）\n", maxURLs, originalCount)
		}
		
		// 5. 展示本层优先级TOP5
		s.showLayerPriorityTop(urlsWithPriority, 5)
		
		// 6. 爬取（按优先级顺序）
		results := s.crawlLayerWithPriority(urlsWithPriority, currentDepth)
		
		// 7. 自适应学习（根据爬取结果调整权重）
		if s.adaptiveLearner != nil {
			s.adaptiveLearner.LearnFromResults(results)
			
			// 检查是否需要调整权重
			if shouldAdjust, reason := s.adaptiveLearner.ShouldAdjustWeights(); shouldAdjust {
				fmt.Printf("\n🤖 [自适应学习] %s\n", reason)
				if s.adaptiveLearner.AdjustWeights(s.priorityScheduler) {
					fmt.Println("✅ 权重已优化，下一层将使用新权重")
				}
			}
		}
		
		// 8. 合并结果
		s.mutex.Lock()
		s.results = append(s.results, results...)
		s.mutex.Unlock()
		
		totalCrawled += len(results)
		
		fmt.Printf("\n第 %d 层完成！爬取 %d 个URL，累计 %d 个\n", 
			currentDepth, len(results), totalCrawled)
		
		// 检查是否达到限制
		if totalCrawled >= 500 {
			fmt.Printf("已达到URL限制(500)，爬取结束\n")
			break
		}
	}
	
	fmt.Printf("\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n")
	fmt.Printf("【混合策略爬取完成】\n")
	fmt.Printf("总深度: %d 层 | 总URL: %d 个\n", currentDepth, totalCrawled)
	fmt.Printf("━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\n")
}

// calculateURLPriorities 计算URL列表的优先级
func (s *Spider) calculateURLPriorities(urls []string, depth int) []*URLWithPriority {
	result := make([]*URLWithPriority, 0, len(urls))
	
	// 获取权重配置
	weights := s.config.SchedulingSettings.HybridConfig.PriorityWeights
	
	for _, urlStr := range urls {
		// 基础优先级（来自PriorityScheduler）
		basePriority := s.priorityScheduler.CalculatePriority(urlStr, depth)
		
		// 业务价值加成（来自BusinessAwareFilter）
		businessBonus := 0.0
		if s.businessFilter != nil && s.config.DeduplicationSettings.EnableBusinessAwareFilter {
			_, _, businessScore := s.businessFilter.ShouldCrawlURL(urlStr)
			// 将业务分数（0-100）转换为优先级加成
			businessBonus = (businessScore / 100.0) * weights.BusinessValue * 10
		}
		
		// 最终优先级 = 基础优先级 + 业务加成
		finalPriority := basePriority + businessBonus
		
		result = append(result, &URLWithPriority{
			URL:      urlStr,
			Priority: finalPriority,
			Depth:    depth,
		})
	}
	
	return result
}

// showLayerPriorityTop 展示本层优先级TOP N
func (s *Spider) showLayerPriorityTop(urls []*URLWithPriority, topN int) {
	if len(urls) == 0 {
		return
	}
	
	fmt.Printf("\n  [优先级TOP%d] 本层最高价值URL:\n", topN)
	
	displayCount := topN
	if len(urls) < topN {
		displayCount = len(urls)
	}
	
	for i := 0; i < displayCount; i++ {
		item := urls[i]
		fmt.Printf("    %d. [优先级: %.2f] %s\n", i+1, item.Priority, item.URL)
	}
	
	if len(urls) > displayCount {
		fmt.Printf("    ... 还有 %d 个URL按优先级排序\n", len(urls)-displayCount)
	}
	fmt.Println()
}

// crawlLayerWithPriority 按优先级顺序爬取一层的URL
func (s *Spider) crawlLayerWithPriority(urlsWithPriority []*URLWithPriority, depth int) []*Result {
	if len(urlsWithPriority) == 0 {
		return []*Result{}
	}
	
	// 提取URL列表
	urls := make([]string, 0, len(urlsWithPriority))
	for _, item := range urlsWithPriority {
		urls = append(urls, item.URL)
	}
	
	// 标记为已访问
	s.mutex.Lock()
	for _, url := range urls {
		s.visitedURLs[url] = true
	}
	s.mutex.Unlock()
	
	// 创建工作池（复用现有的crawlLayer逻辑）
	return s.crawlLayer(urls, depth)
}

// crawlWithPriorityQueue 🆕 使用优先级队列模式爬取（实验性）
func (s *Spider) crawlWithPriorityQueue() {
	fmt.Println("\n开始优先级队列模式爬取...")
	fmt.Println("算法：纯优先级队列调度（实验性）")
	
	// 将所有已发现的URL添加到优先级队列
	s.mutex.Lock()
	for _, result := range s.results {
		for _, link := range result.Links {
			// 计算深度（简化：都视为深度2）
			s.priorityScheduler.AddURL(link, 2)
		}
	}
	s.mutex.Unlock()
	
	fmt.Printf("优先级队列初始化完成，队列大小: %d\n", s.priorityScheduler.Size())
	
	totalCrawled := 0
	maxURLs := 500 // 限制最大爬取数量
	
	// 循环从队列中取URL爬取
	for totalCrawled < maxURLs && s.priorityScheduler.Size() > 0 {
		// 批量取出高优先级URL
		batchSize := 30 // 每批30个（匹配worker数量）
		batch := s.priorityScheduler.PopBatch(batchSize)
		
		if len(batch) == 0 {
			break
		}
		
		fmt.Printf("\n批次爬取: %d个URL（优先级排序）\n", len(batch))
		
		// 显示前3个URL的优先级
		for i := 0; i < len(batch) && i < 3; i++ {
			fmt.Printf("  [优先级: %.2f] %s\n", batch[i].Priority, batch[i].URL)
		}
		if len(batch) > 3 {
			fmt.Printf("  ... 还有 %d 个URL\n", len(batch)-3)
		}
		
		// 提取URL列表
		urls := make([]string, 0, len(batch))
		for _, item := range batch {
			urls = append(urls, item.URL)
		}
		
		// 爬取这批URL
		newResults := s.crawlLayer(urls, batch[0].Depth)
		
		// 合并结果
		s.mutex.Lock()
		s.results = append(s.results, newResults...)
		
		// 将新发现的URL添加到优先级队列
		for _, result := range newResults {
			for _, newLink := range result.Links {
				if !s.priorityScheduler.IsVisited(newLink) {
					// 新链接的深度 = 当前深度 + 1
					newDepth := batch[0].Depth + 1
					if newDepth <= s.config.DepthSettings.MaxDepth {
						s.priorityScheduler.AddURL(newLink, newDepth)
					}
				}
			}
		}
		s.mutex.Unlock()
		
		totalCrawled += len(batch)
		fmt.Printf("已爬取: %d个，队列剩余: %d个\n", totalCrawled, s.priorityScheduler.Size())
		
		// 检查是否达到最大深度
		if batch[0].Depth >= s.config.DepthSettings.MaxDepth {
			fmt.Printf("已达到最大深度 %d，停止爬取\n", s.config.DepthSettings.MaxDepth)
			break
		}
	}
	
	// 打印最终统计
	fmt.Printf("\n优先级队列爬取完成！总共爬取 %d 个URL\n", totalCrawled)
	s.priorityScheduler.PrintStatistics()
}
