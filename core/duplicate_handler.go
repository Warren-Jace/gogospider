package core

import (
	"crypto/md5"
	"encoding/hex"
	"fmt"
	"math"
	"net/url"
	"sort"
	"strings"
	"sync"
	"sync/atomic"
)

// URLInfo URLä¿¡æ¯
type URLInfo struct {
	URL       string // åŸå§‹URL
	Index     int    // åºå·ï¼ˆåœ¨resultsä¸­çš„ç´¢å¼•ï¼Œä»1å¼€å§‹ï¼‰
	IsCrawled bool   // æ˜¯å¦çœŸæ­£è¢«çˆ¬å–è¿‡ï¼ˆä¸æ˜¯ä»…ä»…è¢«å‘ç°ï¼‰
}

// DuplicateHandler å»é‡å¤„ç†å™¨
type DuplicateHandler struct {
	// ğŸ”§ ä¿®å¤ï¼šæ·»åŠ äº’æ–¥é”ä¿æŠ¤å¹¶å‘è®¿é—®
	mutex sync.RWMutex
	
	// å·²å¤„ç†URLçš„å“ˆå¸Œé›†åˆ
	processedURLs map[string]bool
	
	// ğŸ†• v4.7: å“ˆå¸Œåˆ°URLä¿¡æ¯çš„æ˜ å°„ï¼ˆåŒ…å«åŸå§‹URLã€åºå·å’Œçˆ¬å–çŠ¶æ€ï¼‰
	hashToInfo map[string]*URLInfo
	
	// å·²å¤„ç†å†…å®¹çš„å“ˆå¸Œé›†åˆ
	processedContent map[string]bool
	
	// ç›¸ä¼¼åº¦é˜ˆå€¼
	similarityThreshold float64
	
	// ğŸ†• è°ƒè¯•ç»Ÿè®¡ä¿¡æ¯
	totalChecks   int64
	duplicateHits int64
	enableDebug   bool
}

// NewDuplicateHandler åˆ›å»ºå»é‡å¤„ç†å™¨å®ä¾‹
func NewDuplicateHandler(threshold float64) *DuplicateHandler {
	d := &DuplicateHandler{
		processedURLs:       make(map[string]bool),
		hashToInfo:          make(map[string]*URLInfo),
		processedContent:    make(map[string]bool),
		similarityThreshold: threshold,
		enableDebug:         true, // å¯ç”¨è°ƒè¯•æ¨¡å¼
	}
	fmt.Printf("ğŸ”§ [å»é‡å™¨] åˆ›å»ºæ–°å®ä¾‹ (åœ°å€: %p)\n", d)
	return d
}

// IsDuplicateURL æ£€æŸ¥URLæ˜¯å¦é‡å¤
func (d *DuplicateHandler) IsDuplicateURL(rawURL string) bool {
	isDup, _ := d.IsDuplicateURLWithOriginal(rawURL)
	return isDup
}

// MarkURLAsStarted æ ‡è®°URLå¼€å§‹çˆ¬å–ï¼ˆåœ¨OnRequestæ—¶è°ƒç”¨ï¼‰
func (d *DuplicateHandler) MarkURLAsStarted(rawURL string) {
	d.mutex.Lock()
	defer d.mutex.Unlock()
	
	// è§£æURLå¹¶è®¡ç®—hash
	parsedURL, err := url.Parse(rawURL)
	var hash string
	if err != nil {
		hash = d.calculateMD5(rawURL)
	} else {
		urlKey := parsedURL.Scheme + "://" + parsedURL.Host + parsedURL.Path
		if parsedURL.RawQuery != "" {
			queryParams := parsedURL.Query()
			var paramKeys []string
			for key := range queryParams {
				paramKeys = append(paramKeys, key)
			}
			sort.Strings(paramKeys)
			
			var queryParts []string
			for _, key := range paramKeys {
				for _, value := range queryParams[key] {
					queryParts = append(queryParts, key+"="+value)
				}
			}
			if len(queryParts) > 0 {
				urlKey += "?" + strings.Join(queryParts, "&")
			}
		}
		hash = d.calculateMD5(urlKey)
	}
	
	// å¦‚æœURLä¸å­˜åœ¨ï¼Œæ·»åŠ åˆ°å»é‡å™¨ï¼ˆæ ‡è®°ä¸ºå¼€å§‹çˆ¬å–ï¼‰
	if _, exists := d.processedURLs[hash]; !exists {
		d.processedURLs[hash] = true
		d.hashToInfo[hash] = &URLInfo{
			URL:       rawURL,
			Index:     0,
			IsCrawled: false,
		}
	}
}

// ğŸ†• v4.7: IsDuplicateURLWithOriginal æ£€æŸ¥URLæ˜¯å¦é‡å¤ï¼Œå¹¶è¿”å›é‡å¤çš„URLä¿¡æ¯
// è¿”å›: (æ˜¯å¦é‡å¤, é‡å¤çš„URLä¿¡æ¯)
// æ³¨æ„ï¼šæ­¤æ–¹æ³•åªæ£€æŸ¥ï¼Œä¸æ·»åŠ ï¼
func (d *DuplicateHandler) IsDuplicateURLWithOriginal(rawURL string) (bool, *URLInfo) {
	// ğŸ†• ç»Ÿè®¡æ£€æŸ¥æ¬¡æ•°
	atomic.AddInt64(&d.totalChecks, 1)
	
	// è§£æURL
	parsedURL, err := url.Parse(rawURL)
	if err != nil {
		// å¦‚æœæ— æ³•è§£æURLï¼Œåˆ™ä½¿ç”¨åŸå§‹å»é‡é€»è¾‘
		hash := d.calculateMD5(rawURL)
		
		// ğŸ”§ ä¿®å¤ï¼šåŠ é”ä¿æŠ¤å¹¶å‘è®¿é—®
		d.mutex.Lock()
		defer d.mutex.Unlock()
		
		if _, exists := d.processedURLs[hash]; exists {
			atomic.AddInt64(&d.duplicateHits, 1)
			if d.enableDebug {
				fmt.Printf("âŒ [å»é‡] è·³è¿‡é‡å¤URL: %s\n", rawURL)
			}
			info := d.hashToInfo[hash]
			return true, info
		}
		// ğŸ”§ v4.7: ä¸åœ¨æ£€æŸ¥æ—¶æ·»åŠ ï¼Œåªæ£€æŸ¥ä¸æ·»åŠ 
		return false, nil
	}
	
	// æ„é€ ç”¨äºå»é‡æ£€æŸ¥çš„URLé”®å€¼
	// åŒ…å«åè®®ã€ä¸»æœºå’Œè·¯å¾„ï¼Œä½†ä¸åŒ…å«æŸ¥è¯¢å‚æ•°
	urlKey := parsedURL.Scheme + "://" + parsedURL.Host + parsedURL.Path
	
	// å¦‚æœæœ‰æŸ¥è¯¢å‚æ•°ï¼Œåˆ™å°†å…¶åŒ…å«åœ¨é”®å€¼ä¸­
	if parsedURL.RawQuery != "" {
		// è§£ææŸ¥è¯¢å‚æ•°
		queryParams := parsedURL.Query()
		
		// å¯¹æŸ¥è¯¢å‚æ•°è¿›è¡Œæ’åºä»¥ç¡®ä¿ä¸€è‡´æ€§
		var paramKeys []string
		for key := range queryParams {
			paramKeys = append(paramKeys, key)
		}
		sort.Strings(paramKeys)
		
		// æ„å»ºæ’åºåçš„æŸ¥è¯¢å­—ç¬¦ä¸²
		var queryParts []string
		for _, key := range paramKeys {
			for _, value := range queryParams[key] {
				queryParts = append(queryParts, key+"="+value)
			}
		}
		
		if len(queryParts) > 0 {
			urlKey += "?" + strings.Join(queryParts, "&")
		}
	}
	
	// è®¡ç®—URLé”®å€¼çš„MD5å“ˆå¸Œ
	hash := d.calculateMD5(urlKey)
	
	// ğŸ”§ ä¿®å¤ï¼šåŠ é”ä¿æŠ¤å¹¶å‘è®¿é—®
	d.mutex.Lock()
	defer d.mutex.Unlock()
	
	// æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡
	if _, exists := d.processedURLs[hash]; exists {
		atomic.AddInt64(&d.duplicateHits, 1)
		info := d.hashToInfo[hash]
		if d.enableDebug && strings.Contains(rawURL, "showimage.php") {
			// åªæ‰“å°showimage.phpçš„é‡å¤ä¿¡æ¯ï¼Œé¿å…æ—¥å¿—è¿‡å¤š
			fmt.Printf("âŒ [å»é‡] è·³è¿‡é‡å¤URL: %s\n    â†’ è§„èŒƒåŒ–: %s\n    â†’ hash: %s\n    â†’ åŸå§‹URL: %s (åºå·:%d, å·²çˆ¬å–:%v)\n", 
				rawURL, urlKey, hash[:8], info.URL, info.Index, info.IsCrawled)
		}
		return true, info
	}
	
	// ğŸ”§ v4.7: ä¸åœ¨æ£€æŸ¥æ—¶æ·»åŠ ï¼Œåªæ£€æŸ¥ä¸æ·»åŠ 
	return false, nil
}

// IsDuplicateContent æ£€æŸ¥å†…å®¹æ˜¯å¦é‡å¤
func (d *DuplicateHandler) IsDuplicateContent(content string) bool {
	// è®¡ç®—å†…å®¹çš„MD5å“ˆå¸Œ
	hash := d.calculateMD5(content)
	
	// ğŸ”§ ä¿®å¤ï¼šåŠ é”ä¿æŠ¤å¹¶å‘è®¿é—®
	d.mutex.Lock()
	defer d.mutex.Unlock()
	
	// æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡
	if _, exists := d.processedContent[hash]; exists {
		return true
	}
	
	// æ·»åŠ åˆ°å·²å¤„ç†é›†åˆ
	d.processedContent[hash] = true
	return false
}

// IsSimilarContent åŸºäºç›¸ä¼¼åº¦æ£€æŸ¥å†…å®¹æ˜¯å¦ç›¸ä¼¼
func (d *DuplicateHandler) IsSimilarContent(content1, content2 string) bool {
	similarity := d.calculateSimilarity(content1, content2)
	return similarity >= d.similarityThreshold
}

// calculateMD5 è®¡ç®—å­—ç¬¦ä¸²çš„MD5å“ˆå¸Œå€¼
func (d *DuplicateHandler) calculateMD5(text string) string {
	hasher := md5.New()
	hasher.Write([]byte(text))
	return hex.EncodeToString(hasher.Sum(nil))
}

// calculateSimilarity è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ç®€åŒ–ç‰ˆï¼‰
func (d *DuplicateHandler) calculateSimilarity(text1, text2 string) float64 {
	// è½¬æ¢ä¸ºå°å†™å¹¶åˆ†å‰²ä¸ºè¯æ±‡
	words1 := strings.Fields(strings.ToLower(text1))
	words2 := strings.Fields(strings.ToLower(text2))
	
	// åˆ›å»ºè¯æ±‡é¢‘ç‡æ˜ å°„
	freq1 := make(map[string]int)
	freq2 := make(map[string]int)
	
	for _, word := range words1 {
		// ç®€å•æ¸…ç†è¯æ±‡ï¼ˆç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼‰
		cleanWord := d.cleanWord(word)
		if cleanWord != "" {
			freq1[cleanWord]++
		}
	}
	
	for _, word := range words2 {
		// ç®€å•æ¸…ç†è¯æ±‡ï¼ˆç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼‰
		cleanWord := d.cleanWord(word)
		if cleanWord != "" {
			freq2[cleanWord]++
		}
	}
	
	// è®¡ç®—ç‚¹ç§¯
	dotProduct := 0.0
	for word, freq := range freq1 {
		if freq2[word] > 0 {
			dotProduct += float64(freq * freq2[word])
		}
	}
	
	// è®¡ç®—å‘é‡çš„æ¨¡
	magnitude1 := 0.0
	magnitude2 := 0.0
	
	for _, freq := range freq1 {
		magnitude1 += float64(freq * freq)
	}
	
	for _, freq := range freq2 {
		magnitude2 += float64(freq * freq)
	}
	
	// è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
	if magnitude1 == 0 || magnitude2 == 0 {
		return 0.0
	}
	
	similarity := dotProduct / (math.Sqrt(magnitude1) * math.Sqrt(magnitude2))
	return similarity
}

// cleanWord æ¸…ç†è¯æ±‡ï¼Œç§»é™¤æ ‡ç‚¹ç¬¦å·
func (d *DuplicateHandler) cleanWord(word string) string {
	// ç§»é™¤å¸¸è§çš„æ ‡ç‚¹ç¬¦å·
	cleaned := strings.Trim(word, ".,;:!?()[]{}\"'`-")
	return strings.ToLower(cleaned)
}

// IsSimilarDOM åŸºäºDOMç»“æ„æ£€æŸ¥ç›¸ä¼¼æ€§
func (d *DuplicateHandler) IsSimilarDOM(dom1, dom2 string) bool {
	// æå–DOMç»“æ„ç‰¹å¾
	features1 := d.extractDOMFeatures(dom1)
	features2 := d.extractDOMFeatures(dom2)
	
	// è®¡ç®—ç‰¹å¾ç›¸ä¼¼åº¦
	similarity := d.calculateFeatureSimilarity(features1, features2)
	return similarity >= d.similarityThreshold
}

// extractDOMFeatures æå–DOMç»“æ„ç‰¹å¾
func (d *DuplicateHandler) extractDOMFeatures(dom string) map[string]int {
	features := make(map[string]int)
	
	// ç®€åŒ–çš„DOMç‰¹å¾æå–
	// å®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨HTMLè§£æå™¨æå–æ›´ç²¾ç¡®çš„ç‰¹å¾
	
	// ç»Ÿè®¡æ ‡ç­¾ç±»å‹
	tagPatterns := []string{"<div", "<span", "<a", "<img", "<form", "<input", "<button"}
	
	for _, pattern := range tagPatterns {
		count := strings.Count(dom, pattern)
		if count > 0 {
			features[pattern] = count
		}
	}
	
	// ç»Ÿè®¡ç±»åå’ŒID
	// è¿™é‡Œç®€åŒ–å¤„ç†
	classCount := strings.Count(dom, "class=")
	idCount := strings.Count(dom, "id=")
	
	if classCount > 0 {
		features["class"] = classCount
	}
	
	if idCount > 0 {
		features["id"] = idCount
	}
	
	return features
}

// calculateFeatureSimilarity è®¡ç®—ç‰¹å¾ç›¸ä¼¼åº¦
func (d *DuplicateHandler) calculateFeatureSimilarity(features1, features2 map[string]int) float64 {
	// è®¡ç®—ç‚¹ç§¯
	dotProduct := 0.0
	for feature, freq := range features1 {
		if features2[feature] > 0 {
			dotProduct += float64(freq * features2[feature])
		}
	}
	
	// è®¡ç®—å‘é‡çš„æ¨¡
	magnitude1 := 0.0
	magnitude2 := 0.0
	
	for _, freq := range features1 {
		magnitude1 += float64(freq * freq)
	}
	
	for _, freq := range features2 {
		magnitude2 += float64(freq * freq)
	}
	
	// è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
	if magnitude1 == 0 || magnitude2 == 0 {
		return 0.0
	}
	
	similarity := dotProduct / (math.Sqrt(magnitude1) * math.Sqrt(magnitude2))
	return similarity
}

// ğŸ†• v4.7: UpdateURLInfo æ›´æ–°URLä¿¡æ¯ï¼ˆæ ‡è®°ä¸ºå·²çˆ¬å–å¹¶è®¾ç½®åºå·ï¼‰
func (d *DuplicateHandler) UpdateURLInfo(rawURL string, index int, isCrawled bool) {
	d.mutex.Lock()
	defer d.mutex.Unlock()
	
	// è§£æURLå¹¶è®¡ç®—hashï¼ˆä¸IsDuplicateURLWithOriginalä¸­çš„é€»è¾‘ä¸€è‡´ï¼‰
	parsedURL, err := url.Parse(rawURL)
	var hash string
	if err != nil {
		hash = d.calculateMD5(rawURL)
	} else {
		urlKey := parsedURL.Scheme + "://" + parsedURL.Host + parsedURL.Path
		if parsedURL.RawQuery != "" {
			queryParams := parsedURL.Query()
			var paramKeys []string
			for key := range queryParams {
				paramKeys = append(paramKeys, key)
			}
			sort.Strings(paramKeys)
			
			var queryParts []string
			for _, key := range paramKeys {
				for _, value := range queryParams[key] {
					queryParts = append(queryParts, key+"="+value)
				}
			}
			if len(queryParts) > 0 {
				urlKey += "?" + strings.Join(queryParts, "&")
			}
		}
		hash = d.calculateMD5(urlKey)
	}
	
	// æ›´æ–°URLä¿¡æ¯
	if info, exists := d.hashToInfo[hash]; exists {
		// ğŸ”§ v4.7: åªæ›´æ–°åºå·å’ŒçŠ¶æ€ï¼ˆå¦‚æœæä¾›ï¼‰
		// å¦‚æœIndex > 0ï¼Œåˆ™æ›´æ–°åºå·
		if index > 0 {
			info.Index = index
		}
		// IsCrawledåªèƒ½ä»falseå˜æˆtrueï¼Œä¸èƒ½åå‘ï¼ˆä¸€æ—¦çˆ¬å–æˆåŠŸå°±æ°¸è¿œæ˜¯trueï¼‰
		if isCrawled {
			info.IsCrawled = true
		}
	}
}

// ClearProcessed æ¸…ç©ºå·²å¤„ç†è®°å½•
func (d *DuplicateHandler) ClearProcessed() {
	// ğŸ”§ ä¿®å¤ï¼šåŠ é”ä¿æŠ¤å¹¶å‘è®¿é—®
	d.mutex.Lock()
	defer d.mutex.Unlock()
	
	d.processedURLs = make(map[string]bool)
	d.hashToInfo = make(map[string]*URLInfo)
	d.processedContent = make(map[string]bool)
}

// PrintStats æ‰“å°ç»Ÿè®¡ä¿¡æ¯
func (d *DuplicateHandler) PrintStats() {
	totalChecks := atomic.LoadInt64(&d.totalChecks)
	duplicateHits := atomic.LoadInt64(&d.duplicateHits)
	
	if totalChecks == 0 {
		fmt.Println("\nâš ï¸  [å»é‡å™¨ç»Ÿè®¡] æœªè¿›è¡Œä»»ä½•å»é‡æ£€æŸ¥ï¼")
		return
	}
	
	dupRate := float64(duplicateHits) / float64(totalChecks) * 100
	
	fmt.Println("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
	fmt.Println("ğŸ“Š å»é‡å™¨ç»Ÿè®¡ä¿¡æ¯")
	fmt.Println("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
	fmt.Printf("ğŸ” æ€»æ£€æŸ¥æ¬¡æ•°: %d\n", totalChecks)
	fmt.Printf("âŒ é‡å¤å‘½ä¸­: %d\n", duplicateHits)
	fmt.Printf("âœ… æ–°URL: %d\n", totalChecks-duplicateHits)
	fmt.Printf("ğŸ“ˆ å»é‡ç‡: %.1f%%\n", dupRate)
	
	d.mutex.RLock()
	urlCount := len(d.processedURLs)
	d.mutex.RUnlock()
	
	fmt.Printf("ğŸ’¾ å·²å­˜å‚¨URLæ•°: %d\n", urlCount)
	fmt.Println("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
}