package core

import (
	"fmt"
	"io"
	"net/http"
	"net/url"
	"strings"
	"sync"
	"time"
	
	"spider-golang/config"
)

// Spider ä¸»çˆ¬è™«åè°ƒå™¨
type Spider struct {
	config             *config.Config
	staticCrawler      StaticCrawler
	dynamicCrawler     DynamicCrawler
	jsAnalyzer         *JSAnalyzer
	paramHandler       *ParamHandler
	duplicateHandler   *DuplicateHandler
	smartDeduplication *SmartDeduplication
	hiddenPathDiscovery *HiddenPathDiscovery
	cdnDetector        *CDNDetector // CDNæ£€æµ‹å™¨
	workerPool         *WorkerPool  // å¹¶å‘å·¥ä½œæ± 
	
	// æ–°å¢ä¼˜åŒ–ç»„ä»¶
	formFiller         *SmartFormFiller        // æ™ºèƒ½è¡¨å•å¡«å……å™¨
	advancedScope      *AdvancedScope          // é«˜çº§ä½œç”¨åŸŸæ§åˆ¶
	perfOptimizer      *PerformanceOptimizer   // æ€§èƒ½ä¼˜åŒ–å™¨
	
	// é«˜çº§åŠŸèƒ½ç»„ä»¶
	techDetector       *TechStackDetector      // æŠ€æœ¯æ ˆæ£€æµ‹å™¨
	sensitiveDetector  *SensitiveInfoDetector  // æ•æ„Ÿä¿¡æ¯æ£€æµ‹å™¨
	passiveCrawler     *PassiveCrawler         // è¢«åŠ¨çˆ¬å–å™¨
	subdomainExtractor *SubdomainExtractor     // å­åŸŸåæå–å™¨
	domSimilarity      *DOMSimilarityDetector  // DOMç›¸ä¼¼åº¦æ£€æµ‹å™¨
	sitemapCrawler     *SitemapCrawler         // Sitemapçˆ¬å–å™¨
	assetClassifier    *AssetClassifier        // é™æ€èµ„æºåˆ†ç±»å™¨
	ipDetector         *IPDetector             // IPåœ°å€æ£€æµ‹å™¨
	
	results            []*Result
	sitemapURLs        []string // ä»sitemapå‘ç°çš„URL
	robotsURLs         []string // ä»robots.txtå‘ç°çš„URL
	externalLinks      []string // è®°å½•å¤–éƒ¨é“¾æ¥
	hiddenPaths        []string // è®°å½•éšè—è·¯å¾„
	securityFindings   []string // è®°å½•å®‰å…¨å‘ç°
	crossDomainJS      []string // è®°å½•è·¨åŸŸJSå‘ç°çš„URL
	detectedTechs      []*TechInfo // æ£€æµ‹åˆ°çš„æŠ€æœ¯æ ˆ
	sensitiveFindings  []*SensitiveInfo // æ•æ„Ÿä¿¡æ¯å‘ç°
	mutex              sync.Mutex
	targetDomain       string          // ç›®æ ‡åŸŸå
	visitedURLs        map[string]bool // å·²è®¿é—®URL
	
	// èµ„æºç®¡ç†ï¼ˆä¼˜åŒ–ï¼šé˜²æ­¢æ³„æ¼ï¼‰
	done               chan struct{}   // å®Œæˆä¿¡å·
	wg                 sync.WaitGroup  // ç­‰å¾…æ‰€æœ‰goroutineå®Œæˆ
	closed             bool            // æ˜¯å¦å·²å…³é—­
	closeMux           sync.Mutex      // å…³é—­é”
}

// NewSpider åˆ›å»ºçˆ¬è™«å®ä¾‹
func NewSpider(cfg *config.Config) *Spider {
	// åˆ›å»ºç»“æœé€šé“å’Œåœæ­¢é€šé“
	resultChan := make(chan Result, 100)
	stopChan := make(chan struct{})
	
	// è®¡ç®—å¹¶å‘workeræ•°é‡ï¼ˆé»˜è®¤20ä¸ªï¼Œå¯é…ç½®ï¼‰
	workerCount := 20
	if cfg.DepthSettings.MaxDepth > 2 {
		workerCount = 30 // æ·±åº¦çˆ¬å–æ—¶å¢åŠ workeræ•°
	}
	
	// é€Ÿç‡é™åˆ¶ï¼ˆæ¯ç§’æœ€å¤š20ä¸ªè¯·æ±‚ï¼Œé¿å…è¿‡è½½ï¼‰
	maxQPS := 20
	
	spider := &Spider{
		config:             cfg,
		staticCrawler:      NewStaticCrawler(cfg, resultChan, stopChan),
		dynamicCrawler:     NewDynamicCrawler(),
		jsAnalyzer:         NewJSAnalyzer(),
		paramHandler:       NewParamHandler(),
		duplicateHandler:   NewDuplicateHandler(cfg.DeduplicationSettings.SimilarityThreshold),
		smartDeduplication: NewSmartDeduplication(), // åˆå§‹åŒ–æ™ºèƒ½å»é‡
		cdnDetector:        NewCDNDetector(), // åˆå§‹åŒ–CDNæ£€æµ‹å™¨
		workerPool:         NewWorkerPool(workerCount, maxQPS), // åˆå§‹åŒ–å·¥ä½œæ± 
		
		// åˆå§‹åŒ–æ–°å¢ç»„ä»¶
		formFiller:         NewSmartFormFiller(),          // æ™ºèƒ½è¡¨å•å¡«å……å™¨
		advancedScope:      nil,                           // å°†åœ¨Startä¸­åˆå§‹åŒ–
		perfOptimizer:      NewPerformanceOptimizer(500),  // æ€§èƒ½ä¼˜åŒ–å™¨ï¼ˆé™åˆ¶500MBï¼‰
		
		// åˆå§‹åŒ–é«˜çº§åŠŸèƒ½ç»„ä»¶
		techDetector:       NewTechStackDetector(),        // æŠ€æœ¯æ ˆæ£€æµ‹å™¨
		sensitiveDetector:  NewSensitiveInfoDetector(),    // æ•æ„Ÿä¿¡æ¯æ£€æµ‹å™¨
		passiveCrawler:     nil,                           // æŒ‰éœ€åˆ›å»º
		domSimilarity:      NewDOMSimilarityDetector(0.85), // DOMç›¸ä¼¼åº¦æ£€æµ‹å™¨ï¼ˆé˜ˆå€¼85%ï¼‰
		sitemapCrawler:     NewSitemapCrawler(),           // Sitemapçˆ¬å–å™¨
		assetClassifier:    NewAssetClassifier(),          // é™æ€èµ„æºåˆ†ç±»å™¨
		ipDetector:         NewIPDetector(),               // IPåœ°å€æ£€æµ‹å™¨
		
		hiddenPathDiscovery: nil, // å°†åœ¨Startæ–¹æ³•ä¸­åˆå§‹åŒ–ï¼Œéœ€è¦ç”¨æˆ·ä»£ç†
		results:            make([]*Result, 0),
		externalLinks:      make([]string, 0),
		hiddenPaths:        make([]string, 0),
		securityFindings:   make([]string, 0),
		crossDomainJS:      make([]string, 0),
		detectedTechs:      make([]*TechInfo, 0),
		sensitiveFindings:  make([]*SensitiveInfo, 0),
		sitemapURLs:        make([]string, 0),
		robotsURLs:         make([]string, 0),
		visitedURLs:        make(map[string]bool),
		
		// åˆå§‹åŒ–èµ„æºç®¡ç†
		done:               make(chan struct{}),
		closed:             false,
	}
	
	// é…ç½®å„ä¸ªç»„ä»¶
	spider.staticCrawler.Configure(cfg)
	spider.dynamicCrawler.Configure(cfg)
	
	// è®¾ç½®JSåˆ†æå™¨çš„ç›®æ ‡åŸŸå
	spider.jsAnalyzer.SetTargetDomain(cfg.TargetURL)
	
	return spider
}

// Start å¼€å§‹çˆ¬å–
func (s *Spider) Start(targetURL string) error {
	// ç¡®ä¿èµ„æºæ¸…ç†ï¼ˆä¼˜åŒ–ï¼šé˜²æ­¢æ³„æ¼ï¼‰
	defer s.cleanup()
	
	// è§£æç›®æ ‡URLå¹¶æå–åŸŸå
	parsedURL, err := url.Parse(targetURL)
	if err != nil {
		return fmt.Errorf("æ— æ•ˆçš„URL: %v", err)
	}
	s.targetDomain = parsedURL.Host
	
	// è®¾ç½®JSåˆ†æå™¨çš„ç›®æ ‡åŸŸå
	s.jsAnalyzer.SetTargetDomain(s.targetDomain)
	
	// åˆå§‹åŒ–é«˜çº§ä½œç”¨åŸŸæ§åˆ¶
	s.advancedScope = NewAdvancedScope(s.targetDomain)
	s.advancedScope.SetMode(ScopeRDN) // æ ¹åŸŸåæ¨¡å¼
	s.advancedScope.PresetStaticFilterScope() // è¿‡æ»¤é™æ€èµ„æº
	
	// åˆå§‹åŒ–å­åŸŸåæå–å™¨
	s.subdomainExtractor = NewSubdomainExtractor(targetURL)
	
	// æ£€æŸ¥æ˜¯å¦é‡å¤
	if s.duplicateHandler.IsDuplicateURL(targetURL) {
		return fmt.Errorf("URLå·²å¤„ç†è¿‡: %s", targetURL)
	}
	
	fmt.Printf("å¼€å§‹çˆ¬å–URL: %s\n", targetURL)
	fmt.Printf("é™åˆ¶åŸŸåèŒƒå›´: %s\n", s.targetDomain)
	fmt.Printf("\nã€å·²å¯ç”¨åŠŸèƒ½ã€‘Spider Enhanced v2.2\n")
	fmt.Printf("  âœ“ è·¨åŸŸJSåˆ†æï¼ˆæ”¯æŒ60+ä¸ªCDNï¼‰\n")
	fmt.Printf("  âœ“ æ™ºèƒ½è¡¨å•å¡«å……ï¼ˆæ”¯æŒ20+ç§å­—æ®µç±»å‹ï¼‰\n")
	fmt.Printf("  âœ“ ä½œç”¨åŸŸç²¾ç¡®æ§åˆ¶ï¼ˆ10ä¸ªè¿‡æ»¤ç»´åº¦ï¼‰\n")
	fmt.Printf("  âœ“ æ€§èƒ½ä¼˜åŒ–ï¼ˆå¯¹è±¡æ± +è¿æ¥æ± ï¼‰\n")
	fmt.Printf("  âœ“ æŠ€æœ¯æ ˆè¯†åˆ«ï¼ˆ15+ç§æ¡†æ¶ï¼‰\n")
	fmt.Printf("  âœ“ æ•æ„Ÿä¿¡æ¯æ£€æµ‹ï¼ˆ30+ç§æ¨¡å¼ï¼‰\n")
	fmt.Printf("  âœ“ JavaScriptäº‹ä»¶è§¦å‘ï¼ˆç‚¹å‡»ã€æ‚¬åœã€è¾“å…¥ã€æ»šåŠ¨ï¼‰\n")
	fmt.Printf("  âœ“ AJAXè¯·æ±‚æ‹¦æˆªï¼ˆåŠ¨æ€URLæ•è·ï¼‰ğŸ†•\n")
	fmt.Printf("  âœ“ å¢å¼ºJSåˆ†æï¼ˆå¯¹è±¡ã€è·¯ç”±ã€é…ç½®ï¼‰ğŸ†•\n")
	fmt.Printf("  âœ“ é™æ€èµ„æºåˆ†ç±»ï¼ˆ7ç§ç±»å‹ï¼‰ğŸ†•\n")
	fmt.Printf("  âœ“ IPåœ°å€æ£€æµ‹ï¼ˆå†…ç½‘æ³„éœ²è¯†åˆ«ï¼‰ğŸ†•\n")
	fmt.Printf("  âœ“ URLä¼˜å…ˆçº§æ’åºï¼ˆæ™ºèƒ½çˆ¬å–ç­–ç•¥ï¼‰ğŸ†•\n")
	fmt.Printf("\nçˆ¬å–é…ç½®:\n")
	fmt.Printf("  æ·±åº¦: %d å±‚ | å¹¶å‘: 20-30 | æœ€å¤§URL: 500\n", s.config.DepthSettings.MaxDepth)
	fmt.Printf("\n")
	
	// åˆå§‹åŒ–éšè—è·¯å¾„å‘ç°å™¨
	userAgent := ""
	if len(s.config.AntiDetectionSettings.UserAgents) > 0 {
		userAgent = s.config.AntiDetectionSettings.UserAgents[0]
	}
	s.hiddenPathDiscovery = NewHiddenPathDiscovery(targetURL, userAgent)
	
	// === ä¼˜åŒ–ï¼šå…ˆçˆ¬å–sitemap.xmlå’Œrobots.txt ===
	fmt.Println("å¼€å§‹çˆ¬å–sitemap.xmlå’Œrobots.txt...")
	sitemapURLs, robotsInfo := s.sitemapCrawler.GetAllURLs(targetURL)
	s.mutex.Lock()
	s.sitemapURLs = sitemapURLs
	s.robotsURLs = append(robotsInfo.DisallowPaths, robotsInfo.AllowPaths...)
	s.mutex.Unlock()
	
	if len(sitemapURLs) > 0 {
		fmt.Printf("  [Sitemap] å‘ç° %d ä¸ªURL\n", len(sitemapURLs))
	}
	if len(robotsInfo.DisallowPaths) > 0 {
		fmt.Printf("  [robots.txt] å‘ç° %d ä¸ªDisallowè·¯å¾„ï¼ˆæ¸—é€æµ‹è¯•é‡ç‚¹ï¼‰\n", len(robotsInfo.DisallowPaths))
	}
	if len(robotsInfo.SitemapURLs) > 0 {
		fmt.Printf("  [robots.txt] å‘ç° %d ä¸ªé¢å¤–sitemap\n", len(robotsInfo.SitemapURLs))
	}
	
	// å°†sitemapå’Œrobotsä¸­çš„URLæ·»åŠ åˆ°å¾…çˆ¬å–åˆ—è¡¨
	for _, u := range sitemapURLs {
		s.visitedURLs[u] = false // æ ‡è®°ä¸ºå¾…çˆ¬å–
	}
	for _, u := range robotsInfo.DisallowPaths {
		s.visitedURLs[u] = false // Disallowè·¯å¾„ä¹Ÿè¦çˆ¬å–
	}
	
	// å¼€å§‹éšè—è·¯å¾„å‘ç°
	fmt.Println("å¼€å§‹éšè—è·¯å¾„å‘ç°...")
	hiddenPaths := s.hiddenPathDiscovery.DiscoverAllHiddenPaths()
	s.mutex.Lock()
	s.hiddenPaths = append(s.hiddenPaths, hiddenPaths...)
	s.mutex.Unlock()
	fmt.Printf("å‘ç° %d ä¸ªéšè—è·¯å¾„\n", len(hiddenPaths))
	
	// æ ¹æ®é…ç½®å†³å®šä½¿ç”¨å“ªç§çˆ¬è™«ç­–ç•¥
	if s.config.StrategySettings.EnableStaticCrawler {
		fmt.Println("ä½¿ç”¨é™æ€çˆ¬è™«...")
		result, err := s.staticCrawler.Crawl(parsedURL)
		if err != nil {
			fmt.Printf("é™æ€çˆ¬è™«é”™è¯¯: %v\n", err)
		} else {
			s.addResult(result)
			fmt.Printf("é™æ€çˆ¬è™«å®Œæˆï¼Œå‘ç° %d ä¸ªé“¾æ¥, %d ä¸ªèµ„æº, %d ä¸ªè¡¨å•, %d ä¸ªAPI\n", 
				len(result.Links), len(result.Assets), len(result.Forms), len(result.APIs))
		}
	}
	
	// å¦‚æœå¯ç”¨äº†åŠ¨æ€çˆ¬è™«ï¼Œæ€»æ˜¯ä½¿ç”¨ï¼ˆPhase 2/3ä¼˜åŒ–ï¼šæ•è·AJAXå’ŒJSåŠ¨æ€å†…å®¹ï¼‰
	if s.config.StrategySettings.EnableDynamicCrawler {
		fmt.Println("ä½¿ç”¨åŠ¨æ€çˆ¬è™«ï¼ˆæ•è·AJAXå’ŒåŠ¨æ€JSå†…å®¹ï¼‰...")
		result, err := s.dynamicCrawler.Crawl(parsedURL)
		if err != nil {
			fmt.Printf("åŠ¨æ€çˆ¬è™«é”™è¯¯: %v\n", err)
		} else {
			s.addResult(result)
			fmt.Printf("åŠ¨æ€çˆ¬è™«å®Œæˆï¼Œå‘ç° %d ä¸ªé“¾æ¥, %d ä¸ªèµ„æº, %d ä¸ªè¡¨å•, %d ä¸ªAPI\n", 
				len(result.Links), len(result.Assets), len(result.Forms), len(result.APIs))
		}
	}
	
	// å¤„ç†å‘ç°çš„å‚æ•°ï¼ˆåŒ…æ‹¬GETå‚æ•°çˆ†ç ´ï¼‰
	paramFuzzURLs := s.processParams(targetURL)
	
	// å°†å‚æ•°çˆ†ç ´ç”Ÿæˆçš„URLæ·»åŠ åˆ°ç¬¬ä¸€ä¸ªç»“æœçš„Linksä¸­ï¼Œä»¥ä¾¿é€’å½’çˆ¬å–ï¼ˆä¼˜åŒ–ï¼šä¿®å¤å¹¶å‘å®‰å…¨ï¼‰
	s.mutex.Lock()
	if len(paramFuzzURLs) > 1 && len(s.results) > 0 {
		// æ·»åŠ åˆ°ç¬¬ä¸€ä¸ªç»“æœçš„Linksä¸­ï¼ˆä½œä¸ºå‘ç°çš„é“¾æ¥ï¼‰
		s.results[0].Links = append(s.results[0].Links, paramFuzzURLs...)
		s.mutex.Unlock()
		fmt.Printf("  [å‚æ•°çˆ†ç ´] å·²å°† %d ä¸ªçˆ†ç ´URLæ·»åŠ åˆ°çˆ¬å–é˜Ÿåˆ—\n", len(paramFuzzURLs))
	} else {
		s.mutex.Unlock()
	}
	
	// å¤„ç†å‘ç°çš„è¡¨å•ï¼ˆåŒ…æ‹¬POSTå‚æ•°çˆ†ç ´ï¼‰
	s.processForms(targetURL)
	
	// åˆ†æè·¨åŸŸJSæ–‡ä»¶ï¼ˆåœ¨é€’å½’çˆ¬å–ä¹‹å‰ï¼‰
	s.processCrossDomainJS()
	
	// å¦‚æœå¯ç”¨äº†é€’å½’çˆ¬å–ï¼Œç»§ç»­çˆ¬å–å‘ç°çš„é“¾æ¥ï¼ˆçœŸæ­£çš„å¤šå±‚é€’å½’ï¼‰
	if s.config.DepthSettings.MaxDepth > 1 {
		s.crawlRecursivelyMultiLayer()
	}
	
	return nil
}

// shouldUseDynamicCrawler åˆ¤æ–­æ˜¯å¦éœ€è¦ä½¿ç”¨åŠ¨æ€çˆ¬è™«
func (s *Spider) shouldUseDynamicCrawler() bool {
	// å¦‚æœæ²¡æœ‰å‘ç°è¶³å¤Ÿçš„é“¾æ¥æˆ–APIï¼Œå¯èƒ½éœ€è¦åŠ¨æ€çˆ¬è™«
	if len(s.results) == 0 {
		return true
	}
	
	// æ£€æŸ¥æœ€è¿‘çš„ç»“æœ
	lastResult := s.results[len(s.results)-1]
	// é™ä½è§¦å‘åŠ¨æ€çˆ¬è™«çš„é˜ˆå€¼ï¼ˆæ›´å®¹æ˜“è§¦å‘ï¼‰
	if len(lastResult.Links) < 20 && len(lastResult.APIs) < 10 {
		return true
	}
	
	return false
}

// addResult æ·»åŠ çˆ¬å–ç»“æœï¼ˆå¢å¼ºç‰ˆï¼šåŒ…å«DOMç›¸ä¼¼åº¦æ£€æµ‹ã€æŠ€æœ¯æ ˆæ£€æµ‹å’Œæ•æ„Ÿä¿¡æ¯æ£€æµ‹ï¼‰
func (s *Spider) addResult(result *Result) {
	s.mutex.Lock()
	defer s.mutex.Unlock()
	
	// å¦‚æœæœ‰HTMLå†…å®¹ï¼Œå…ˆè¿›è¡ŒDOMç›¸ä¼¼åº¦æ£€æµ‹
	if result.HTMLContent != "" && s.domSimilarity != nil {
		isSimilar, record := s.domSimilarity.CheckSimilarity(result.URL, result.HTMLContent)
		if isSimilar && record != nil {
			fmt.Printf("  [DOMç›¸ä¼¼åº¦] å‘ç°ç›¸ä¼¼é¡µé¢ï¼ç›¸ä¼¼åº¦: %.1f%%, ç›¸ä¼¼äº: %s\n", 
				record.Similarity*100, record.SimilarToURL)
			fmt.Printf("  [DOMç›¸ä¼¼åº¦] åŸå› : %s\n", record.Reason)
			fmt.Printf("  [DOMç›¸ä¼¼åº¦] âœ“ è·³è¿‡é‡å¤çˆ¬å–ï¼ŒèŠ‚çœèµ„æº\n")
			// ç›¸ä¼¼é¡µé¢ä»ç„¶è®°å½•ï¼Œä½†æ ‡è®°ä¸ºå·²è·³è¿‡
			result.IsSimilar = true
			result.SimilarToURL = record.SimilarToURL
		}
	}
	
	s.results = append(s.results, result)
	
	// å¦‚æœæœ‰HTMLå†…å®¹ï¼Œè¿›è¡Œé«˜çº§æ£€æµ‹
	if result.HTMLContent != "" {
		// æŠ€æœ¯æ ˆæ£€æµ‹
		if s.techDetector != nil {
			techs := s.techDetector.DetectFromContent(result.HTMLContent, result.Headers)
			s.detectedTechs = append(s.detectedTechs, techs...)
			
			if len(techs) > 0 {
				techNames := make([]string, 0)
				for _, tech := range techs {
					if tech.Version != "" {
						techNames = append(techNames, tech.Name+" "+tech.Version)
					} else {
						techNames = append(techNames, tech.Name)
					}
				}
				fmt.Printf("  [æŠ€æœ¯æ ˆ] æ£€æµ‹åˆ°: %s\n", strings.Join(techNames, ", "))
			}
		}
		
		// æ•æ„Ÿä¿¡æ¯æ£€æµ‹
		if s.sensitiveDetector != nil {
			// æ‰«æHTMLå†…å®¹
			findings := s.sensitiveDetector.Scan(result.HTMLContent, result.URL)
			s.sensitiveFindings = append(s.sensitiveFindings, findings...)
			
			// æ‰«æHTTPå¤´
			if len(result.Headers) > 0 {
				headerContent := ""
				for key, value := range result.Headers {
					headerContent += key + ": " + value + "\n"
				}
				headerFindings := s.sensitiveDetector.Scan(headerContent, result.URL+" (Headers)")
				s.sensitiveFindings = append(s.sensitiveFindings, headerFindings...)
				findings = append(findings, headerFindings...)
			}
			
			if len(findings) > 0 {
				highCount := 0
				for _, finding := range findings {
					if finding.Severity == "HIGH" {
						highCount++
					}
				}
				
				if highCount > 0 {
					fmt.Printf("  [æ•æ„Ÿä¿¡æ¯] âš ï¸  å‘ç° %d å¤„é«˜å±æ•æ„Ÿä¿¡æ¯ï¼\n", highCount)
				} else if len(findings) > 0 {
					fmt.Printf("  [æ•æ„Ÿä¿¡æ¯] å‘ç° %d å¤„æ•æ„Ÿä¿¡æ¯\n", len(findings))
				}
			}
		}
	}
}

// addResultWithDetection æ·»åŠ ç»“æœå¹¶è¿›è¡Œæ£€æµ‹
func (s *Spider) addResultWithDetection(result *Result, response *http.Response, htmlContent string) {
	s.mutex.Lock()
	defer s.mutex.Unlock()
	
	s.results = append(s.results, result)
	
	// æŠ€æœ¯æ ˆæ£€æµ‹
	if response != nil && s.techDetector != nil {
		techs := s.techDetector.Detect(response, htmlContent)
		s.detectedTechs = append(s.detectedTechs, techs...)
		
		if len(techs) > 0 {
			fmt.Printf("  [æŠ€æœ¯æ ˆ] æ£€æµ‹åˆ°: ")
			techNames := make([]string, 0)
			for _, tech := range techs {
				if tech.Version != "" {
					techNames = append(techNames, tech.Name+" "+tech.Version)
				} else {
					techNames = append(techNames, tech.Name)
				}
			}
			fmt.Printf("%s\n", strings.Join(techNames, ", "))
		}
	}
	
	// æ•æ„Ÿä¿¡æ¯æ£€æµ‹
	if s.sensitiveDetector != nil {
		findings := s.sensitiveDetector.Scan(htmlContent, result.URL)
		s.sensitiveFindings = append(s.sensitiveFindings, findings...)
		
		if len(findings) > 0 {
			highCount := 0
			for _, finding := range findings {
				if finding.Severity == "HIGH" {
					highCount++
				}
			}
			
			if highCount > 0 {
				fmt.Printf("  [æ•æ„Ÿä¿¡æ¯] âš ï¸  å‘ç° %d å¤„é«˜å±æ•æ„Ÿä¿¡æ¯ï¼\n", highCount)
			} else {
				fmt.Printf("  [æ•æ„Ÿä¿¡æ¯] å‘ç° %d å¤„æ•æ„Ÿä¿¡æ¯\n", len(findings))
			}
		}
	}
	
	// å­åŸŸåæå–
	if s.subdomainExtractor != nil && htmlContent != "" {
		// ä»HTMLå†…å®¹æå–å­åŸŸå
		subdomains := s.subdomainExtractor.ExtractFromHTML(htmlContent)
		if len(subdomains) > 0 {
			fmt.Printf("  [å­åŸŸå] å‘ç° %d ä¸ªæ–°å­åŸŸå\n", len(subdomains))
		}
		
		// ä»URLæœ¬èº«æå–
		s.subdomainExtractor.ExtractFromURL(result.URL)
	}
}

// GetResults è·å–æ‰€æœ‰çˆ¬å–ç»“æœ
func (s *Spider) GetResults() []*Result {
	s.mutex.Lock()
	defer s.mutex.Unlock()
	
	// è¿”å›ç»“æœå‰¯æœ¬
	results := make([]*Result, len(s.results))
	copy(results, s.results)
	return results
}

// processParams å¤„ç†å‚æ•°å˜ä½“ç”Ÿæˆå’Œå®‰å…¨åˆ†æ
func (s *Spider) processParams(rawURL string) []string {
	// æå–å‚æ•°
	params, err := s.paramHandler.ExtractParams(rawURL)
	if err != nil {
		// å¦‚æœæå–å‚æ•°å¤±è´¥ï¼Œè‡³å°‘è¿”å›åŸå§‹URL
		return []string{rawURL}
	}
	
	// === æ–°å¢ï¼šå¯¹æ— å‚æ•°URLè¿›è¡Œå‚æ•°çˆ†ç ´ ===
	if len(params) == 0 && s.config.StrategySettings.EnableParamFuzzing {
		fmt.Printf("  [å‚æ•°çˆ†ç ´] æ£€æµ‹åˆ°æ— å‚æ•°URLï¼Œå¼€å§‹å‚æ•°æšä¸¾...\n")
		
		// ç”Ÿæˆå‚æ•°çˆ†ç ´åˆ—è¡¨
		fuzzList := s.paramHandler.GenerateParameterFuzzList(rawURL)
		
		// åº”ç”¨é™åˆ¶ï¼ˆé¿å…ç”Ÿæˆè¿‡å¤šURLï¼‰
		if s.config.StrategySettings.ParamFuzzLimit > 0 && len(fuzzList) > s.config.StrategySettings.ParamFuzzLimit {
			fuzzList = fuzzList[:s.config.StrategySettings.ParamFuzzLimit]
			fmt.Printf("  [å‚æ•°çˆ†ç ´] é™åˆ¶çˆ†ç ´æ•°é‡ä¸º %d ä¸ªï¼ˆå¯é…ç½®ï¼‰\n", s.config.StrategySettings.ParamFuzzLimit)
		}
		
		if len(fuzzList) > 0 {
			fmt.Printf("  [å‚æ•°çˆ†ç ´] ä¸ºæ— å‚æ•°URLç”Ÿæˆ %d ä¸ªå‚æ•°çˆ†ç ´å˜ä½“\n", len(fuzzList))
			fmt.Printf("  [å‚æ•°çˆ†ç ´] ç¤ºä¾‹: %s\n", fuzzList[0])
			if len(fuzzList) > 1 {
				fmt.Printf("  [å‚æ•°çˆ†ç ´] ç¤ºä¾‹: %s\n", fuzzList[1])
			}
			if len(fuzzList) > 2 {
				fmt.Printf("  [å‚æ•°çˆ†ç ´] ç¤ºä¾‹: %s\n", fuzzList[2])
			}
			fmt.Printf("  [å‚æ•°çˆ†ç ´] ... è¿˜æœ‰ %d ä¸ªçˆ†ç ´URL\n", len(fuzzList)-3)
			
			return fuzzList
		}
		
		// å¦‚æœçˆ†ç ´å¤±è´¥ï¼Œè¿”å›åŸå§‹URL
		return []string{rawURL}
	}
	
	// === åŸæœ‰é€»è¾‘ï¼šå¯¹æœ‰å‚æ•°URLè¿›è¡Œå˜ä½“ç”Ÿæˆï¼ˆçº¯çˆ¬è™«æ¨¡å¼ï¼Œä¸å«æ”»å‡»payloadï¼‰ ===
	// å¯¹æ¯ä¸ªå‚æ•°è¿›è¡Œä¿¡æ¯åˆ†æï¼ˆä»…è®°å½•ï¼Œä¸ç”¨äºæ”»å‡»ï¼‰
	for paramName := range params {
		risk, level := s.paramHandler.AnalyzeParameterSecurity(paramName)
		if level >= 2 { // ä¸­ç­‰é£é™©ä»¥ä¸Š
			finding := fmt.Sprintf("PARAM_INFO: %s - %s (Risk Level: %d)", paramName, risk, level)
			s.mutex.Lock()
			s.securityFindings = append(s.securityFindings, finding)
			s.mutex.Unlock()
			fmt.Printf("  [å‚æ•°åˆ†æ] %s\n", finding)
		}
	}
	
	// ç”Ÿæˆå‚æ•°å˜ä½“ï¼ˆåªä½¿ç”¨æ­£å¸¸å€¼ï¼Œä¸å«æ”»å‡»payloadï¼‰
	variations := s.paramHandler.GenerateParamVariations(rawURL)
	
	// === ç§»é™¤ï¼šå®‰å…¨æµ‹è¯•å˜ä½“ï¼ˆæ”»å‡»æ€§payloadï¼‰ ===
	// ä½œä¸ºçº¯çˆ¬è™«å·¥å…·ï¼Œä¸åº”å‘é€SQLæ³¨å…¥ã€XSSç­‰æ”»å‡»æ€§payload
	// securityVariations := s.paramHandler.GenerateSecurityTestVariations(rawURL)
	// variations = append(variations, securityVariations...)
	
	// å¦‚æœæ²¡æœ‰ç”Ÿæˆå˜ä½“ï¼Œè¿”å›åŸå§‹URL
	if len(variations) == 0 {
		return []string{rawURL}
	}
	
	// æ‰“å°ç”Ÿæˆçš„å˜ä½“
	fmt.Printf("  [å‚æ•°å˜ä½“] ä¸ºURLç”Ÿæˆ %d ä¸ªå‚æ•°å˜ä½“ï¼ˆæ­£å¸¸æµ‹è¯•å€¼ï¼‰\n", len(variations))
	for i, variation := range variations {
		if i < 5 { // åªæ˜¾ç¤ºå‰5ä¸ªï¼Œé¿å…è¾“å‡ºè¿‡å¤š
			fmt.Printf("    å˜ä½“: %s\n", variation)
		}
	}
	if len(variations) > 5 {
		fmt.Printf("    ... è¿˜æœ‰ %d ä¸ªå˜ä½“\n", len(variations)-5)
	}
	
	return variations
}

// processForms å¤„ç†è¡¨å•ï¼ˆåŒ…æ‹¬POSTå‚æ•°çˆ†ç ´ï¼‰
func (s *Spider) processForms(targetURL string) {
	// æ£€æŸ¥æ˜¯å¦å¯ç”¨POSTå‚æ•°çˆ†ç ´
	if !s.config.StrategySettings.EnablePOSTParamFuzzing {
		return
	}
	
	s.mutex.Lock()
	defer s.mutex.Unlock()
	
	// æ”¶é›†æ‰€æœ‰ç©ºè¡¨å•æˆ–æ— æ•ˆè¡¨å•
	emptyForms := make([]string, 0)
	totalForms := 0
	
	for _, result := range s.results {
		totalForms += len(result.Forms)
		
		for _, form := range result.Forms {
			// æ£€æŸ¥è¡¨å•æ˜¯å¦æœ‰æœ‰æ•ˆå­—æ®µ
			hasValidFields := false
			for _, field := range form.Fields {
				// è·³è¿‡æäº¤æŒ‰é’®å’Œæ™®é€šæŒ‰é’®
				fieldTypeLower := strings.ToLower(field.Type)
				if fieldTypeLower != "submit" && fieldTypeLower != "button" && field.Name != "" {
					hasValidFields = true
					break
				}
			}
			
			// å¦‚æœè¡¨å•æ²¡æœ‰æœ‰æ•ˆå­—æ®µï¼Œæ·»åŠ åˆ°çˆ†ç ´åˆ—è¡¨
			if !hasValidFields {
				emptyForms = append(emptyForms, form.Action)
			}
		}
		
		// åŒæ ·æ£€æŸ¥POSTè¯·æ±‚
		for _, postReq := range result.POSTRequests {
			// å¦‚æœPOSTè¯·æ±‚æ²¡æœ‰å‚æ•°æˆ–å‚æ•°ä¸ºç©º
			if len(postReq.Parameters) == 0 {
				emptyForms = append(emptyForms, postReq.URL)
			}
		}
	}
	
	// å»é‡
	uniqueEmptyForms := make(map[string]bool)
	for _, formURL := range emptyForms {
		uniqueEmptyForms[formURL] = true
	}
	
	if len(uniqueEmptyForms) == 0 {
		if totalForms > 0 {
			fmt.Printf("  [POSTçˆ†ç ´] å‘ç° %d ä¸ªè¡¨å•ï¼Œå…¨éƒ¨æœ‰å­—æ®µï¼Œæ— éœ€çˆ†ç ´\n", totalForms)
		}
		return
	}
	
	fmt.Printf("  [POSTçˆ†ç ´] æ£€æµ‹åˆ° %d ä¸ªç©ºè¡¨å•ï¼Œå¼€å§‹POSTå‚æ•°çˆ†ç ´...\n", len(uniqueEmptyForms))
	
	// å¯¹æ¯ä¸ªç©ºè¡¨å•ç”ŸæˆPOSTçˆ†ç ´è¯·æ±‚
	totalPOSTFuzzRequests := 0
	for formURL := range uniqueEmptyForms {
		// ç”ŸæˆPOSTå‚æ•°çˆ†ç ´åˆ—è¡¨
		postFuzzList := s.paramHandler.GeneratePOSTParameterFuzzList(formURL)
		
		// åº”ç”¨é™åˆ¶
		if s.config.StrategySettings.POSTParamFuzzLimit > 0 && len(postFuzzList) > s.config.StrategySettings.POSTParamFuzzLimit {
			postFuzzList = postFuzzList[:s.config.StrategySettings.POSTParamFuzzLimit]
		}
		
		if len(postFuzzList) > 0 {
			// æ·»åŠ åˆ°ç¬¬ä¸€ä¸ªç»“æœçš„POSTRequestsä¸­
			if len(s.results) > 0 {
				s.results[0].POSTRequests = append(s.results[0].POSTRequests, postFuzzList...)
			}
			totalPOSTFuzzRequests += len(postFuzzList)
		}
	}
	
	if totalPOSTFuzzRequests > 0 {
		fmt.Printf("  [POSTçˆ†ç ´] ä¸º %d ä¸ªç©ºè¡¨å•ç”Ÿæˆ %d ä¸ªPOSTçˆ†ç ´è¯·æ±‚\n", len(uniqueEmptyForms), totalPOSTFuzzRequests)
		fmt.Printf("  [POSTçˆ†ç ´] ç¤ºä¾‹: POST %s {username=admin, password=admin123}\n", emptyForms[0])
		if len(uniqueEmptyForms) > 1 {
			fmt.Printf("  [POSTçˆ†ç ´] ç¤ºä¾‹: POST %s {search=test, q=admin}\n", emptyForms[0])
		}
	}
}

// processCrossDomainJS å¤„ç†è·¨åŸŸJSæ–‡ä»¶
func (s *Spider) processCrossDomainJS() {
	fmt.Println("\nå¼€å§‹åˆ†æè·¨åŸŸJSæ–‡ä»¶...")
	
	// æ”¶é›†æ‰€æœ‰èµ„æºé“¾æ¥
	allAssets := make(map[string]bool)
	
	s.mutex.Lock()
	for _, result := range s.results {
		for _, asset := range result.Assets {
			allAssets[asset] = true
		}
		// ä¹Ÿæ£€æŸ¥Linksä¸­çš„JSæ–‡ä»¶
		for _, link := range result.Links {
			if strings.HasSuffix(strings.ToLower(link), ".js") {
				allAssets[link] = true
			}
		}
	}
	s.mutex.Unlock()
	
	// è¿‡æ»¤å‡ºéœ€è¦åˆ†æçš„JSæ–‡ä»¶
	jsToAnalyze := make([]string, 0)
	for asset := range allAssets {
		// æ£€æŸ¥æ˜¯å¦ä¸ºJSæ–‡ä»¶
		if !strings.HasSuffix(strings.ToLower(asset), ".js") {
			continue
		}
		
		// è§£æURL
		parsedURL, err := url.Parse(asset)
		if err != nil {
			continue
		}
		
		domain := parsedURL.Host
		if domain == "" {
			continue
		}
		
		// æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†æ
		shouldAnalyze := false
		reason := ""
		
		// 1. æ˜¯ç›®æ ‡åŸŸå - å·²ç»æ­£å¸¸çˆ¬å–äº†ï¼Œä¸éœ€è¦ç‰¹æ®Šå¤„ç†
		if domain == s.targetDomain {
			continue
		}
		
		// 2. æ˜¯åŒæºåŸŸå
		if s.cdnDetector.IsSameBaseDomain(domain, s.targetDomain) {
			shouldAnalyze = true
			reason = "åŒæºåŸŸå"
		}
		
		// 3. æ˜¯å·²çŸ¥CDN
		if s.cdnDetector.IsCDN(domain) {
			shouldAnalyze = true
			cdnInfo := s.cdnDetector.GetCDNInfo(domain)
			reason = cdnInfo
		}
		
		if shouldAnalyze {
			jsToAnalyze = append(jsToAnalyze, asset)
			fmt.Printf("  å‘ç°è·¨åŸŸJS: %s (%s)\n", asset, reason)
		}
	}
	
	if len(jsToAnalyze) == 0 {
		fmt.Println("æœªå‘ç°éœ€è¦åˆ†æçš„è·¨åŸŸJSæ–‡ä»¶")
		return
	}
	
	fmt.Printf("å‡†å¤‡åˆ†æ %d ä¸ªè·¨åŸŸJSæ–‡ä»¶...\n", len(jsToAnalyze))
	
	// åˆ†ææ¯ä¸ªJSæ–‡ä»¶
	totalURLsFound := 0
	for _, jsURL := range jsToAnalyze {
		urls := s.analyzeExternalJS(jsURL)
		if len(urls) > 0 {
			fmt.Printf("  ä» %s æå–äº† %d ä¸ªURL\n", jsURL, len(urls))
			totalURLsFound += len(urls)
			
			// æ·»åŠ åˆ°è·¨åŸŸJSå‘ç°åˆ—è¡¨
			s.mutex.Lock()
			s.crossDomainJS = append(s.crossDomainJS, urls...)
			s.mutex.Unlock()
			
			// æ·»åŠ åˆ°çˆ¬å–é˜Ÿåˆ—ï¼ˆå¦‚æœå¯ç”¨é€’å½’çˆ¬å–ï¼‰
			if s.config.DepthSettings.MaxDepth > 1 {
				for _, u := range urls {
					// æ·»åŠ åˆ°ç»“æœä¸­ï¼ˆä½œä¸ºå‘ç°çš„é“¾æ¥ï¼‰
					if len(s.results) > 0 {
						s.results[0].Links = append(s.results[0].Links, u)
					}
				}
			}
		}
	}
	
	fmt.Printf("è·¨åŸŸJSåˆ†æå®Œæˆï¼å…±ä» %d ä¸ªJSæ–‡ä»¶ä¸­æå–äº† %d ä¸ªç›®æ ‡åŸŸåURL\n\n", len(jsToAnalyze), totalURLsFound)
}

// analyzeExternalJS ä¸‹è½½å¹¶åˆ†æå¤–éƒ¨JSæ–‡ä»¶ï¼ˆä½¿ç”¨æ€§èƒ½ä¼˜åŒ–ï¼‰
func (s *Spider) analyzeExternalJS(jsURL string) []string {
	// ä½¿ç”¨æ€§èƒ½ä¼˜åŒ–çš„HTTPå®¢æˆ·ç«¯
	req, err := http.NewRequest("GET", jsURL, nil)
	if err != nil {
		fmt.Printf("    åˆ›å»ºè¯·æ±‚å¤±è´¥: %v\n", err)
		return []string{}
	}
	
	// è®¾ç½®User-Agent
	if len(s.config.AntiDetectionSettings.UserAgents) > 0 {
		req.Header.Set("User-Agent", s.config.AntiDetectionSettings.UserAgents[0])
	} else {
		req.Header.Set("User-Agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
	}
	
	// ä½¿ç”¨ä¼˜åŒ–çš„HTTPå®¢æˆ·ç«¯ï¼ˆå¸¦è¿æ¥æ± ï¼‰
	resp, err := s.perfOptimizer.DoRequest(req)
	if err != nil {
		fmt.Printf("    ä¸‹è½½å¤±è´¥: %v\n", err)
		return []string{}
	}
	defer resp.Body.Close()
	
	// æ£€æŸ¥çŠ¶æ€ç 
	if resp.StatusCode != 200 {
		fmt.Printf("    HTTP %d\n", resp.StatusCode)
		return []string{}
	}
	
	// ä½¿ç”¨Bufferæ± è¯»å–å†…å®¹
	buf := s.perfOptimizer.GetBuffer()
	defer s.perfOptimizer.PutBuffer(buf)
	
	// é™åˆ¶æ–‡ä»¶å¤§å°ï¼ˆæœ€å¤§5MBï¼‰
	const maxSize = 5 * 1024 * 1024
	limitedReader := &io.LimitedReader{R: resp.Body, N: maxSize}
	
	_, err = buf.ReadFrom(limitedReader)
	if err != nil {
		fmt.Printf("    è¯»å–å†…å®¹å¤±è´¥: %v\n", err)
		return []string{}
	}
	
	// ä½¿ç”¨å¢å¼ºçš„JSåˆ†æå™¨æå–URL
	jsCode := buf.String()
	
	// ä½¿ç”¨å¢å¼ºåˆ†æ
	enhancedResult := s.jsAnalyzer.EnhancedAnalyze(jsCode)
	
	// åˆå¹¶æ‰€æœ‰å‘ç°çš„URL
	urls := make([]string, 0)
	seen := make(map[string]bool)
	
	for category, categoryURLs := range enhancedResult {
		for _, url := range categoryURLs {
			if !seen[url] {
				seen[url] = true
				urls = append(urls, url)
			}
		}
		
		// æ‰“å°å„ç±»åˆ«çš„å‘ç°
		if len(categoryURLs) > 0 {
			fmt.Printf("    [%s] å‘ç° %d ä¸ªURL\n", category, len(categoryURLs))
		}
	}
	
	return urls
}

// crawlRecursively é€’å½’çˆ¬å–å‘ç°çš„é“¾æ¥ï¼ˆå•å±‚çˆ¬å–ï¼Œå·²åºŸå¼ƒï¼‰
// è¯·ä½¿ç”¨ crawlRecursivelyMultiLayer
func (s *Spider) crawlRecursively() {
	s.crawlRecursivelyMultiLayer()
}

// crawlRecursivelyMultiLayer çœŸæ­£çš„å¤šå±‚é€’å½’çˆ¬å–ï¼ˆä¿®å¤æ·±åº¦é—®é¢˜ï¼‰
func (s *Spider) crawlRecursivelyMultiLayer() {
	fmt.Println("å¼€å§‹å¤šå±‚é€’å½’çˆ¬å–...")
	
	currentDepth := 1
	totalCrawled := 0
	
	// å¾ªç¯çˆ¬å–æ¯ä¸€å±‚ï¼Œç›´åˆ°è¾¾åˆ°æœ€å¤§æ·±åº¦
	for currentDepth < s.config.DepthSettings.MaxDepth {
		currentDepth++
		
		fmt.Printf("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
		fmt.Printf("ã€ç¬¬ %d å±‚çˆ¬å–ã€‘æœ€å¤§æ·±åº¦: %d\n", currentDepth, s.config.DepthSettings.MaxDepth)
		fmt.Printf("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
		
		// æ”¶é›†å½“å‰å±‚éœ€è¦çˆ¬å–çš„é“¾æ¥
		layerLinks := s.collectLinksForLayer(currentDepth)
		
		if len(layerLinks) == 0 {
			fmt.Printf("ç¬¬ %d å±‚æ²¡æœ‰æ–°é“¾æ¥ï¼Œé€’å½’ç»“æŸ\n", currentDepth)
			break
		}
		
		fmt.Printf("ç¬¬ %d å±‚å‡†å¤‡çˆ¬å– %d ä¸ªé“¾æ¥...\n", currentDepth, len(layerLinks))
		
		// çˆ¬å–å½“å‰å±‚çš„æ‰€æœ‰é“¾æ¥
		newResults := s.crawlLayer(layerLinks, currentDepth)
		
		// åˆå¹¶ç»“æœ
		s.mutex.Lock()
		s.results = append(s.results, newResults...)
		s.mutex.Unlock()
		
		totalCrawled += len(layerLinks)
		fmt.Printf("ç¬¬ %d å±‚çˆ¬å–å®Œæˆï¼æœ¬å±‚çˆ¬å– %d ä¸ªURLï¼Œç´¯è®¡ %d ä¸ª\n", 
			currentDepth, len(layerLinks), totalCrawled)
		
		// æ£€æŸ¥æ˜¯å¦è¾¾åˆ°URLé™åˆ¶
		if totalCrawled >= 500 {
			fmt.Printf("å·²è¾¾åˆ°URLé™åˆ¶(500)ï¼Œé€’å½’ç»“æŸ\n")
			break
		}
	}
	
	fmt.Printf("\nå¤šå±‚é€’å½’çˆ¬å–å®Œæˆï¼æ€»å…±çˆ¬å– %d ä¸ªURLï¼Œæ·±åº¦ %d å±‚\n", totalCrawled, currentDepth)
}

// collectLinksForLayer æ”¶é›†æŒ‡å®šå±‚éœ€è¦çˆ¬å–çš„é“¾æ¥
func (s *Spider) collectLinksForLayer(targetDepth int) []string {
	allLinks := make(map[string]bool)
	externalLinks := make([]string, 0)
	
	s.mutex.Lock()
	// ä»æ‰€æœ‰ç»“æœä¸­æ”¶é›†é“¾æ¥
	for _, result := range s.results {
		for _, link := range result.Links {
			// æ£€æŸ¥æ˜¯å¦å·²è®¿é—®
			if s.visitedURLs[link] {
				continue
			}
			
			// è§£æé“¾æ¥
			parsedURL, err := url.Parse(link)
			if err != nil {
				continue
			}
			
			// ä½œç”¨åŸŸæ£€æŸ¥
			inScope, _ := s.advancedScope.InScope(link)
			if inScope {
				// è§„èŒƒåŒ–URL
				normalizedURL, err := s.paramHandler.NormalizeURL(link)
				if err == nil {
					allLinks[normalizedURL] = true
				} else {
					allLinks[link] = true
				}
			} else {
				if parsedURL.Host != s.targetDomain && parsedURL.Host != "" {
					externalLinks = append(externalLinks, link)
				}
			}
		}
	}
	s.mutex.Unlock()
	
	// è®°å½•å¤–éƒ¨é“¾æ¥
	if len(externalLinks) > 0 {
		s.mutex.Lock()
		s.externalLinks = append(s.externalLinks, externalLinks...)
		s.mutex.Unlock()
		fmt.Printf("  å‘ç° %d ä¸ªå¤–éƒ¨é“¾æ¥ï¼ˆå·²è®°å½•ä½†ä¸çˆ¬å–ï¼‰\n", len(externalLinks))
	}
	
	// è½¬æ¢ä¸ºåˆ—è¡¨å¹¶ä¼˜å…ˆçº§æ’åº
	tasksToSubmit := make([]string, 0)
	for link := range allLinks {
		// å»é‡æ£€æŸ¥
		if s.duplicateHandler.IsDuplicateURL(link) {
			continue
		}
		
		// éªŒè¯æ ¼å¼
		if !IsValidURL(link) {
			continue
		}
		
		tasksToSubmit = append(tasksToSubmit, link)
		
		// æ¯å±‚é™åˆ¶100ä¸ªURL
		if len(tasksToSubmit) >= 100 {
			break
		}
	}
	
	// ä¼˜å…ˆçº§æ’åº
	tasksToSubmit = s.prioritizeURLs(tasksToSubmit)
	
	return tasksToSubmit
}

// crawlLayer çˆ¬å–ä¸€å±‚çš„æ‰€æœ‰é“¾æ¥
func (s *Spider) crawlLayer(links []string, depth int) []*Result {
	results := make([]*Result, 0)
	
	// æ ‡è®°ä¸ºå·²è®¿é—®
	s.mutex.Lock()
	for _, link := range links {
		s.visitedURLs[link] = true
	}
	s.mutex.Unlock()
	
	// ä¸ºæ¯å±‚åˆ›å»ºæ–°çš„å·¥ä½œæ± ï¼ˆä¿®å¤ï¼šé¿å…å¤ç”¨å·²å…³é—­çš„å·¥ä½œæ± ï¼‰
	layerWorkerPool := NewWorkerPool(30, 20)
	
	// å¯åŠ¨å·¥ä½œæ± 
	layerWorkerPool.Start(func(task Task) (*Result, error) {
		return s.crawlURL(task.URL)
	})
	
	// æäº¤æ‰€æœ‰ä»»åŠ¡
	for _, link := range links {
		task := Task{
			URL:   link,
			Depth: depth,
		}
		if err := layerWorkerPool.Submit(task); err != nil {
			fmt.Printf("  æäº¤ä»»åŠ¡å¤±è´¥ %s: %v\n", link, err)
		}
	}
	
	// ç­‰å¾…å®Œæˆï¼ˆä¸æ˜¾ç¤ºè¿›åº¦ï¼Œé¿å…å¹²æ‰°ï¼‰
	layerWorkerPool.Wait()
	
	// æ”¶é›†ç»“æœ
	results = layerWorkerPool.GetResults()
	
	// åœæ­¢å·¥ä½œæ± 
	layerWorkerPool.Stop()
	
	// æ˜¾ç¤ºç»Ÿè®¡
	stats := layerWorkerPool.GetStats()
	fmt.Printf("  æœ¬å±‚ç»Ÿè®¡ - æ€»ä»»åŠ¡: %d, æˆåŠŸ: %d, å¤±è´¥: %d\n", 
		stats["total"], stats["completed"]-stats["failed"], stats["failed"])
	
	return results
}

// crawlURL çˆ¬å–å•ä¸ªURLï¼ˆä¾›å·¥ä½œæ± ä½¿ç”¨ï¼‰
func (s *Spider) crawlURL(targetURL string) (*Result, error) {
	// è§£æURL
	parsedURL, err := url.Parse(targetURL)
	if err != nil {
		return nil, fmt.Errorf("URLè§£æå¤±è´¥: %v", err)
	}
	
	// ä½¿ç”¨é™æ€çˆ¬è™«
	result, err := s.staticCrawler.Crawl(parsedURL)
	if err != nil {
		// å¦‚æœé™æ€çˆ¬è™«å¤±è´¥ï¼Œå°è¯•åŠ¨æ€çˆ¬è™«
		if s.config.StrategySettings.EnableDynamicCrawler {
			result, err = s.dynamicCrawler.Crawl(parsedURL)
		if err != nil {
				return nil, fmt.Errorf("çˆ¬å–å¤±è´¥: %v", err)
			}
		} else {
			return nil, fmt.Errorf("é™æ€çˆ¬è™«å¤±è´¥: %v", err)
		}
	}
	
	return result, nil
}

// showProgress æ˜¾ç¤ºçˆ¬å–è¿›åº¦
func (s *Spider) showProgress() {
	ticker := time.NewTicker(2 * time.Second)
	defer ticker.Stop()
	
	for {
		select {
		case <-ticker.C:
			progress := s.workerPool.GetProgress()
			stats := s.workerPool.GetStats()
			
			// è®¡ç®—è¿›åº¦æ¡
			barWidth := 30
			filled := int(progress * float64(barWidth) / 100)
			bar := strings.Repeat("â–ˆ", filled) + strings.Repeat("â–‘", barWidth-filled)
			
			fmt.Printf("\r[è¿›åº¦] %s %.1f%% (%d/%d)", bar, progress, stats["completed"], stats["total"])
			
			if stats["completed"] >= stats["total"] {
				fmt.Println()
				return
			}
		}
	}
}

// ImportFromBurp ä»Burp Suiteæ–‡ä»¶å¯¼å…¥
func (s *Spider) ImportFromBurp(filename string) error {
	fmt.Printf("ä»Burp Suiteå¯¼å…¥æµé‡: %s\n", filename)
	
	// åˆ›å»ºè¢«åŠ¨çˆ¬å–å™¨
	s.passiveCrawler = NewPassiveCrawler("burp")
	
	// åŠ è½½Burpæ–‡ä»¶
	err := s.passiveCrawler.LoadFromBurp(filename)
	if err != nil {
		return err
	}
	
	// è¿‡æ»¤ç›®æ ‡åŸŸåçš„URL
	targetURLs := s.passiveCrawler.FilterByDomain(s.targetDomain)
	fmt.Printf("è¿‡æ»¤åå¾—åˆ°ç›®æ ‡åŸŸåURL: %dä¸ª\n", len(targetURLs))
	
	// å°†å¯¼å…¥çš„URLå’Œè¡¨å•åŠ å…¥ç»“æœ
	passiveResult := s.passiveCrawler.ExportToResult(s.targetDomain)
	s.addResult(passiveResult)
	
	return nil
}

// ImportFromHAR ä»HARæ–‡ä»¶å¯¼å…¥
func (s *Spider) ImportFromHAR(filename string) error {
	fmt.Printf("ä»HARæ–‡ä»¶å¯¼å…¥æµé‡: %s\n", filename)
	
	// åˆ›å»ºè¢«åŠ¨çˆ¬å–å™¨
	s.passiveCrawler = NewPassiveCrawler("har")
	
	// åŠ è½½HARæ–‡ä»¶
	err := s.passiveCrawler.LoadFromHAR(filename)
	if err != nil {
		return err
	}
	
	// è¿‡æ»¤ç›®æ ‡åŸŸåçš„URL
	targetURLs := s.passiveCrawler.FilterByDomain(s.targetDomain)
	fmt.Printf("è¿‡æ»¤åå¾—åˆ°ç›®æ ‡åŸŸåURL: %dä¸ª\n", len(targetURLs))
	
	// å°†å¯¼å…¥çš„URLå’Œè¡¨å•åŠ å…¥ç»“æœ
	passiveResult := s.passiveCrawler.ExportToResult(s.targetDomain)
	s.addResult(passiveResult)
	
	return nil
}

// Stop åœæ­¢çˆ¬å–
func (s *Spider) Stop() {
	fmt.Println("åœæ­¢çˆ¬å–...")
	s.staticCrawler.Stop()
	s.dynamicCrawler.Stop()
	
	// å…³é—­æ€§èƒ½ä¼˜åŒ–å™¨
	if s.perfOptimizer != nil {
		s.perfOptimizer.Close()
	}
}

// Close ä¼˜é›…å…³é—­çˆ¬è™«ï¼Œé‡Šæ”¾æ‰€æœ‰èµ„æºï¼ˆå®ç° io.Closer æ¥å£ï¼‰
func (s *Spider) Close() error {
	s.closeMux.Lock()
	defer s.closeMux.Unlock()
	
	// é˜²æ­¢é‡å¤å…³é—­
	if s.closed {
		return nil
	}
	
	fmt.Println("\næ­£åœ¨å…³é—­çˆ¬è™«ï¼Œæ¸…ç†èµ„æº...")
	
	// åœæ­¢çˆ¬å–
	s.Stop()
	
	// ç­‰å¾…æ‰€æœ‰ goroutine å®Œæˆ
	s.wg.Wait()
	
	// å…³é—­ done channel
	close(s.done)
	
	// æ ‡è®°ä¸ºå·²å…³é—­
	s.closed = true
	
	fmt.Println("èµ„æºæ¸…ç†å®Œæˆ")
	return nil
}

// cleanup å†…éƒ¨æ¸…ç†æ–¹æ³•ï¼ˆåœ¨ Start ä¸­ä½¿ç”¨ defer è°ƒç”¨ï¼‰
func (s *Spider) cleanup() {
	// Close æ–¹æ³•å·²ç»å¤„ç†äº†æ‰€æœ‰æ¸…ç†å·¥ä½œ
	s.Close()
}

// prioritizeURLs URLä¼˜å…ˆçº§æ’åºï¼ˆPhase 3æ–°å¢ï¼‰
func (s *Spider) prioritizeURLs(urls []string) []string {
	highPriority := make([]string, 0)    // é«˜ä¼˜å…ˆçº§
	mediumPriority := make([]string, 0)  // ä¸­ä¼˜å…ˆçº§
	lowPriority := make([]string, 0)     // ä½ä¼˜å…ˆçº§
	
	for _, url := range urls {
		urlLower := strings.ToLower(url)
		
		// é«˜ä¼˜å…ˆçº§ï¼šå¸¦å¤šä¸ªå‚æ•°çš„URLã€admin/api/loginç­‰æ•æ„Ÿè·¯å¾„
		if (strings.Count(url, "=") >= 2) ||
			strings.Contains(urlLower, "/admin") ||
			strings.Contains(urlLower, "/api/") ||
			strings.Contains(urlLower, "/login") ||
			strings.Contains(urlLower, "/user") ||
			strings.Contains(urlLower, "/account") {
			highPriority = append(highPriority, url)
		} else if strings.Contains(url, "?") {
			// ä¸­ä¼˜å…ˆçº§ï¼šå¸¦å‚æ•°çš„URL
			mediumPriority = append(mediumPriority, url)
		} else {
			// ä½ä¼˜å…ˆçº§ï¼šæ™®é€šURL
			lowPriority = append(lowPriority, url)
		}
	}
	
	// åˆå¹¶ï¼šé«˜ â†’ ä¸­ â†’ ä½
	result := make([]string, 0, len(urls))
	result = append(result, highPriority...)
	result = append(result, mediumPriority...)
	result = append(result, lowPriority...)
	
	return result
}

// IsValidURL æ£€æŸ¥URLæ˜¯å¦ä¸ºæœ‰æ•ˆçš„HTTP/HTTPSé“¾æ¥
func IsValidURL(url string) bool {
	// æ£€æŸ¥æ˜¯å¦ä¸ºç©º
	if url == "" {
		return false
	}
	
	// æ£€æŸ¥æ˜¯å¦ä¸ºjavascript:æˆ–mailto:ç­‰éHTTPé“¾æ¥
	if strings.HasPrefix(url, "javascript:") || 
	   strings.HasPrefix(url, "mailto:") || 
	   strings.HasPrefix(url, "tel:") || 
	   strings.HasPrefix(url, "sms:") || 
	   strings.HasPrefix(url, "ftp:") || 
	   strings.HasPrefix(url, "file:") {
		return false
	}
	
	// æ£€æŸ¥æ˜¯å¦ä¸ºç›¸å¯¹é“¾æ¥ï¼ˆä¸åŒ…å«åè®®ï¼‰
	if !strings.Contains(url, "://") && !strings.HasPrefix(url, "//") {
		return true
	}
	
	// æ£€æŸ¥æ˜¯å¦ä¸ºHTTP/HTTPSåè®®
	if strings.HasPrefix(url, "http://") || strings.HasPrefix(url, "https://") {
		return true
	}
	
	// å…¶ä»–æƒ…å†µè§†ä¸ºæ— æ•ˆ
	return false
}

// ExportResults å¯¼å‡ºç»“æœ
func (s *Spider) ExportResults() map[string]interface{} {
	s.mutex.Lock()
	defer s.mutex.Unlock()
	
	exportData := make(map[string]interface{})
	exportData["total_results"] = len(s.results)
	
	// ç»Ÿè®¡ä¿¡æ¯
	totalLinks := 0
	totalAssets := 0
	totalForms := 0
	totalAPIs := 0
	
	// è¯¦ç»†ç»“æœæ•°æ®
	detailedResults := make([]map[string]interface{}, 0)
	allLinks := make([]string, 0)
	allAPIs := make([]string, 0)
	
	// ä½¿ç”¨æ™ºèƒ½å»é‡å¤„ç†æ‰€æœ‰URLå’Œè¡¨å•
	for _, result := range s.results {
		totalLinks += len(result.Links)
		totalAssets += len(result.Assets)
		totalForms += len(result.Forms)
		totalAPIs += len(result.APIs)
		
		// ä¿å­˜è¯¦ç»†ç»“æœ
		resultData := make(map[string]interface{})
		resultData["url"] = result.URL
		resultData["status_code"] = result.StatusCode
		resultData["content_type"] = result.ContentType
		resultData["links"] = result.Links
		resultData["assets"] = result.Assets
		resultData["forms"] = result.Forms
		resultData["apis"] = result.APIs
		resultData["post_requests"] = result.POSTRequests // æ·»åŠ POSTè¯·æ±‚æ•°æ®
		detailedResults = append(detailedResults, resultData)
		
		// å¤„ç†é“¾æ¥è¿›è¡Œæ™ºèƒ½å»é‡
		for _, link := range result.Links {
			s.smartDeduplication.ProcessURL(link)
		}
		
		// å¤„ç†è¡¨å•è¿›è¡Œæ™ºèƒ½å»é‡
		for _, form := range result.Forms {
			s.smartDeduplication.ProcessForm(form)
		}
		
		// æ”¶é›†æ‰€æœ‰é“¾æ¥
		allLinks = append(allLinks, result.Links...)
		allAPIs = append(allAPIs, result.APIs...)
	}
	
	exportData["total_links"] = totalLinks
	exportData["total_assets"] = totalAssets
	exportData["total_forms"] = totalForms
	exportData["total_apis"] = totalAPIs
	exportData["detailed_results"] = detailedResults
	exportData["links"] = allLinks
	exportData["apis"] = allAPIs
	exportData["external_links"] = s.externalLinks
	exportData["hidden_paths"] = s.hiddenPaths
	exportData["security_findings"] = s.securityFindings
	exportData["cross_domain_js_urls"] = s.crossDomainJS
	exportData["total_hidden_paths"] = len(s.hiddenPaths)
	exportData["total_security_findings"] = len(s.securityFindings)
	exportData["total_cross_domain_js_urls"] = len(s.crossDomainJS)
	
	// æ·»åŠ æ™ºèƒ½å»é‡ç»Ÿè®¡
	exportData["deduplication_stats"] = s.smartDeduplication.GetDeduplicationStats()
	exportData["unique_url_patterns"] = s.smartDeduplication.GetUniqueURLs()
	exportData["unique_form_patterns"] = s.smartDeduplication.GetUniqueForms()
	
	// æ·»åŠ æ–°åŠŸèƒ½ç»Ÿè®¡
	if s.advancedScope != nil {
		exportData["scope_stats"] = s.advancedScope.GetStatistics()
	}
	if s.perfOptimizer != nil {
		exportData["performance_stats"] = s.perfOptimizer.GetStatistics()
	}
	if s.formFiller != nil {
		exportData["form_filler_stats"] = s.formFiller.GetStatistics()
	}
	
	// æ·»åŠ é«˜çº§åŠŸèƒ½ç»Ÿè®¡
	exportData["detected_technologies"] = s.detectedTechs
	exportData["tech_stack_summary"] = s.techDetector.GetTechStackSummary(s.detectedTechs)
	exportData["sensitive_findings"] = s.sensitiveFindings
	exportData["sensitive_stats"] = s.sensitiveDetector.GetStatistics()
	exportData["total_sensitive_findings"] = len(s.sensitiveFindings)
	
	// è¢«åŠ¨çˆ¬å–ç»Ÿè®¡ï¼ˆå¦‚æœä½¿ç”¨ï¼‰
	if s.passiveCrawler != nil {
		exportData["passive_stats"] = s.passiveCrawler.GetStatistics()
	}
	
	// å­åŸŸåæå–ç»Ÿè®¡
	if s.subdomainExtractor != nil {
		exportData["subdomains"] = s.subdomainExtractor.ExportSubdomains()
		exportData["subdomain_stats"] = s.subdomainExtractor.GetStatistics()
		exportData["total_subdomains"] = s.subdomainExtractor.GetSubdomainCount()
	}
	
	// DOMç›¸ä¼¼åº¦æ£€æµ‹ç»Ÿè®¡
	if s.domSimilarity != nil {
		exportData["dom_similarity_stats"] = s.domSimilarity.GetStatistics()
		exportData["similar_pages"] = s.domSimilarity.GetSimilarPages()
		exportData["total_similar_pages"] = len(s.domSimilarity.GetSimilarPages())
	}
	
	// Sitemapå’Œrobots.txtç»Ÿè®¡
	exportData["sitemap_urls"] = s.sitemapURLs
	exportData["robots_urls"] = s.robotsURLs
	exportData["total_sitemap_urls"] = len(s.sitemapURLs)
	exportData["total_robots_urls"] = len(s.robotsURLs)
	
	// === æ–°å¢ï¼šé™æ€èµ„æºåˆ†ç±» ===
	allAssets := make([]string, 0)
	for _, result := range s.results {
		allAssets = append(allAssets, result.Assets...)
	}
	
	if s.assetClassifier != nil {
		classifiedAssets := s.assetClassifier.ClassifyAssets(allAssets)
		exportData["classified_assets"] = classifiedAssets
		exportData["assets_stats"] = s.assetClassifier.GetAssetStats(classifiedAssets)
	}
	
	// === æ–°å¢ï¼šIPé“¾æ¥åˆ†ç±» ===
	// é‡ç½®allLinksç”¨äºIPæ£€æµ‹ï¼ˆåŒ…å«æ‰€æœ‰é“¾æ¥æºï¼‰
	allLinks = make([]string, 0)
	for _, result := range s.results {
		allLinks = append(allLinks, result.Links...)
	}
	// ä¹Ÿæ£€æŸ¥å¤–éƒ¨é“¾æ¥å’Œå…¶ä»–é“¾æ¥æº
	allLinks = append(allLinks, s.externalLinks...)
	allLinks = append(allLinks, s.crossDomainJS...)
	
	if s.ipDetector != nil {
		classifiedIPs := s.ipDetector.ClassifyIPLinks(allLinks)
		exportData["ip_links"] = map[string]interface{}{
			"private_ips":   classifiedIPs["private_ip"],
			"public_ips":    classifiedIPs["public_ip"],
			"private_count": len(classifiedIPs["private_ip"]),
			"public_count":  len(classifiedIPs["public_ip"]),
			"total_count":   len(classifiedIPs["private_ip"]) + len(classifiedIPs["public_ip"]),
			"has_leak":      len(classifiedIPs["private_ip"]) > 0,
		}
	}
	
	return exportData
}