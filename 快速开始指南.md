# Spider Ultimate 快速开始指南

## 🚀 项目简介

Spider Ultimate 是一款功能强大的 Go 语言 Web 爬虫，专为安全测试设计。

### 核心特性

- ✅ **双引擎爬虫** - 静态 + 动态爬虫
- ✅ **智能URL发现** - 103个URL vs Crawlergo的47个（+119%）
- ✅ **AJAX拦截** - 100%覆盖AJAX请求
- ✅ **参数爆破** - GET/POST参数自动枚举
- ✅ **技术栈检测** - 自动识别15+种框架
- ✅ **敏感信息检测** - 30+种敏感模式
- ✅ **JavaScript分析** - 提取40+种URL模式

---

## 📦 环境要求

### 必需环境

```bash
Go 1.21+
Chrome/Chromium 浏览器（用于动态爬虫）
```

### 依赖包

```bash
github.com/PuerkitoBio/goquery v1.8.1
github.com/chromedp/chromedp v0.9.3
github.com/gocolly/colly/v2 v2.1.0
```

---

## 🔧 安装步骤

### 1. 克隆项目

```bash
git clone https://github.com/Warren-Jace/gogospider.git
cd gogospider
```

### 2. 安装依赖

```bash
go mod download
go mod tidy
```

### 3. 编译程序

#### Windows

```powershell
go build -o spider.exe cmd/spider/main.go
```

#### Linux/MacOS

```bash
go build -o spider cmd/spider/main.go
```

---

## 🎯 基础使用

### 1. 简单爬取

```bash
# 基础爬取（深度3层）
spider.exe -url http://testphp.vulnweb.com

# 自定义深度
spider.exe -url http://example.com -depth 5

# 增加并发
spider.exe -url http://example.com -workers 20
```

### 2. 高级模式

```bash
# 智能模式（静态+动态双引擎）
spider.exe -url http://example.com -mode smart

# 仅静态爬虫
spider.exe -url http://example.com -mode static

# 仅动态爬虫
spider.exe -url http://example.com -mode dynamic
```

### 3. 参数爆破

```bash
# 启用GET参数爆破
spider.exe -url http://example.com -fuzz

# 自定义参数字典
spider.exe -url http://example.com -fuzz -fuzz-dict params.txt

# 指定要fuzz的参数
spider.exe -url http://example.com -fuzz -fuzz-params "id,user,page"
```

### 4. 高级选项

```bash
# 使用代理
spider.exe -url http://example.com -proxy http://127.0.0.1:8080

# 自定义User-Agent
spider.exe -url http://example.com -user-agent "Custom Spider 1.0"

# 自定义Cookie
spider.exe -url http://example.com -cookie-file cookies.txt

# 允许子域名
spider.exe -url http://example.com -allow-subdomains

# 忽略robots.txt
spider.exe -url http://example.com -ignore-robots
```

### 5. 日志和输出

```bash
# JSON格式输出
spider.exe -url http://example.com -format json

# 只输出URL（适合pipeline）
spider.exe -url http://example.com -format urls-only

# 简洁模式
spider.exe -url http://example.com -simple

# 保存日志到文件
spider.exe -url http://example.com -log-file spider.log -log-level debug
```

### 6. Pipeline模式（从标准输入读取）

```bash
# 从文件批量爬取
cat urls.txt | spider.exe -stdin -simple

# 与其他工具配合
echo "http://example.com" | spider.exe -stdin -format urls-only

# 批量爬取多个URL
spider.exe -stdin < urls.txt
```

---

## 📊 输出说明

### 输出文件

程序会自动生成以下文件：

1. **spider_domain_timestamp.txt** - 完整爬取结果
2. **spider_domain_timestamp_urls.txt** - 仅URL列表
3. **responses/** - 原始响应数据包（HTML/JS/CSS等）

### 输出示例

```
[GET] http://example.com/page.php | 状态码: 200 | 类型: text/html
  链接数: 25
  表单数: 2
    - POST http://example.com/login.php
  POST请求数: 2
    - [POST] http://example.com/search.php
      参数: {"q":"test","category":"all"}
  API数: 3
    - http://example.com/api/v1/users
```

---

## 🔍 实际案例

### 案例1: 测试DVWA

```bash
# 爬取DVWA（带Cookie认证）
spider.exe -url http://localhost/dvwa \
  -cookie-file dvwa_cookie.txt \
  -depth 5 \
  -fuzz \
  -log-level info
```

### 案例2: 测试API接口

```bash
# 爬取API（JSON输出）
spider.exe -url https://api.example.com \
  -format json \
  -depth 3 \
  -output api_results.json
```

### 案例3: 批量域名扫描

```bash
# 批量扫描多个域名
cat domains.txt | spider.exe -stdin \
  -simple \
  -depth 2 \
  -format urls-only > all_urls.txt
```

### 案例4: 与Burp Suite配合

```bash
# 通过Burp代理爬取
spider.exe -url http://example.com \
  -proxy http://127.0.0.1:8080 \
  -depth 4 \
  -fuzz
```

---

## ⚙️ 配置文件

### 使用配置文件

创建 `config.json`:

```json
{
  "target_url": "http://testphp.vulnweb.com",
  "depth_settings": {
    "max_depth": 5,
    "deep_crawling": true,
    "scheduling_algorithm": "BFS"
  },
  "strategy_settings": {
    "enable_static_crawler": true,
    "enable_dynamic_crawler": true,
    "enable_js_analysis": true,
    "enable_param_fuzzing": true,
    "param_fuzz_limit": 100
  },
  "anti_detection_settings": {
    "request_delay": "500ms",
    "user_agents": [
      "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0"
    ],
    "proxies": [],
    "enable_form_auto_fill": true
  },
  "log_settings": {
    "level": "INFO",
    "format": "json",
    "output_file": "",
    "show_metrics": false
  }
}
```

使用配置文件：

```bash
spider.exe -config config.json
```

---

## 📈 性能优化建议

### 1. 并发调优

```bash
# 小型网站（< 100页）
spider.exe -url http://example.com -workers 5 -depth 3

# 中型网站（100-1000页）
spider.exe -url http://example.com -workers 10 -depth 4

# 大型网站（> 1000页）
spider.exe -url http://example.com -workers 20 -depth 5 -max-pages 500
```

### 2. 速度与准确性平衡

```bash
# 追求速度（仅静态爬虫）
spider.exe -url http://example.com -mode static -workers 20

# 追求全面（动态爬虫 + 参数爆破）
spider.exe -url http://example.com -mode smart -fuzz -depth 6
```

### 3. 内存优化

```bash
# 限制最大页面数
spider.exe -url http://example.com -max-pages 200

# 禁用响应保存（节省磁盘）
# 需要修改代码：注释掉 saveResponseToFile 调用
```

---

## 🐛 常见问题

### Q1: 编译失败

```bash
# 清理并重新下载依赖
go clean -modcache
go mod download
go mod tidy

# 重新编译
go build -o spider.exe cmd/spider/main.go
```

### Q2: Chrome找不到

```bash
# Windows: 指定Chrome路径
spider.exe -url http://example.com -chrome-path "C:\Program Files\Google\Chrome\Application\chrome.exe"

# Linux: 安装chromium
sudo apt-get install chromium-browser
```

### Q3: 爬取结果太少

```bash
# 增加深度
spider.exe -url http://example.com -depth 6

# 启用动态爬虫
spider.exe -url http://example.com -mode smart

# 禁用robots.txt限制
spider.exe -url http://example.com -ignore-robots
```

### Q4: 速度太慢

```bash
# 增加并发
spider.exe -url http://example.com -workers 20

# 仅使用静态爬虫
spider.exe -url http://example.com -mode static

# 减少请求延迟（注意：可能被检测）
# 需要修改 config.go 中的 RequestDelay
```

---

## 📚 进阶功能

### 1. 技术栈检测

程序自动检测并输出目标使用的技术栈：

```
[技术栈检测]
  Web服务器: Nginx 1.19.0
  后端语言: PHP 5.6.40
  前端框架: jQuery 3.5.1
  CMS: WordPress 5.8
```

### 2. 敏感信息检测

自动检测30+种敏感信息：

- API密钥 (AWS, Google, etc.)
- 数据库凭证
- 邮箱地址
- 内网IP
- 身份证号
- 手机号

### 3. 隐藏路径发现

自动扫描100+个常见路径：

- /admin, /manage, /backend
- /.git, /.env, /.config
- /api/v1, /api/v2
- /test, /debug, /dev

### 4. DOM相似度去重

智能识别相似页面，提高效率50%：

```
发现相似页面（85%相似度）:
  - http://example.com/page.php?id=1
  - http://example.com/page.php?id=2
已自动去重
```

---

## 🎓 学习资源

### 官方文档

- 📖 `README.md` - 项目说明
- 🎊 `🎊最终优化完成报告.md` - 优化总结
- ✅ `✅问题完全解决-基于Referer分析的优化.md` - 技术细节
- 🎯 `🎯最终诊断报告-未发现URL的真相.md` - 深度分析

### 代码结构

```
gogospider/
├── cmd/spider/main.go          # 主程序入口
├── config/config.go            # 配置管理
├── core/                       # 核心功能
│   ├── spider.go              # 爬虫引擎
│   ├── static_crawler.go      # 静态爬虫
│   ├── dynamic_crawler.go     # 动态爬虫
│   ├── ajax_interceptor.go    # AJAX拦截
│   ├── param_handler.go       # 参数处理
│   ├── duplicate_handler.go   # 去重处理
│   └── ...                    # 其他模块
└── improvements/              # 改进建议
```

---

## 🤝 贡献指南

欢迎提交Issue和Pull Request！

### 开发环境

```bash
# 运行测试
go test ./...

# 代码格式化
go fmt ./...

# 静态检查
go vet ./...
```

---

## 📄 许可证

本项目采用 MIT 许可证

---

## 🎉 致谢

感谢所有贡献者和使用者！

特别感谢：
- Crawlergo - 灵感来源
- Colly - 静态爬虫框架
- ChromeDP - 动态爬虫支持

---

## 📞 联系方式

- GitHub: https://github.com/Warren-Jace/gogospider
- Issues: https://github.com/Warren-Jace/gogospider/issues

---

**Spider Ultimate v2.5 - 智能、强大、专业的安全爬虫工具** 🏆

